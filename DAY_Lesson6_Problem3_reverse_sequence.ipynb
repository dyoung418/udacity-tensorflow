{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Answer 3\n",
    "Here are my thoughts on the approach:\n",
    "* Use a multi-layer LSTM encoder - decoder as explained in the translate example in Tensorflow\n",
    "* ---Build the batches so that input words are given in reverse order but not mirrored.  For example, \"Now is the time for all good people\" would be input into the encoder as \"people good all for time the is Now\".  This will help the LSTM to remember things closer to where the decoder will need them (just as it helped in translating English to French).--- (EDIT maybe do this in a v2, but start with simple character encoding)\n",
    "* Also build the batches such that the target labels are the mirrored words, i.e. \"won si eht emit rof lla doog elpoep\".\n",
    "* Keep it simple by using a character model and using one-hot encoding since the vocabulary size will be small for a character-based model.  That means I don't think I'll need embeddings for inputs or output_projection for outputs (which is the equivalent of embeddings for inputs which prevent you from having to specify a one-hot for a large vocab on the output -- it is used differently than embeddings though -- you do a matrix multiply + bias to get the one-hot logits and then use sample_softmax to get the predicted percentages.)\n",
    "* Use the end-of-sentence token and pad to a certain length (you could use buckets to make it more efficient for padding, but let's keep it simple for now.\"\n",
    "* Make it a character model, not a word model.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n",
      "Data size 100000000\n",
      "training data size: 99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "validation data size: 1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "          'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)\n",
    "\n",
    "def read_data(filename):\n",
    "    f = zipfile.ZipFile(filename)\n",
    "    for name in f.namelist():\n",
    "        return tf.compat.as_str(f.read(name))\n",
    "    f.close()\n",
    "\n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))\n",
    "\n",
    "# Create small validation set\n",
    "\n",
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print('training data size: %s %s' % (train_size, train_text[:64]))\n",
    "print('validation data size: %s %s' % (valid_size, valid_text[:64]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['if', 'is', 'be', 'is'], ['it', 'to', 'it', 'up'], ['is', 'be', 'is', 'to']]\n",
      "[['is', 'be', 'is', 'to'], ['to', 'it', 'up', 'me'], ['be', 'is', 'to', 'if']]\n"
     ]
    }
   ],
   "source": [
    "import collections, re\n",
    "# DAY\n",
    "#\n",
    "# This is important to understand.  Our NN needs a constant sized vector with each input.  We are\n",
    "# providing that here.  As the video says, just as convolution lets us use the same weight parameters\n",
    "# at different parts of the image, a recurrent neural net lets us use the same weights at different\n",
    "# points in time (or rather, different points in the input sequence).\n",
    "#\n",
    "# The notion of \"unrollings\" is that a recurrent NN has it's output connected to it's input, but really\n",
    "# the way to think about it is over time where the output of time t-1 is input to time t.  That way\n",
    "# of looking at it is like \"unrolling\" the recurrent NN over time so it is understood more as a\n",
    "# sequence of copies of the NN.  \n",
    "# In this case, we are going to be feeding in sequences that are 10 long, so we will in effect\n",
    "# create 10 LSTM cells (which are really just a NN) and hook the output of LSTM cell t with inputs\n",
    "# from input_sub_t and also the output of LSTM cell t-1.\n",
    "\n",
    "# I'm re-writing the BatchGenerator to be more general purpose.  I want it to always output\n",
    "# IDs (not embeddings, not the actual text, but the ID of the embeddings)\n",
    "\n",
    "class SequenceGenerator(object):\n",
    "    '''This class will take a text input and generate batches of IDs suitable for use with\n",
    "    tf.nn.embedding_lookup() (i.e. goes from 0 to len(vocab)-1).  This class can be \n",
    "    inherited to create classes that create IDs for single characters, bigrams, \n",
    "    or entire words.  It also has the ability to take\n",
    "    ID output from the RNN and convert it back to the original text.'''\n",
    "    def __init__(self, text, batch_size, num_unrollings, vocab_size_limit=None):\n",
    "        self._text = text\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        self._vocab_size_limit = vocab_size_limit\n",
    "        self._init_vocab()\n",
    "        self._init_token_sequence()\n",
    "        self._text = None # garbage collect now that self._token_seq is written\n",
    "        segment = self._token_seq_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "        \n",
    "    def _init_vocab(self):\n",
    "        '''Must be implemented in subclasses and create the following:\n",
    "                self._vocab_dict (text keys and ID values)  \n",
    "                self._reverse_vocab_dict (ID keys and text values)\n",
    "                self.vocab_size\n",
    "            Consider also what you will do with self._vocab_size_limit'''\n",
    "        raise NotImplementedError\n",
    "    def _init_token_sequence(self):\n",
    "        '''Must be implemented in subclasses and create the following:\n",
    "                self._token_seq = list of token id's in original input order (so duplicate is ok)\n",
    "                self._token_seq_size = the total # of tokens in the input stream\n",
    "            Consider also what you will do with self._vocab_size_limit'''\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def token_2_id(self, token):\n",
    "        return self._vocab_dict[token]\n",
    "    def id_2_token(self, token_id):\n",
    "        return self._reverse_vocab_dict[token_id]\n",
    "    def onehot_2_id(self, one_hot):\n",
    "        \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "        characters back into its (most likely) ID representation.\n",
    "        This will always return the same result for identical inputs -- it does\n",
    "        not sample across the probability distribution.\n",
    "        This used to be character()\"\"\"\n",
    "        return [c for c in np.argmax(one_hot, 1)]\n",
    "    def id_2_onehot(self, id_list):\n",
    "        '''Turn a list of ids into a list of one_hot encoded vectors'''\n",
    "        identity = tf.constant(np.identity(self.vocab_size, dtype = np.float32))\n",
    "        return tf.nn.embedding_lookup(identity, id_list)\n",
    "    \n",
    "    def softmax_2_sampled_id(self, softmax_distribution):\n",
    "        \"\"\"Turn a softMax probability distribution over the possible\n",
    "        characters into an ID representation based on a sampling over that probability\n",
    "        distribution.\n",
    "        This randomly samples the distribution. So, for example, if in the\n",
    "        softmax distribution 'a' is 40% likely, 'b' is 40% likely and \n",
    "        'c' is 20% likely, this will generate an 'a' 40% of the time\n",
    "        it is called and a 'c' 20% of the time it is called, etc.\"\"\"\n",
    "        r = random.uniform(0, 1)\n",
    "        s = 0\n",
    "        for i in range(len(softmax_distribution)):\n",
    "            s += softmax_distribution[i]\n",
    "            if s >= r:\n",
    "                return i\n",
    "        return len(softmax_distribution) - 1\n",
    "    \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single row (or unrolling) of length 'batch' from the current \n",
    "        cursor position in the token data.  It will be in the form of a row of token IDs.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, ), dtype=np.int32)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b] = self._token_seq[self._cursor[b]]\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._token_seq_size\n",
    "        return batch\n",
    "  \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        (for a total of num_unrollings + 1).\n",
    "        The reason the last batch from the previous array is included is because\n",
    "        in the previous array, the last batch was just used as a label to the model,\n",
    "        not as an input -- so we include it this next time to be used as an input.\n",
    "        Note that the sequential tokens end up being read into the columns of these\n",
    "        'num_unrolling\" batches.  Each column is a separate part of the token input.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "    \n",
    "    def batches_2_tokens(self, batches):\n",
    "        \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "        representation.\"\"\"\n",
    "        # DAY\n",
    "        # This mangles the real batch structure in the interest of readability, but\n",
    "        # by doing so, makes your understanding of batches wrong.\n",
    "        # See 'honest_batches2string' below which gives you a better\n",
    "        # understanding of the batch format.\n",
    "\n",
    "        # batches has dimensions (num_unrollings, batch_size)\n",
    "        s = [''] * batches[0].shape[0]  # batches[0].shape[0] will end up being same as batch_size\n",
    "        for b in batches: # there will be num_unrollings of these...\n",
    "            s = [''.join(self.id_2_token(x)) for x in zip(s, b)]  # DAY __ NEEDS WORK\n",
    "        return s\n",
    "\n",
    "    def honest_batches_2_tokens(self, batches):\n",
    "        import pprint\n",
    "        output = []\n",
    "        for b_index, b in enumerate(batches):  # there will be 'num_unrollings' of these\n",
    "            output.append(list())\n",
    "            for token_id_index, token_id in enumerate(b):  # there will be 'batch_size' of these\n",
    "                output[b_index].append(self.id_2_token(token_id))\n",
    "        return pprint.pformat(output)\n",
    "\n",
    "class SingleCharacterGenerator(SequenceGenerator):\n",
    "    def _init_vocab(self):\n",
    "        '''Must be implemented in subclasses and create the following:\n",
    "                self._reverse_vocab_dict (ID keys and text values)\n",
    "                self.vocab_size\n",
    "            Consider also what you will do with self._vocab_size_limit'''\n",
    "        self.vocab_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "        self._vocab_dict = dict()\n",
    "        self._vocab_dict[' '] = 0\n",
    "        for i, v in enumerate(list(string.ascii_lowercase[:self.vocab_size - 1])):\n",
    "            self._vocab_dict[v] = i+1\n",
    "        self._reverse_vocab_dict = dict(zip(self._vocab_dict.values(), self._vocab_dict.keys()))\n",
    "    def _init_token_sequence(self):\n",
    "        '''Must be implemented in subclasses and create the following:\n",
    "                self._token_seq = list of token id's in original input order (so duplicate is ok)\n",
    "                self._token_seq_size = the total # of tokens in the input stream\n",
    "            Consider also what you will do with self._vocab_size_limit'''\n",
    "        first_letter = ord(string.ascii_lowercase[0])\n",
    "        self._token_seq = list()\n",
    "        for char in self._text.lower():\n",
    "            if char in string.ascii_lowercase:\n",
    "                self._token_seq.append(self.token_2_id(char))\n",
    "            elif char == ' ':\n",
    "                self._token_seq.append(self.token_2_id(' '))\n",
    "            else:\n",
    "                pass # don't enter unknown characters (DAY should we have an UNK ID?)\n",
    "        self._token_seq_size = len(self._token_seq)\n",
    "\n",
    "class BigramGenerator(SequenceGenerator):\n",
    "    def _init_vocab(self):\n",
    "        '''Must be implemented in subclasses and create the following:\n",
    "                self._vocab_dict (text keys and ID values)  \n",
    "                self._reverse_vocab_dict (ID keys and text values)\n",
    "                self.vocab_size\n",
    "            Consider also what you will do with self._vocab_size_limit'''\n",
    "        self._vocab_dict = dict()\n",
    "        self._token_seq = list()\n",
    "        id_index = 0\n",
    "        for i in range(0, len(self._text)-2, 2):\n",
    "            bigram = self._text[i].lower() + self._text[i+1].lower()\n",
    "            if bigram not in self._vocab_dict:\n",
    "                self._vocab_dict[bigram] = id_index\n",
    "                id_index += 1\n",
    "            self._token_seq.append(self.token_2_id(bigram)) # dup ok -- this is just input stream as token ids\n",
    "        self.vocab_size = len(self._vocab_dict)\n",
    "        self._reverse_vocab_dict = dict(zip(self._vocab_dict.values(), self._vocab_dict.keys()))\n",
    "        self._token_seq_size = len(self._token_seq)\n",
    "    def _init_token_sequence(self):\n",
    "        '''Must be implemented in subclasses and create the following:\n",
    "                self._token_seq = list of token id's in original input order (so duplicate is ok)\n",
    "                self._token_seq_size = the total # of tokens in the input stream\n",
    "            Consider also what you will do with self._vocab_size_limit'''\n",
    "        pass # I did this work in _init_vocab() to use just one loop\n",
    "    \n",
    "class WordGenerator(SequenceGenerator):\n",
    "    def _init_vocab(self):\n",
    "        '''Must be implemented in subclasses and create the following:\n",
    "                self._vocab_dict (text keys and ID values)  \n",
    "                self._reverse_vocab_dict (ID keys and text values)\n",
    "                self.vocab_size\n",
    "            Consider also what you will do with self._vocab_size_limit'''\n",
    "        words = self._text.lower().split()\n",
    "        count = [['UNK', -1]]\n",
    "        if self._vocab_size_limit is not None:\n",
    "            count.extend(collections.Counter(words).most_common(self._vocab_size_limit - 1))\n",
    "        else:\n",
    "            count.extend(collections.Counter(words).most_common())\n",
    "        self._vocab_dict = dict()\n",
    "        id_index = 1\n",
    "        for word, _ in count:\n",
    "            self._vocab_dict[word] = id_index\n",
    "            id_index += 1\n",
    "        self._vocab_dict['UNK'] = 0 # force this id value for convenience\n",
    "        self._token_seq = list()\n",
    "        unk_count = 0\n",
    "        for word in words:\n",
    "            if word in self._vocab_dict:\n",
    "                index = self._vocab_dict[word]\n",
    "            else:\n",
    "                index = 0  # self._vocab_dict['UNK']\n",
    "                unk_count = unk_count + 1\n",
    "            self._token_seq.append(index)\n",
    "        count[0][1] = unk_count\n",
    "        self.vocab_size = len(self._vocab_dict)\n",
    "        self._reverse_vocab_dict = dict(zip(self._vocab_dict.values(), self._vocab_dict.keys()))\n",
    "        self._token_seq_size = len(self._token_seq)\n",
    "        #print('count[:1000]: %s' % count[:1000])\n",
    "        del words\n",
    "        del count\n",
    "        # if called with build_dataset('if it is to be it is up to me'.split())\n",
    "        # we get the following:\n",
    "        #>>> count               # count of the '_vocab_size_limit' most common words\n",
    "        #[['UNK', 3], ('is', 2), ('it', 2), ('to', 2), ('me', 1)]\n",
    "        #>>> self._token_seq                # the original text as indices into 'count'\n",
    "        #[0, 2, 1, 3, 0, 2, 1, 0, 3, 4]\n",
    "        #>>> self._vocab_dict          # value refers to index in count\n",
    "        #{'me': 4, 'to': 3, 'is': 1, 'UNK': 0, 'it': 2}\n",
    "        #>>> self._reverse_vocab_dict\n",
    "        #{0: 'UNK', 1: 'is', 2: 'it', 3: 'to', 4: 'me'}\n",
    "\n",
    "        # OLD method that did not have vocab_limit\n",
    "        #self._vocab_dict = dict()\n",
    "        #self._token_seq = list()\n",
    "        #for i, word in enumerate(self._text.lower().split()):\n",
    "        #    if word not in self._vocab_dict:\n",
    "        #        self._vocab_dict[word] = i\n",
    "        #    self._token_seq.append(self.token_2_id(word)) # dup ok -- this is just input stream as token ids\n",
    "        #self.vocab_size = len(self._vocab_dict)\n",
    "        #self._reverse_vocab_dict = dict(zip(self._vocab_dict.values(), self._vocab_dict.keys()))\n",
    "        #self._token_seq_size = len(self._token_seq)\n",
    "    def _init_token_sequence(self):\n",
    "        '''Must be implemented in subclasses and create the following:\n",
    "                self._token_seq = list of token id's in original input order (so duplicate is ok)\n",
    "                self._token_seq_size = the total # of tokens in the input stream\n",
    "                '''\n",
    "        pass # did this as part of _init_vocab()\n",
    "\n",
    "class SingleCharWordAtTimeGenerator(SequenceGenerator):\n",
    "    '''A bespoke generator for Problem 3 which is training a model to spit out mirrored words.\n",
    "    For example \"Now is the time\" should output \"woN si eht emit\".\n",
    "    For this, I want to feed in just one character at a time, but need this generator\n",
    "    to be a bit sophisticated about how to generate the desired labels.  In order to make\n",
    "    it work, I need to make sure I feed in an integral number of words (as opposed to a\n",
    "    fractional number) and I need to have the algorithm use word boundaries to generate the\n",
    "    right label.'''\n",
    "    def _init_vocab(self):\n",
    "        '''Must be implemented in subclasses and create the following:\n",
    "                self._reverse_vocab_dict (ID keys and text values)\n",
    "                self.vocab_size\n",
    "            Consider also what you will do with self._vocab_size_limit\n",
    "            \n",
    "            Changed to add a padding character to the vocabulary\n",
    "            '''\n",
    "        self.vocab_size = len(string.ascii_lowercase) + 2 # [a-z] + ' ' + '#'\n",
    "        self._vocab_dict = dict()\n",
    "        self._vocab_dict[' '] = 0\n",
    "        self._vocab_dict['#'] = 1 # this is the padding character\n",
    "        for i, v in enumerate(list(string.ascii_lowercase[:self.vocab_size - 2])):\n",
    "            self._vocab_dict[v] = i+2\n",
    "        self._reverse_vocab_dict = dict(zip(self._vocab_dict.values(), self._vocab_dict.keys()))\n",
    "    def _init_token_sequence(self):\n",
    "        '''Must be implemented in subclasses and create the following:\n",
    "                self._token_seq = list of token id's in original input order (so duplicate is ok)\n",
    "                self._token_seq_size = the total # of tokens in the input stream\n",
    "            Consider also what you will do with self._vocab_size_limit'''\n",
    "        first_letter = ord(string.ascii_lowercase[0])\n",
    "        self._token_seq = list()\n",
    "        for char in self._text.lower():\n",
    "            if char in string.ascii_lowercase:\n",
    "                self._token_seq.append(self.token_2_id(char))\n",
    "            elif char == ' ':\n",
    "                self._token_seq.append(self.token_2_id(' '))\n",
    "            elif char == '#':  # The padding char (although I doubt it will be in the input)\n",
    "                self._token_seq.append(self.token_2_id('#'))\n",
    "            else:\n",
    "                pass # don't enter unknown characters (DAY should we have an UNK ID?)\n",
    "        self._token_seq_size = len(self._token_seq)\n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single row (or unrolling) of length 'batch' from the current \n",
    "        cursor position in the token data.  It will be in the form of a row of token IDs.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, ), dtype=np.int32)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b] = self._token_seq[self._cursor[b]]\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._token_seq_size\n",
    "        return batch\n",
    "  \n",
    "    def next(self):\n",
    "        \"\"\"While _next_batch() works as it is, this method (next()) needs to be\n",
    "        reimplemented for this class to do the following:\n",
    "        1. Choose word boundaries for the unrollings (that fit in num_unrollings)\n",
    "        and then add padding to get to the num_unrolling size.  There needs to be\n",
    "        at least one padding token at a minimum to serve as an end-of-input token.\n",
    "        2. This will be complicated because we need to figure out the optimal\n",
    "        word boundary for each of the num_batches sequences simultaneously as we\n",
    "        create this batch.\n",
    "        3. I don't think we need to include the last batch from the previous\n",
    "        array like we did before (that was done because the last batch from the \n",
    "        previous array was only used as a label, but never as an input -- that\n",
    "        won't be the case here since labels are no longer just a shift-right-one\n",
    "        of the next char\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "\n",
    "\n",
    "if False:\n",
    "    my_text = \"abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz\"\n",
    "    #my_batches = SingleCharacterGenerator(my_text.lower(), 4, 2)\n",
    "    my_batches = BigramGenerator(my_text.lower(), 4, 2)\n",
    "    print(my_batches.next())\n",
    "    print(my_batches.next())\n",
    "    print(my_batches.next())\n",
    "    print(my_batches.next())\n",
    "    #my_batches = SingleCharacterGenerator(my_text.lower(), 4, 2)\n",
    "    my_batches = BigramGenerator(my_text.lower(), 4, 2)\n",
    "    print(my_batches.honest_batches_2_tokens(my_batches.next()))\n",
    "    print(my_batches.honest_batches_2_tokens(my_batches.next()))\n",
    "    print(my_batches.honest_batches_2_tokens(my_batches.next()))\n",
    "    print(my_batches.honest_batches_2_tokens(my_batches.next()))\n",
    "else:\n",
    "    my_text = \"if it is to be it is up to me\"\n",
    "    my_batches = WordGenerator(my_text.lower(), 4, 2)\n",
    "    print(my_batches.honest_batches_2_tokens(my_batches.next()))\n",
    "    print(my_batches.honest_batches_2_tokens(my_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sihT si a tset fo rorrim sdrow \n"
     ]
    }
   ],
   "source": [
    "def mirror_words(text):\n",
    "    output = []\n",
    "    for word in re.split('\\W+', text):\n",
    "        output.append(word[::-1])\n",
    "    return ' '.join(output)\n",
    "\n",
    "print(mirror_words('This is a test of mirror words.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction, vocabulary_size):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0  # prediction is in column format, so it must be indexed by [0]\n",
    "    return p\n",
    "\n",
    "def random_distribution(vocabulary_size):\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "num_nodes = 64\n",
    "embedding_size = 10\n",
    "\n",
    "if False:\n",
    "    train_batches = SingleCharacterGenerator(train_text, batch_size, num_unrollings)\n",
    "    vocabulary_size = train_batches.vocab_size\n",
    "    valid_batches = SingleCharacterGenerator(valid_text, 1, 1)\n",
    "else:\n",
    "    train_batches = BigramGenerator(train_text, batch_size, num_unrollings)\n",
    "    vocabulary_size = train_batches.vocab_size\n",
    "    valid_batches = BigramGenerator(valid_text, 1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data_utils\n",
    "\n",
    "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"Utilities for downloading data from WMT, tokenizing, vocabularies.\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import gzip\n",
    "import os\n",
    "import re\n",
    "import tarfile\n",
    "\n",
    "from six.moves import urllib\n",
    "\n",
    "from tensorflow.python.platform import gfile\n",
    "import tensorflow as tf\n",
    "\n",
    "# Special vocabulary symbols - we always put them at the start.\n",
    "_PAD = b\"_PAD\"\n",
    "_GO = b\"_GO\"\n",
    "_EOS = b\"_EOS\"\n",
    "_UNK = b\"_UNK\"\n",
    "_START_VOCAB = [_PAD, _GO, _EOS, _UNK]\n",
    "\n",
    "PAD_ID = 0\n",
    "GO_ID = 1\n",
    "EOS_ID = 2\n",
    "UNK_ID = 3\n",
    "\n",
    "# Regular expressions used to tokenize.\n",
    "_WORD_SPLIT = re.compile(b\"([.,!?\\\"':;)(])\")\n",
    "_DIGIT_RE = re.compile(br\"\\d\")\n",
    "\n",
    "# URLs for WMT data.\n",
    "_WMT_ENFR_TRAIN_URL = \"http://www.statmt.org/wmt10/training-giga-fren.tar\"\n",
    "_WMT_ENFR_DEV_URL = \"http://www.statmt.org/wmt15/dev-v2.tgz\"\n",
    "\n",
    "\n",
    "def maybe_download(directory, filename, url):\n",
    "  \"\"\"Download filename from url unless it's already in directory.\"\"\"\n",
    "  if not os.path.exists(directory):\n",
    "    print(\"Creating directory %s\" % directory)\n",
    "    os.mkdir(directory)\n",
    "  filepath = os.path.join(directory, filename)\n",
    "  if not os.path.exists(filepath):\n",
    "    print(\"Downloading %s to %s\" % (url, filepath))\n",
    "    filepath, _ = urllib.request.urlretrieve(url, filepath)\n",
    "    statinfo = os.stat(filepath)\n",
    "    print(\"Succesfully downloaded\", filename, statinfo.st_size, \"bytes\")\n",
    "  return filepath\n",
    "\n",
    "\n",
    "def gunzip_file(gz_path, new_path):\n",
    "  \"\"\"Unzips from gz_path into new_path.\"\"\"\n",
    "  print(\"Unpacking %s to %s\" % (gz_path, new_path))\n",
    "  with gzip.open(gz_path, \"rb\") as gz_file:\n",
    "    with open(new_path, \"wb\") as new_file:\n",
    "      for line in gz_file:\n",
    "        new_file.write(line)\n",
    "\n",
    "\n",
    "def get_wmt_enfr_train_set(directory):\n",
    "  \"\"\"Download the WMT en-fr training corpus to directory unless it's there.\"\"\"\n",
    "  train_path = os.path.join(directory, \"giga-fren.release2.fixed\")\n",
    "  if not (gfile.Exists(train_path +\".fr\") and gfile.Exists(train_path +\".en\")):\n",
    "    corpus_file = maybe_download(directory, \"training-giga-fren.tar\",\n",
    "                                 _WMT_ENFR_TRAIN_URL)\n",
    "    print(\"Extracting tar file %s\" % corpus_file)\n",
    "    with tarfile.open(corpus_file, \"r\") as corpus_tar:\n",
    "      corpus_tar.extractall(directory)\n",
    "    gunzip_file(train_path + \".fr.gz\", train_path + \".fr\")\n",
    "    gunzip_file(train_path + \".en.gz\", train_path + \".en\")\n",
    "  return train_path\n",
    "\n",
    "\n",
    "def get_wmt_enfr_dev_set(directory):\n",
    "  \"\"\"Download the WMT en-fr training corpus to directory unless it's there.\"\"\"\n",
    "  dev_name = \"newstest2013\"\n",
    "  dev_path = os.path.join(directory, dev_name)\n",
    "  if not (gfile.Exists(dev_path + \".fr\") and gfile.Exists(dev_path + \".en\")):\n",
    "    dev_file = maybe_download(directory, \"dev-v2.tgz\", _WMT_ENFR_DEV_URL)\n",
    "    print(\"Extracting tgz file %s\" % dev_file)\n",
    "    with tarfile.open(dev_file, \"r:gz\") as dev_tar:\n",
    "      fr_dev_file = dev_tar.getmember(\"dev/\" + dev_name + \".fr\")\n",
    "      en_dev_file = dev_tar.getmember(\"dev/\" + dev_name + \".en\")\n",
    "      fr_dev_file.name = dev_name + \".fr\"  # Extract without \"dev/\" prefix.\n",
    "      en_dev_file.name = dev_name + \".en\"\n",
    "      dev_tar.extract(fr_dev_file, directory)\n",
    "      dev_tar.extract(en_dev_file, directory)\n",
    "  return dev_path\n",
    "\n",
    "\n",
    "def basic_tokenizer(sentence):\n",
    "  \"\"\"Very basic tokenizer: split the sentence into a list of tokens.\"\"\"\n",
    "  words = []\n",
    "  for space_separated_fragment in sentence.strip().split():\n",
    "    words.extend(_WORD_SPLIT.split(space_separated_fragment))\n",
    "  return [w for w in words if w]\n",
    "\n",
    "\n",
    "def create_vocabulary(vocabulary_path, data_path, max_vocabulary_size,\n",
    "                      tokenizer=None, normalize_digits=True):\n",
    "  \"\"\"Create vocabulary file (if it does not exist yet) from data file.\n",
    "\n",
    "  Data file is assumed to contain one sentence per line. Each sentence is\n",
    "  tokenized and digits are normalized (if normalize_digits is set).\n",
    "  Vocabulary contains the most-frequent tokens up to max_vocabulary_size.\n",
    "  We write it to vocabulary_path in a one-token-per-line format, so that later\n",
    "  token in the first line gets id=0, second line gets id=1, and so on.\n",
    "\n",
    "  Args:\n",
    "    vocabulary_path: path where the vocabulary will be created.\n",
    "    data_path: data file that will be used to create vocabulary.\n",
    "    max_vocabulary_size: limit on the size of the created vocabulary.\n",
    "    tokenizer: a function to use to tokenize each data sentence;\n",
    "      if None, basic_tokenizer will be used.\n",
    "    normalize_digits: Boolean; if true, all digits are replaced by 0s.\n",
    "  \"\"\"\n",
    "  if not gfile.Exists(vocabulary_path):\n",
    "    print(\"Creating vocabulary %s from data %s\" % (vocabulary_path, data_path))\n",
    "    vocab = {}\n",
    "    with gfile.GFile(data_path, mode=\"rb\") as f:\n",
    "      counter = 0\n",
    "      for line in f:\n",
    "        counter += 1\n",
    "        if counter % 100000 == 0:\n",
    "          print(\"  processing line %d\" % counter)\n",
    "        line = tf.compat.as_bytes(line)\n",
    "        tokens = tokenizer(line) if tokenizer else basic_tokenizer(line)\n",
    "        for w in tokens:\n",
    "          word = _DIGIT_RE.sub(b\"0\", w) if normalize_digits else w\n",
    "          if word in vocab:\n",
    "            vocab[word] += 1\n",
    "          else:\n",
    "            vocab[word] = 1\n",
    "      vocab_list = _START_VOCAB + sorted(vocab, key=vocab.get, reverse=True)\n",
    "      if len(vocab_list) > max_vocabulary_size:\n",
    "        vocab_list = vocab_list[:max_vocabulary_size]\n",
    "      with gfile.GFile(vocabulary_path, mode=\"wb\") as vocab_file:\n",
    "        for w in vocab_list:\n",
    "          vocab_file.write(w + b\"\\n\")\n",
    "\n",
    "\n",
    "def initialize_vocabulary(vocabulary_path):\n",
    "  \"\"\"Initialize vocabulary from file.\n",
    "\n",
    "  We assume the vocabulary is stored one-item-per-line, so a file:\n",
    "    dog\n",
    "    cat\n",
    "  will result in a vocabulary {\"dog\": 0, \"cat\": 1}, and this function will\n",
    "  also return the reversed-vocabulary [\"dog\", \"cat\"].\n",
    "\n",
    "  Args:\n",
    "    vocabulary_path: path to the file containing the vocabulary.\n",
    "\n",
    "  Returns:\n",
    "    a pair: the vocabulary (a dictionary mapping string to integers), and\n",
    "    the reversed vocabulary (a list, which reverses the vocabulary mapping).\n",
    "\n",
    "  Raises:\n",
    "    ValueError: if the provided vocabulary_path does not exist.\n",
    "  \"\"\"\n",
    "  if gfile.Exists(vocabulary_path):\n",
    "    rev_vocab = []\n",
    "    with gfile.GFile(vocabulary_path, mode=\"rb\") as f:\n",
    "      rev_vocab.extend(f.readlines())\n",
    "    rev_vocab = [line.strip() for line in rev_vocab]\n",
    "    vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\n",
    "    return vocab, rev_vocab\n",
    "  else:\n",
    "    raise ValueError(\"Vocabulary file %s not found.\", vocabulary_path)\n",
    "\n",
    "\n",
    "def sentence_to_token_ids(sentence, vocabulary,\n",
    "                          tokenizer=None, normalize_digits=True):\n",
    "  \"\"\"Convert a string to list of integers representing token-ids.\n",
    "\n",
    "  For example, a sentence \"I have a dog\" may become tokenized into\n",
    "  [\"I\", \"have\", \"a\", \"dog\"] and with vocabulary {\"I\": 1, \"have\": 2,\n",
    "  \"a\": 4, \"dog\": 7\"} this function will return [1, 2, 4, 7].\n",
    "\n",
    "  Args:\n",
    "    sentence: the sentence in bytes format to convert to token-ids.\n",
    "    vocabulary: a dictionary mapping tokens to integers.\n",
    "    tokenizer: a function to use to tokenize each sentence;\n",
    "      if None, basic_tokenizer will be used.\n",
    "    normalize_digits: Boolean; if true, all digits are replaced by 0s.\n",
    "\n",
    "  Returns:\n",
    "    a list of integers, the token-ids for the sentence.\n",
    "  \"\"\"\n",
    "\n",
    "  if tokenizer:\n",
    "    words = tokenizer(sentence)\n",
    "  else:\n",
    "    words = basic_tokenizer(sentence)\n",
    "  if not normalize_digits:\n",
    "    return [vocabulary.get(w, UNK_ID) for w in words]\n",
    "  # Normalize digits by 0 before looking words up in the vocabulary.\n",
    "  return [vocabulary.get(_DIGIT_RE.sub(b\"0\", w), UNK_ID) for w in words]\n",
    "\n",
    "\n",
    "def data_to_token_ids(data_path, target_path, vocabulary_path,\n",
    "                      tokenizer=None, normalize_digits=True):\n",
    "  \"\"\"Tokenize data file and turn into token-ids using given vocabulary file.\n",
    "\n",
    "  This function loads data line-by-line from data_path, calls the above\n",
    "  sentence_to_token_ids, and saves the result to target_path. See comment\n",
    "  for sentence_to_token_ids on the details of token-ids format.\n",
    "\n",
    "  Args:\n",
    "    data_path: path to the data file in one-sentence-per-line format.\n",
    "    target_path: path where the file with token-ids will be created.\n",
    "    vocabulary_path: path to the vocabulary file.\n",
    "    tokenizer: a function to use to tokenize each sentence;\n",
    "      if None, basic_tokenizer will be used.\n",
    "    normalize_digits: Boolean; if true, all digits are replaced by 0s.\n",
    "  \"\"\"\n",
    "  if not gfile.Exists(target_path):\n",
    "    print(\"Tokenizing data in %s\" % data_path)\n",
    "    vocab, _ = initialize_vocabulary(vocabulary_path)\n",
    "    with gfile.GFile(data_path, mode=\"rb\") as data_file:\n",
    "      with gfile.GFile(target_path, mode=\"w\") as tokens_file:\n",
    "        counter = 0\n",
    "        for line in data_file:\n",
    "          counter += 1\n",
    "          if counter % 100000 == 0:\n",
    "            print(\"  tokenizing line %d\" % counter)\n",
    "          token_ids = sentence_to_token_ids(line, vocab, tokenizer,\n",
    "                                            normalize_digits)\n",
    "          tokens_file.write(\" \".join([str(tok) for tok in token_ids]) + \"\\n\")\n",
    "\n",
    "\n",
    "def prepare_wmt_data(data_dir, en_vocabulary_size, fr_vocabulary_size, tokenizer=None):\n",
    "  \"\"\"Get WMT data into data_dir, create vocabularies and tokenize data.\n",
    "\n",
    "  Args:\n",
    "    data_dir: directory in which the data sets will be stored.\n",
    "    en_vocabulary_size: size of the English vocabulary to create and use.\n",
    "    fr_vocabulary_size: size of the French vocabulary to create and use.\n",
    "    tokenizer: a function to use to tokenize each data sentence;\n",
    "      if None, basic_tokenizer will be used.\n",
    "\n",
    "  Returns:\n",
    "    A tuple of 6 elements:\n",
    "      (1) path to the token-ids for English training data-set,\n",
    "      (2) path to the token-ids for French training data-set,\n",
    "      (3) path to the token-ids for English development data-set,\n",
    "      (4) path to the token-ids for French development data-set,\n",
    "      (5) path to the English vocabulary file,\n",
    "      (6) path to the French vocabulary file.\n",
    "  \"\"\"\n",
    "  # Get wmt data to the specified directory.\n",
    "  train_path = get_wmt_enfr_train_set(data_dir)\n",
    "  dev_path = get_wmt_enfr_dev_set(data_dir)\n",
    "\n",
    "  # Create vocabularies of the appropriate sizes.\n",
    "  fr_vocab_path = os.path.join(data_dir, \"vocab%d.fr\" % fr_vocabulary_size)\n",
    "  en_vocab_path = os.path.join(data_dir, \"vocab%d.en\" % en_vocabulary_size)\n",
    "  create_vocabulary(fr_vocab_path, train_path + \".fr\", fr_vocabulary_size, tokenizer)\n",
    "  create_vocabulary(en_vocab_path, train_path + \".en\", en_vocabulary_size, tokenizer)\n",
    "\n",
    "  # Create token ids for the training data.\n",
    "  fr_train_ids_path = train_path + (\".ids%d.fr\" % fr_vocabulary_size)\n",
    "  en_train_ids_path = train_path + (\".ids%d.en\" % en_vocabulary_size)\n",
    "  data_to_token_ids(train_path + \".fr\", fr_train_ids_path, fr_vocab_path, tokenizer)\n",
    "  data_to_token_ids(train_path + \".en\", en_train_ids_path, en_vocab_path, tokenizer)\n",
    "\n",
    "  # Create token ids for the development data.\n",
    "  fr_dev_ids_path = dev_path + (\".ids%d.fr\" % fr_vocabulary_size)\n",
    "  en_dev_ids_path = dev_path + (\".ids%d.en\" % en_vocabulary_size)\n",
    "  data_to_token_ids(dev_path + \".fr\", fr_dev_ids_path, fr_vocab_path, tokenizer)\n",
    "  data_to_token_ids(dev_path + \".en\", en_dev_ids_path, en_vocab_path, tokenizer)\n",
    "\n",
    "  return (en_train_ids_path, fr_train_ids_path,\n",
    "          en_dev_ids_path, fr_dev_ids_path,\n",
    "          en_vocab_path, fr_vocab_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Seq2Seq\n",
    "\n",
    "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"Sequence-to-sequence model with an attention mechanism.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.models.rnn.translate import data_utils\n",
    "\n",
    "\n",
    "class Seq2SeqModel(object):\n",
    "  \"\"\"Sequence-to-sequence model with attention and for multiple buckets.\n",
    "\n",
    "  This class implements a multi-layer recurrent neural network as encoder,\n",
    "  and an attention-based decoder. This is the same as the model described in\n",
    "  this paper: http://arxiv.org/abs/1412.7449 - please look there for details,\n",
    "  or into the seq2seq library for complete model implementation.\n",
    "  This class also allows to use GRU cells in addition to LSTM cells, and\n",
    "  sampled softmax to handle large output vocabulary size. A single-layer\n",
    "  version of this model, but with bi-directional encoder, was presented in\n",
    "    http://arxiv.org/abs/1409.0473\n",
    "  and sampled softmax is described in Section 3 of the following paper.\n",
    "    http://arxiv.org/abs/1412.2007\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               source_vocab_size,\n",
    "               target_vocab_size,\n",
    "               buckets,\n",
    "               size,\n",
    "               num_layers,\n",
    "               max_gradient_norm,\n",
    "               batch_size,\n",
    "               learning_rate,\n",
    "               learning_rate_decay_factor,\n",
    "               use_lstm=False,\n",
    "               num_samples=512,\n",
    "               forward_only=False,\n",
    "               dtype=tf.float32):\n",
    "    \"\"\"Create the model.\n",
    "\n",
    "    Args:\n",
    "      source_vocab_size: size of the source vocabulary.\n",
    "      target_vocab_size: size of the target vocabulary.\n",
    "      buckets: a list of pairs (I, O), where I specifies maximum input length\n",
    "        that will be processed in that bucket, and O specifies maximum output\n",
    "        length. Training instances that have inputs longer than I or outputs\n",
    "        longer than O will be pushed to the next bucket and padded accordingly.\n",
    "        We assume that the list is sorted, e.g., [(2, 4), (8, 16)].\n",
    "      size: number of units in each layer of the model.\n",
    "      num_layers: number of layers in the model.\n",
    "      max_gradient_norm: gradients will be clipped to maximally this norm.\n",
    "      batch_size: the size of the batches used during training;\n",
    "        the model construction is independent of batch_size, so it can be\n",
    "        changed after initialization if this is convenient, e.g., for decoding.\n",
    "      learning_rate: learning rate to start with.\n",
    "      learning_rate_decay_factor: decay learning rate by this much when needed.\n",
    "      use_lstm: if true, we use LSTM cells instead of GRU cells.\n",
    "      num_samples: number of samples for sampled softmax.\n",
    "      forward_only: if set, we do not construct the backward pass in the model.\n",
    "      dtype: the data type to use to store internal variables.\n",
    "    \"\"\"\n",
    "    self.source_vocab_size = source_vocab_size\n",
    "    self.target_vocab_size = target_vocab_size\n",
    "    self.buckets = buckets\n",
    "    self.batch_size = batch_size\n",
    "    self.learning_rate = tf.Variable(\n",
    "        float(learning_rate), trainable=False, dtype=dtype)\n",
    "    self.learning_rate_decay_op = self.learning_rate.assign(\n",
    "        self.learning_rate * learning_rate_decay_factor)\n",
    "    self.global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    # If we use sampled softmax, we need an output projection.\n",
    "    output_projection = None\n",
    "    softmax_loss_function = None\n",
    "    # Sampled softmax only makes sense if we sample less than vocabulary size.\n",
    "    if num_samples > 0 and num_samples < self.target_vocab_size:\n",
    "      w_t = tf.get_variable(\"proj_w\", [self.target_vocab_size, size], dtype=dtype)\n",
    "      w = tf.transpose(w_t)\n",
    "      b = tf.get_variable(\"proj_b\", [self.target_vocab_size], dtype=dtype)\n",
    "      output_projection = (w, b)\n",
    "\n",
    "      def sampled_loss(inputs, labels):\n",
    "        labels = tf.reshape(labels, [-1, 1])\n",
    "        # We need to compute the sampled_softmax_loss using 32bit floats to\n",
    "        # avoid numerical instabilities.\n",
    "        local_w_t = tf.cast(w_t, tf.float32)\n",
    "        local_b = tf.cast(b, tf.float32)\n",
    "        local_inputs = tf.cast(inputs, tf.float32)\n",
    "        return tf.cast(\n",
    "            tf.nn.sampled_softmax_loss(local_w_t, local_b, local_inputs, labels,\n",
    "                                       num_samples, self.target_vocab_size),\n",
    "            dtype)\n",
    "      softmax_loss_function = sampled_loss\n",
    "\n",
    "    # Create the internal multi-layer cell for our RNN.\n",
    "    single_cell = tf.nn.rnn_cell.GRUCell(size)\n",
    "    if use_lstm:\n",
    "      single_cell = tf.nn.rnn_cell.BasicLSTMCell(size)\n",
    "    cell = single_cell\n",
    "    if num_layers > 1:\n",
    "      cell = tf.nn.rnn_cell.MultiRNNCell([single_cell] * num_layers)\n",
    "\n",
    "    # The seq2seq function: we use embedding for the input and attention.\n",
    "    def seq2seq_f(encoder_inputs, decoder_inputs, do_decode):\n",
    "      return tf.nn.seq2seq.embedding_attention_seq2seq(\n",
    "          encoder_inputs,\n",
    "          decoder_inputs,\n",
    "          cell,\n",
    "          num_encoder_symbols=source_vocab_size,\n",
    "          num_decoder_symbols=target_vocab_size,\n",
    "          embedding_size=size,\n",
    "          output_projection=output_projection,\n",
    "          feed_previous=do_decode,\n",
    "          dtype=dtype)\n",
    "\n",
    "    # Feeds for inputs.\n",
    "    self.encoder_inputs = []\n",
    "    self.decoder_inputs = []\n",
    "    self.target_weights = []\n",
    "    for i in xrange(buckets[-1][0]):  # Last bucket is the biggest one.\n",
    "      self.encoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n",
    "                                                name=\"encoder{0}\".format(i)))\n",
    "    for i in xrange(buckets[-1][1] + 1):\n",
    "      self.decoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n",
    "                                                name=\"decoder{0}\".format(i)))\n",
    "      self.target_weights.append(tf.placeholder(dtype, shape=[None],\n",
    "                                                name=\"weight{0}\".format(i)))\n",
    "\n",
    "    # Our targets are decoder inputs shifted by one.\n",
    "    targets = [self.decoder_inputs[i + 1]\n",
    "               for i in xrange(len(self.decoder_inputs) - 1)]\n",
    "\n",
    "    # Training outputs and losses.\n",
    "    if forward_only:\n",
    "      self.outputs, self.losses = tf.nn.seq2seq.model_with_buckets(\n",
    "          self.encoder_inputs, self.decoder_inputs, targets,\n",
    "          self.target_weights, buckets, lambda x, y: seq2seq_f(x, y, True),\n",
    "          softmax_loss_function=softmax_loss_function)\n",
    "      # If we use output projection, we need to project outputs for decoding.\n",
    "      if output_projection is not None:\n",
    "        for b in xrange(len(buckets)):\n",
    "          self.outputs[b] = [\n",
    "              tf.matmul(output, output_projection[0]) + output_projection[1]\n",
    "              for output in self.outputs[b]\n",
    "          ]\n",
    "    else:\n",
    "      self.outputs, self.losses = tf.nn.seq2seq.model_with_buckets(\n",
    "          self.encoder_inputs, self.decoder_inputs, targets,\n",
    "          self.target_weights, buckets,\n",
    "          lambda x, y: seq2seq_f(x, y, False),\n",
    "          softmax_loss_function=softmax_loss_function)\n",
    "\n",
    "    # Gradients and SGD update operation for training the model.\n",
    "    params = tf.trainable_variables()\n",
    "    if not forward_only:\n",
    "      self.gradient_norms = []\n",
    "      self.updates = []\n",
    "      opt = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
    "      for b in xrange(len(buckets)):\n",
    "        gradients = tf.gradients(self.losses[b], params)\n",
    "        clipped_gradients, norm = tf.clip_by_global_norm(gradients,\n",
    "                                                         max_gradient_norm)\n",
    "        self.gradient_norms.append(norm)\n",
    "        self.updates.append(opt.apply_gradients(\n",
    "            zip(clipped_gradients, params), global_step=self.global_step))\n",
    "\n",
    "    self.saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "  def step(self, session, encoder_inputs, decoder_inputs, target_weights,\n",
    "           bucket_id, forward_only):\n",
    "    \"\"\"Run a step of the model feeding the given inputs.\n",
    "\n",
    "    Args:\n",
    "      session: tensorflow session to use.\n",
    "      encoder_inputs: list of numpy int vectors to feed as encoder inputs.\n",
    "      decoder_inputs: list of numpy int vectors to feed as decoder inputs.\n",
    "      target_weights: list of numpy float vectors to feed as target weights.\n",
    "      bucket_id: which bucket of the model to use.\n",
    "      forward_only: whether to do the backward step or only forward.\n",
    "\n",
    "    Returns:\n",
    "      A triple consisting of gradient norm (or None if we did not do backward),\n",
    "      average perplexity, and the outputs.\n",
    "\n",
    "    Raises:\n",
    "      ValueError: if length of encoder_inputs, decoder_inputs, or\n",
    "        target_weights disagrees with bucket size for the specified bucket_id.\n",
    "    \"\"\"\n",
    "    # Check if the sizes match.\n",
    "    encoder_size, decoder_size = self.buckets[bucket_id]\n",
    "    if len(encoder_inputs) != encoder_size:\n",
    "      raise ValueError(\"Encoder length must be equal to the one in bucket,\"\n",
    "                       \" %d != %d.\" % (len(encoder_inputs), encoder_size))\n",
    "    if len(decoder_inputs) != decoder_size:\n",
    "      raise ValueError(\"Decoder length must be equal to the one in bucket,\"\n",
    "                       \" %d != %d.\" % (len(decoder_inputs), decoder_size))\n",
    "    if len(target_weights) != decoder_size:\n",
    "      raise ValueError(\"Weights length must be equal to the one in bucket,\"\n",
    "                       \" %d != %d.\" % (len(target_weights), decoder_size))\n",
    "\n",
    "    # Input feed: encoder inputs, decoder inputs, target_weights, as provided.\n",
    "    input_feed = {}\n",
    "    for l in xrange(encoder_size):\n",
    "      input_feed[self.encoder_inputs[l].name] = encoder_inputs[l]\n",
    "    for l in xrange(decoder_size):\n",
    "      input_feed[self.decoder_inputs[l].name] = decoder_inputs[l]\n",
    "      input_feed[self.target_weights[l].name] = target_weights[l]\n",
    "\n",
    "    # Since our targets are decoder inputs shifted by one, we need one more.\n",
    "    last_target = self.decoder_inputs[decoder_size].name\n",
    "    input_feed[last_target] = np.zeros([self.batch_size], dtype=np.int32)\n",
    "\n",
    "    # Output feed: depends on whether we do a backward step or not.\n",
    "    if not forward_only:\n",
    "      output_feed = [self.updates[bucket_id],  # Update Op that does SGD.\n",
    "                     self.gradient_norms[bucket_id],  # Gradient norm.\n",
    "                     self.losses[bucket_id]]  # Loss for this batch.\n",
    "    else:\n",
    "      output_feed = [self.losses[bucket_id]]  # Loss for this batch.\n",
    "      for l in xrange(decoder_size):  # Output logits.\n",
    "        output_feed.append(self.outputs[bucket_id][l])\n",
    "\n",
    "    outputs = session.run(output_feed, input_feed)\n",
    "    if not forward_only:\n",
    "      return outputs[1], outputs[2], None  # Gradient norm, loss, no outputs.\n",
    "    else:\n",
    "      return None, outputs[0], outputs[1:]  # No gradient norm, loss, outputs.\n",
    "\n",
    "  def get_batch(self, data, bucket_id):\n",
    "    \"\"\"Get a random batch of data from the specified bucket, prepare for step.\n",
    "\n",
    "    To feed data in step(..) it must be a list of batch-major vectors, while\n",
    "    data here contains single length-major cases. So the main logic of this\n",
    "    function is to re-index data cases to be in the proper format for feeding.\n",
    "\n",
    "    Args:\n",
    "      data: a tuple of size len(self.buckets) in which each element contains\n",
    "        lists of pairs of input and output data that we use to create a batch.\n",
    "      bucket_id: integer, which bucket to get the batch for.\n",
    "\n",
    "    Returns:\n",
    "      The triple (encoder_inputs, decoder_inputs, target_weights) for\n",
    "      the constructed batch that has the proper format to call step(...) later.\n",
    "    \"\"\"\n",
    "    encoder_size, decoder_size = self.buckets[bucket_id]\n",
    "    encoder_inputs, decoder_inputs = [], []\n",
    "\n",
    "    # Get a random batch of encoder and decoder inputs from data,\n",
    "    # pad them if needed, reverse encoder inputs and add GO to decoder.\n",
    "    for _ in xrange(self.batch_size):\n",
    "      encoder_input, decoder_input = random.choice(data[bucket_id])\n",
    "\n",
    "      # Encoder inputs are padded and then reversed.\n",
    "      encoder_pad = [data_utils.PAD_ID] * (encoder_size - len(encoder_input))\n",
    "      encoder_inputs.append(list(reversed(encoder_input + encoder_pad)))\n",
    "\n",
    "      # Decoder inputs get an extra \"GO\" symbol, and are padded then.\n",
    "      decoder_pad_size = decoder_size - len(decoder_input) - 1\n",
    "      decoder_inputs.append([data_utils.GO_ID] + decoder_input +\n",
    "                            [data_utils.PAD_ID] * decoder_pad_size)\n",
    "\n",
    "    # Now we create batch-major vectors from the data selected above.\n",
    "    batch_encoder_inputs, batch_decoder_inputs, batch_weights = [], [], []\n",
    "\n",
    "    # Batch encoder inputs are just re-indexed encoder_inputs.\n",
    "    for length_idx in xrange(encoder_size):\n",
    "      batch_encoder_inputs.append(\n",
    "          np.array([encoder_inputs[batch_idx][length_idx]\n",
    "                    for batch_idx in xrange(self.batch_size)], dtype=np.int32))\n",
    "\n",
    "    # Batch decoder inputs are re-indexed decoder_inputs, we create weights.\n",
    "    for length_idx in xrange(decoder_size):\n",
    "      batch_decoder_inputs.append(\n",
    "          np.array([decoder_inputs[batch_idx][length_idx]\n",
    "                    for batch_idx in xrange(self.batch_size)], dtype=np.int32))\n",
    "\n",
    "      # Create target_weights to be 0 for targets that are padding.\n",
    "      batch_weight = np.ones(self.batch_size, dtype=np.float32)\n",
    "      for batch_idx in xrange(self.batch_size):\n",
    "        # We set weight to 0 if the corresponding target is a PAD symbol.\n",
    "        # The corresponding target is decoder_input shifted by 1 forward.\n",
    "        if length_idx < decoder_size - 1:\n",
    "          target = decoder_inputs[batch_idx][length_idx + 1]\n",
    "        if length_idx == decoder_size - 1 or target == data_utils.PAD_ID:\n",
    "          batch_weight[batch_idx] = 0.0\n",
    "      batch_weights.append(batch_weight)\n",
    "    return batch_encoder_inputs, batch_decoder_inputs, batch_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing WMT data in /tmp\n",
      "Downloading http://www.statmt.org/wmt10/training-giga-fren.tar to /tmp/training-giga-fren.tar\n"
     ]
    }
   ],
   "source": [
    "# Translate \n",
    "\n",
    "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"Binary for training translation models and decoding from them.\n",
    "\n",
    "Running this program without --decode will download the WMT corpus into\n",
    "the directory specified as --data_dir and tokenize it in a very basic way,\n",
    "and then start training a model saving checkpoints to --train_dir.\n",
    "\n",
    "Running with --decode starts an interactive loop so you can see how\n",
    "the current checkpoint translates English sentences into French.\n",
    "\n",
    "See the following papers for more information on neural translation models.\n",
    " * http://arxiv.org/abs/1409.3215\n",
    " * http://arxiv.org/abs/1409.0473\n",
    " * http://arxiv.org/abs/1412.2007\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.models.rnn.translate import data_utils\n",
    "from tensorflow.models.rnn.translate import seq2seq_model\n",
    "\n",
    "\n",
    "tf.app.flags.DEFINE_float(\"learning_rate\", 0.5, \"Learning rate.\")\n",
    "tf.app.flags.DEFINE_float(\"learning_rate_decay_factor\", 0.99,\n",
    "                          \"Learning rate decays by this much.\")\n",
    "tf.app.flags.DEFINE_float(\"max_gradient_norm\", 5.0,\n",
    "                          \"Clip gradients to this norm.\")\n",
    "tf.app.flags.DEFINE_integer(\"batch_size\", 64,\n",
    "                            \"Batch size to use during training.\")\n",
    "tf.app.flags.DEFINE_integer(\"size\", 1024, \"Size of each model layer.\")\n",
    "tf.app.flags.DEFINE_integer(\"num_layers\", 3, \"Number of layers in the model.\")\n",
    "tf.app.flags.DEFINE_integer(\"en_vocab_size\", 40000, \"English vocabulary size.\")\n",
    "tf.app.flags.DEFINE_integer(\"fr_vocab_size\", 40000, \"French vocabulary size.\")\n",
    "tf.app.flags.DEFINE_string(\"data_dir\", \"/tmp\", \"Data directory\")\n",
    "tf.app.flags.DEFINE_string(\"train_dir\", \"/tmp\", \"Training directory.\")\n",
    "tf.app.flags.DEFINE_integer(\"max_train_data_size\", 0,\n",
    "                            \"Limit on the size of training data (0: no limit).\")\n",
    "tf.app.flags.DEFINE_integer(\"steps_per_checkpoint\", 200,\n",
    "                            \"How many training steps to do per checkpoint.\")\n",
    "tf.app.flags.DEFINE_boolean(\"decode\", False,\n",
    "                            \"Set to True for interactive decoding.\")\n",
    "tf.app.flags.DEFINE_boolean(\"self_test\", False,\n",
    "                            \"Run a self-test if this is set to True.\")\n",
    "tf.app.flags.DEFINE_boolean(\"use_fp16\", False,\n",
    "                            \"Train using fp16 instead of fp32.\")\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "# We use a number of buckets and pad to the closest one for efficiency.\n",
    "# See seq2seq_model.Seq2SeqModel for details of how they work.\n",
    "_buckets = [(5, 10), (10, 15), (20, 25), (40, 50)]\n",
    "\n",
    "\n",
    "def read_data(source_path, target_path, max_size=None):\n",
    "  \"\"\"Read data from source and target files and put into buckets.\n",
    "\n",
    "  Args:\n",
    "    source_path: path to the files with token-ids for the source language.\n",
    "    target_path: path to the file with token-ids for the target language;\n",
    "      it must be aligned with the source file: n-th line contains the desired\n",
    "      output for n-th line from the source_path.\n",
    "    max_size: maximum number of lines to read, all other will be ignored;\n",
    "      if 0 or None, data files will be read completely (no limit).\n",
    "\n",
    "  Returns:\n",
    "    data_set: a list of length len(_buckets); data_set[n] contains a list of\n",
    "      (source, target) pairs read from the provided data files that fit\n",
    "      into the n-th bucket, i.e., such that len(source) < _buckets[n][0] and\n",
    "      len(target) < _buckets[n][1]; source and target are lists of token-ids.\n",
    "  \"\"\"\n",
    "  data_set = [[] for _ in _buckets]\n",
    "  with tf.gfile.GFile(source_path, mode=\"r\") as source_file:\n",
    "    with tf.gfile.GFile(target_path, mode=\"r\") as target_file:\n",
    "      source, target = source_file.readline(), target_file.readline()\n",
    "      counter = 0\n",
    "      while source and target and (not max_size or counter < max_size):\n",
    "        counter += 1\n",
    "        if counter % 100000 == 0:\n",
    "          print(\"  reading data line %d\" % counter)\n",
    "          sys.stdout.flush()\n",
    "        source_ids = [int(x) for x in source.split()]\n",
    "        target_ids = [int(x) for x in target.split()]\n",
    "        target_ids.append(data_utils.EOS_ID)\n",
    "        for bucket_id, (source_size, target_size) in enumerate(_buckets):\n",
    "          if len(source_ids) < source_size and len(target_ids) < target_size:\n",
    "            data_set[bucket_id].append([source_ids, target_ids])\n",
    "            break\n",
    "        source, target = source_file.readline(), target_file.readline()\n",
    "  return data_set\n",
    "\n",
    "\n",
    "def create_model(session, forward_only):\n",
    "  \"\"\"Create translation model and initialize or load parameters in session.\"\"\"\n",
    "  dtype = tf.float16 if FLAGS.use_fp16 else tf.float32\n",
    "  model = seq2seq_model.Seq2SeqModel(\n",
    "      FLAGS.en_vocab_size,\n",
    "      FLAGS.fr_vocab_size,\n",
    "      _buckets,\n",
    "      FLAGS.size,\n",
    "      FLAGS.num_layers,\n",
    "      FLAGS.max_gradient_norm,\n",
    "      FLAGS.batch_size,\n",
    "      FLAGS.learning_rate,\n",
    "      FLAGS.learning_rate_decay_factor,\n",
    "      forward_only=forward_only,\n",
    "      dtype=dtype)\n",
    "  ckpt = tf.train.get_checkpoint_state(FLAGS.train_dir)\n",
    "  if ckpt and tf.gfile.Exists(ckpt.model_checkpoint_path):\n",
    "    print(\"Reading model parameters from %s\" % ckpt.model_checkpoint_path)\n",
    "    model.saver.restore(session, ckpt.model_checkpoint_path)\n",
    "  else:\n",
    "    print(\"Created model with fresh parameters.\")\n",
    "    session.run(tf.initialize_all_variables())\n",
    "  return model\n",
    "\n",
    "\n",
    "def train():\n",
    "  \"\"\"Train a en->fr translation model using WMT data.\"\"\"\n",
    "  # Prepare WMT data.\n",
    "  print(\"Preparing WMT data in %s\" % FLAGS.data_dir)\n",
    "  en_train, fr_train, en_dev, fr_dev, _, _ = data_utils.prepare_wmt_data(\n",
    "      FLAGS.data_dir, FLAGS.en_vocab_size, FLAGS.fr_vocab_size)\n",
    "\n",
    "  with tf.Session() as sess:\n",
    "    # Create model.\n",
    "    print(\"Creating %d layers of %d units.\" % (FLAGS.num_layers, FLAGS.size))\n",
    "    model = create_model(sess, False)\n",
    "\n",
    "    # Read data into buckets and compute their sizes.\n",
    "    print (\"Reading development and training data (limit: %d).\"\n",
    "           % FLAGS.max_train_data_size)\n",
    "    dev_set = read_data(en_dev, fr_dev)\n",
    "    train_set = read_data(en_train, fr_train, FLAGS.max_train_data_size)\n",
    "    train_bucket_sizes = [len(train_set[b]) for b in xrange(len(_buckets))]\n",
    "    train_total_size = float(sum(train_bucket_sizes))\n",
    "\n",
    "    # A bucket scale is a list of increasing numbers from 0 to 1 that we'll use\n",
    "    # to select a bucket. Length of [scale[i], scale[i+1]] is proportional to\n",
    "    # the size if i-th training bucket, as used later.\n",
    "    train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size\n",
    "                           for i in xrange(len(train_bucket_sizes))]\n",
    "\n",
    "    # This is the training loop.\n",
    "    step_time, loss = 0.0, 0.0\n",
    "    current_step = 0\n",
    "    previous_losses = []\n",
    "    while True:\n",
    "      # Choose a bucket according to data distribution. We pick a random number\n",
    "      # in [0, 1] and use the corresponding interval in train_buckets_scale.\n",
    "      random_number_01 = np.random.random_sample()\n",
    "      bucket_id = min([i for i in xrange(len(train_buckets_scale))\n",
    "                       if train_buckets_scale[i] > random_number_01])\n",
    "\n",
    "      # Get a batch and make a step.\n",
    "      start_time = time.time()\n",
    "      encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
    "          train_set, bucket_id)\n",
    "      _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs,\n",
    "                                   target_weights, bucket_id, False)\n",
    "      step_time += (time.time() - start_time) / FLAGS.steps_per_checkpoint\n",
    "      loss += step_loss / FLAGS.steps_per_checkpoint\n",
    "      current_step += 1\n",
    "\n",
    "      # Once in a while, we save checkpoint, print statistics, and run evals.\n",
    "      if current_step % FLAGS.steps_per_checkpoint == 0:\n",
    "        # Print statistics for the previous epoch.\n",
    "        perplexity = math.exp(float(loss)) if loss < 300 else float(\"inf\")\n",
    "        print (\"global step %d learning rate %.4f step-time %.2f perplexity \"\n",
    "               \"%.2f\" % (model.global_step.eval(), model.learning_rate.eval(),\n",
    "                         step_time, perplexity))\n",
    "        # Decrease learning rate if no improvement was seen over last 3 times.\n",
    "        if len(previous_losses) > 2 and loss > max(previous_losses[-3:]):\n",
    "          sess.run(model.learning_rate_decay_op)\n",
    "        previous_losses.append(loss)\n",
    "        # Save checkpoint and zero timer and loss.\n",
    "        checkpoint_path = os.path.join(FLAGS.train_dir, \"translate.ckpt\")\n",
    "        model.saver.save(sess, checkpoint_path, global_step=model.global_step)\n",
    "        step_time, loss = 0.0, 0.0\n",
    "        # Run evals on development set and print their perplexity.\n",
    "        for bucket_id in xrange(len(_buckets)):\n",
    "          if len(dev_set[bucket_id]) == 0:\n",
    "            print(\"  eval: empty bucket %d\" % (bucket_id))\n",
    "            continue\n",
    "          encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
    "              dev_set, bucket_id)\n",
    "          _, eval_loss, _ = model.step(sess, encoder_inputs, decoder_inputs,\n",
    "                                       target_weights, bucket_id, True)\n",
    "          eval_ppx = math.exp(float(eval_loss)) if eval_loss < 300 else float(\n",
    "              \"inf\")\n",
    "          print(\"  eval: bucket %d perplexity %.2f\" % (bucket_id, eval_ppx))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "\n",
    "def decode():\n",
    "  with tf.Session() as sess:\n",
    "    # Create model and load parameters.\n",
    "    model = create_model(sess, True)\n",
    "    model.batch_size = 1  # We decode one sentence at a time.\n",
    "\n",
    "    # Load vocabularies.\n",
    "    en_vocab_path = os.path.join(FLAGS.data_dir,\n",
    "                                 \"vocab%d.en\" % FLAGS.en_vocab_size)\n",
    "    fr_vocab_path = os.path.join(FLAGS.data_dir,\n",
    "                                 \"vocab%d.fr\" % FLAGS.fr_vocab_size)\n",
    "    en_vocab, _ = data_utils.initialize_vocabulary(en_vocab_path)\n",
    "    _, rev_fr_vocab = data_utils.initialize_vocabulary(fr_vocab_path)\n",
    "\n",
    "    # Decode from standard input.\n",
    "    sys.stdout.write(\"> \")\n",
    "    sys.stdout.flush()\n",
    "    sentence = sys.stdin.readline()\n",
    "    while sentence:\n",
    "      # Get token-ids for the input sentence.\n",
    "      token_ids = data_utils.sentence_to_token_ids(tf.compat.as_bytes(sentence), en_vocab)\n",
    "      # Which bucket does it belong to?\n",
    "      bucket_id = min([b for b in xrange(len(_buckets))\n",
    "                       if _buckets[b][0] > len(token_ids)])\n",
    "      # Get a 1-element batch to feed the sentence to the model.\n",
    "      encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
    "          {bucket_id: [(token_ids, [])]}, bucket_id)\n",
    "      # Get output logits for the sentence.\n",
    "      _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs,\n",
    "                                       target_weights, bucket_id, True)\n",
    "      # This is a greedy decoder - outputs are just argmaxes of output_logits.\n",
    "      outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n",
    "      # If there is an EOS symbol in outputs, cut them at that point.\n",
    "      if data_utils.EOS_ID in outputs:\n",
    "        outputs = outputs[:outputs.index(data_utils.EOS_ID)]\n",
    "      # Print out French sentence corresponding to outputs.\n",
    "      print(\" \".join([tf.compat.as_str(rev_fr_vocab[output]) for output in outputs]))\n",
    "      print(\"> \", end=\"\")\n",
    "      sys.stdout.flush()\n",
    "      sentence = sys.stdin.readline()\n",
    "\n",
    "\n",
    "def self_test():\n",
    "  \"\"\"Test the translation model.\"\"\"\n",
    "  with tf.Session() as sess:\n",
    "    print(\"Self-test for neural translation model.\")\n",
    "    # Create model with vocabularies of 10, 2 small buckets, 2 layers of 32.\n",
    "    model = seq2seq_model.Seq2SeqModel(10, 10, [(3, 3), (6, 6)], 32, 2,\n",
    "                                       5.0, 32, 0.3, 0.99, num_samples=8)\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    # Fake data set for both the (3, 3) and (6, 6) bucket.\n",
    "    data_set = ([([1, 1], [2, 2]), ([3, 3], [4]), ([5], [6])],\n",
    "                [([1, 1, 1, 1, 1], [2, 2, 2, 2, 2]), ([3, 3, 3], [5, 6])])\n",
    "    for _ in xrange(5):  # Train the fake model for 5 steps.\n",
    "      bucket_id = random.choice([0, 1])\n",
    "      encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
    "          data_set, bucket_id)\n",
    "      model.step(sess, encoder_inputs, decoder_inputs, target_weights,\n",
    "                 bucket_id, False)\n",
    "\n",
    "\n",
    "def main(_):\n",
    "  if FLAGS.self_test:\n",
    "    self_test()\n",
    "  elif FLAGS.decode:\n",
    "    decode()\n",
    "  else:\n",
    "    train()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  tf.app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
