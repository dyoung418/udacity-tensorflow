{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "          'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    f = zipfile.ZipFile(filename)\n",
    "    for name in f.namelist():\n",
    "        return tf.compat.as_str(f.read(name))\n",
    "    f.close()\n",
    "\n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data size: 99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "validation data size: 1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print('training data size: %s %s' % (train_size, train_text[:64]))\n",
    "print('validation data size: %s %s' % (valid_size, valid_text[:64]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 26 0 0\n",
      "a z  \n",
      "vocabulary size: 27\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        #print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "    \n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('Ã¯'))\n",
    "print(id2char(1), id2char(26), id2char(0))\n",
    "print('vocabulary size: %s' % vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchGenerator init: \n",
      "  textsize: 26\n",
      "  batch_size: 4\n",
      "  num_unrollings: 2\n",
      "BatchGenerator init -- text starts with: abcdefghijklmnopqrstuvwxyz\n",
      "BatchGenerator init -- segment=6\n",
      "BatchGenerator init -- _cursor=[1, 7, 13, 19]\n",
      "[[['a'], ['g'], ['m'], ['s']],\n",
      " [['b'], ['h'], ['n'], ['t']],\n",
      " [['c'], ['i'], ['o'], ['u']]]\n",
      "[[['c'], ['i'], ['o'], ['u']],\n",
      " [['d'], ['j'], ['p'], ['v']],\n",
      " [['e'], ['k'], ['q'], ['w']]]\n",
      "[[['e'], ['k'], ['q'], ['w']],\n",
      " [['f'], ['l'], ['r'], ['x']],\n",
      " [['g'], ['m'], ['s'], ['y']]]\n",
      "[[['g'], ['m'], ['s'], ['y']],\n",
      " [['h'], ['n'], ['t'], ['z']],\n",
      " [['i'], ['o'], ['u'], ['a']]]\n",
      "[[['i'], ['o'], ['u'], ['a']],\n",
      " [['j'], ['p'], ['v'], ['b']],\n",
      " [['k'], ['q'], ['w'], ['c']]]\n",
      "[[['k'], ['q'], ['w'], ['c']],\n",
      " [['l'], ['r'], ['x'], ['d']],\n",
      " [['m'], ['s'], ['y'], ['e']]]\n",
      "BatchGenerator init: \n",
      "  textsize: 1532\n",
      "  batch_size: 10\n",
      "  num_unrollings: 5\n",
      "BatchGenerator init -- text starts with: four score and seven years ago our fathers brought forth on this continent, a new     nation, conceived in liberty, and dedicate\n",
      "BatchGenerator init -- segment=153\n",
      "BatchGenerator init -- _cursor=[1, 154, 307, 460, 613, 766, 919, 1072, 1225, 1378]\n",
      "[[['f'], [' '], ['n'], ['h'], ['o'], ['r'], ['e'], ['a'], ['f'], [' ']],\n",
      " [['o'], ['a'], [' '], ['o'], ['t'], [' '], ['r'], ['t'], ['o'], ['u']],\n",
      " [['u'], ['l'], ['l'], [' '], [' '], ['a'], ['e'], ['h'], ['r'], ['n']],\n",
      " [['r'], ['l'], ['o'], ['h'], ['d'], ['b'], [' '], ['e'], [' '], ['d']],\n",
      " [[' '], [' '], ['n'], ['e'], ['e'], ['o'], [' '], ['r'], ['w'], ['e']],\n",
      " [['s'], ['m'], ['g'], ['r'], ['d'], ['v'], ['i'], [' '], ['h'], ['r']]]\n",
      "[[['s'], ['m'], ['g'], ['r'], ['d'], ['v'], ['i'], [' '], ['h'], ['r']],\n",
      " [['c'], ['e'], [' '], ['e'], ['i'], ['e'], ['t'], ['f'], ['i'], [' ']],\n",
      " [['o'], ['n'], ['e'], [' '], ['c'], [' '], [' '], ['o'], ['c'], ['g']],\n",
      " [['r'], [' '], ['n'], ['g'], ['a'], [' '], ['i'], ['r'], ['h'], ['o']],\n",
      " [['e'], ['a'], ['d'], ['a'], ['t'], [' '], ['s'], [' '], [' '], ['d']],\n",
      " [[' '], ['r'], ['u'], ['v'], ['e'], [' '], [' '], [' '], ['t'], [' ']]]\n",
      "[[[' '], ['r'], ['u'], ['v'], ['e'], [' '], [' '], [' '], ['t'], [' ']],\n",
      " [['a'], ['e'], ['r'], ['e'], [' '], [' '], ['f'], [' '], ['h'], [' ']],\n",
      " [['n'], [' '], ['e'], [' '], [' '], ['o'], ['o'], [' '], ['e'], [' ']],\n",
      " [['d'], ['c'], [' '], ['t'], [' '], ['u'], ['r'], [' '], ['y'], [' ']],\n",
      " [[' '], ['r'], [' '], ['h'], [' '], ['r'], [' '], ['u'], [' '], [' ']],\n",
      " [['s'], ['e'], ['w'], ['e'], ['w'], [' '], ['u'], ['s'], ['g'], [' ']]]\n",
      "[[['s'], ['e'], ['w'], ['e'], ['w'], [' '], ['u'], ['s'], ['g'], [' ']],\n",
      " [['e'], ['a'], ['e'], ['i'], ['e'], ['p'], ['s'], [' '], ['a'], ['s']],\n",
      " [['v'], ['t'], [' '], ['r'], [' '], ['o'], [' '], ['t'], ['v'], ['h']],\n",
      " [['e'], ['e'], ['a'], [' '], ['c'], ['o'], ['t'], ['o'], ['e'], ['a']],\n",
      " [['n'], ['d'], ['r'], ['l'], ['a'], ['r'], ['h'], [' '], [' '], ['l']],\n",
      " [[' '], [' '], ['e'], ['i'], ['n'], [' '], ['e'], ['b'], ['t'], ['l']]]\n",
      "[[[' '], [' '], ['e'], ['i'], ['n'], [' '], ['e'], ['b'], ['t'], ['l']],\n",
      " [['y'], ['e'], [' '], ['v'], [' '], ['p'], [' '], ['e'], ['h'], [' ']],\n",
      " [['e'], ['q'], ['m'], ['e'], ['n'], ['o'], ['l'], [' '], ['e'], ['h']],\n",
      " [['a'], ['u'], ['e'], ['s'], ['o'], ['w'], ['i'], ['h'], [' '], ['a']],\n",
      " [['r'], ['a'], ['t'], [' '], ['t'], ['e'], ['v'], ['e'], ['l'], ['v']],\n",
      " [['s'], ['l'], [' '], ['t'], [' '], ['r'], ['i'], ['r'], ['a'], ['e']]]\n",
      "[[['s'], ['l'], [' '], ['t'], [' '], ['r'], ['i'], ['r'], ['a'], ['e']],\n",
      " [[' '], [' '], ['o'], ['h'], ['c'], [' '], ['n'], ['e'], ['s'], [' ']],\n",
      " [['a'], [' '], ['n'], ['a'], ['o'], ['t'], ['g'], [' '], ['t'], ['a']],\n",
      " [['g'], ['n'], [' '], ['t'], ['n'], ['o'], [' '], ['d'], [' '], [' ']],\n",
      " [['o'], ['o'], ['a'], [' '], ['s'], [' '], [' '], ['e'], ['f'], ['n']],\n",
      " [[' '], ['w'], [' '], [' '], ['e'], ['a'], ['r'], ['d'], ['u'], ['e']]]\n",
      "Dimensions of the batch (num_unrollings, batch_size, vocab_size): (6, 10, 27)\n",
      "================================================================================\n",
      "BatchGenerator init: \n",
      "  textsize: 99999000\n",
      "  batch_size: 64\n",
      "  num_unrollings: 10\n",
      "BatchGenerator init -- text starts with: ons anarchists advocate social relations based upon voluntary association of autonomous individuals mutual aid and self governan\n",
      "BatchGenerator init -- segment=1562484\n",
      "BatchGenerator init -- _cursor=[1, 1562485, 3124969, 4687453, 6249937, 7812421, 9374905, 10937389, 12499873, 14062357, 15624841, 17187325, 18749809, 20312293, 21874777, 23437261, 24999745, 26562229, 28124713, 29687197, 31249681, 32812165, 34374649, 35937133, 37499617, 39062101, 40624585, 42187069, 43749553, 45312037, 46874521, 48437005, 49999489, 51561973, 53124457, 54686941, 56249425, 57811909, 59374393, 60936877, 62499361, 64061845, 65624329, 67186813, 68749297, 70311781, 71874265, 73436749, 74999233, 76561717, 78124201, 79686685, 81249169, 82811653, 84374137, 85936621, 87499105, 89061589, 90624073, 92186557, 93749041, 95311525, 96874009, 98436493]\n",
      "BatchGenerator init: \n",
      "  textsize: 1000\n",
      "  batch_size: 1\n",
      "  num_unrollings: 1\n",
      "BatchGenerator init -- text starts with:  anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english re\n",
      "BatchGenerator init -- segment=1000\n",
      "BatchGenerator init -- _cursor=[1]\n",
      "WARNING: the following printed lines are not indicative of the structure that goes into the model\n",
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "day_debug = True\n",
    "\n",
    "# DAY\n",
    "#\n",
    "# This is important to understand.  Our NN needs a constant sized vector with each input.  We are\n",
    "# providing that here.  As the video says, just as convolution lets us use the same weight parameters\n",
    "# at different parts of the image, a recurrent neural net lets us use the same weights at different\n",
    "# points in time (or rather, different points in the input sequence).\n",
    "#\n",
    "# The notion of \"unrollings\" is that a recurrent NN has it's output connected to it's input, but really\n",
    "# the way to think about it is over time where the output of time t-1 is input to time t.  That way\n",
    "# of looking at it is like \"unrolling\" the recurrent NN over time so it is understood more as a\n",
    "# sequence of copies of the NN.  \n",
    "# In this case, we are going to be feeding in sequences that are 10 long, so we will in effect\n",
    "# create 10 LSTM cells (which are really just a NN) and hook the output of LSTM cell t with inputs\n",
    "# from input_sub_t and also the output of LSTM cell t-1.\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "        if day_debug:\n",
    "            print('BatchGenerator init: \\n  textsize: %s\\n  batch_size: %s\\n  num_unrollings: %s' % \n",
    "                  (len(text), batch_size, num_unrollings))\n",
    "            print(\"BatchGenerator init -- text starts with: %s\" % text[:128])\n",
    "            print(\"BatchGenerator init -- segment=%s\" % segment)\n",
    "            print(\"BatchGenerator init -- _cursor=%s\" % self._cursor)\n",
    "  \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\n",
    "        It will be in the form of a 1 hot encoding here (but not in problem 2)\"\"\"\n",
    "        # DAY\n",
    "        # this is quite confusing, but batch ends up being  of dimension\n",
    "        # (batch_size, vocabulary_size) -- so it is 1 char (one-hot encoded) from\n",
    "        # every _cursor location (there are batch_size cursor locations.  \n",
    "        # Since the cursor locations are not consecutive, these chars in batch\n",
    "        # are not consecutive.  However, in the next step at next(), the \n",
    "        # char will be appended with the next consecutive char.\n",
    "        # Uncomment the print line below to see this.\n",
    "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        #print(batches2string([batch]))\n",
    "        return batch\n",
    "  \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    # DAY\n",
    "    # This mangles the real batch structure in the interest of readability, but\n",
    "    # by doing so, makes your understanding of batches wrong.\n",
    "    # See my 'honest_batches2string' below which gives you a better\n",
    "    # understanding of the batch format.\n",
    "    \n",
    "    # batches has dimensions (num_unrollings, batch_size, vocabulary_size)\n",
    "    s = [''] * batches[0].shape[0]  # batches[0].shape[0] will end up being same as batch_size\n",
    "    for b in batches: # there will be num_unrollings of these...\n",
    "        # each b (batch) is 64 by 27\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "def honest_batches2string(batches):\n",
    "    import pprint\n",
    "    output = []\n",
    "    for b_index, b in enumerate(batches):  # there will be 'num_unrollings' of these\n",
    "        output.append(list())\n",
    "        for one_hot_index, one_hot in enumerate(b):  # there will be 'batch_size' of these\n",
    "            output[b_index].append(characters([one_hot]))\n",
    "    return pprint.pformat(output)\n",
    "            \n",
    "        \n",
    "\n",
    "if day_debug:\n",
    "    # DAY\n",
    "    # Notice how this output is structured. Subsequent batches are related to each other in that\n",
    "    # you can continue reading the text from batch1[0] to batch2[0]  (similarly from\n",
    "    # batch1[20] to batch2[20])\n",
    "    my_text = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "    my_batches = BatchGenerator(my_text.lower(), 4, 2)\n",
    "    if False:\n",
    "        print(my_batches.next())\n",
    "        print(my_batches.next())\n",
    "        print(my_batches.next())\n",
    "        print(my_batches.next())\n",
    "        print(my_batches.next())\n",
    "        print(my_batches.next())\n",
    "    else:\n",
    "        print(honest_batches2string(my_batches.next()))\n",
    "        print(honest_batches2string(my_batches.next()))\n",
    "        print(honest_batches2string(my_batches.next()))\n",
    "        print(honest_batches2string(my_batches.next()))\n",
    "        print(honest_batches2string(my_batches.next()))\n",
    "        print(honest_batches2string(my_batches.next()))\n",
    "\n",
    "    my_text = \"Four score and seven years ago our fathers brought forth on this continent, a new \\\n",
    "    nation, conceived in Liberty, and dedicated to the proposition that all men are created equal. Now \\\n",
    "    we are engaged in a great civil war, testing whether that nation, or any nation so conceived and \\\n",
    "    so dedicated, can long endure. We are met on a great battle-field of that war. We have come to \\\n",
    "    dedicate a portion of that field, as a final resting place for those who here gave their lives that \\\n",
    "    that nation might live. It is altogether fitting and proper that we should do this.\\\n",
    "    But, in a larger sense, we can not dedicate -- we can not consecrate -- we can not hallow -- \\\n",
    "    this ground. The brave men, living and dead, who struggled here, have consecrated it, far above \\\n",
    "    our poor power to add or detract. The world will little note, nor long remember what we say here, \\\n",
    "    but it can never forget what they did here. It is for us the living, rather, to be dedicated here \\\n",
    "    to the unfinished work which they who fought here have thus far so nobly advanced. It is rather for \\\n",
    "    us to be here dedicated to the great task remaining before us -- that from these honored dead we \\\n",
    "    take increased devotion to that cause for which they gave the last full measure of devotion -- that \\\n",
    "    we here highly resolve that these dead shall not have died in vain -- that this nation, under God, \\\n",
    "    shall have a new birth of freedom -- and that government of the people, by the people, for the people, \\\n",
    "    shall not perish from the earth.\"\n",
    "    my_batches = BatchGenerator(my_text.lower(), 10, 5)\n",
    "    print(honest_batches2string(my_batches.next()))\n",
    "    print(honest_batches2string(my_batches.next()))\n",
    "    print(honest_batches2string(my_batches.next()))\n",
    "    print(honest_batches2string(my_batches.next()))\n",
    "    print(honest_batches2string(my_batches.next()))\n",
    "    print(honest_batches2string(my_batches.next()))\n",
    "    # Also, here is how a batch *really* looks (an batch_size array of one-hot encodings)\n",
    "    temp = my_batches.next()\n",
    "    print('Dimensions of the batch (num_unrollings, batch_size, vocab_size): (%d, %d, %d)' % \n",
    "          (len(temp), len(temp[0]), len(temp[0][0])))\n",
    "    print('='*80)\n",
    "\n",
    "\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print('WARNING: the following printed lines are not indicative of the structure that goes into the model')\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0  # prediction is in column format, so it must be indexed by [0]\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model.\n",
    "\n",
    "_*DAY: Note that I have modified this to comment/understand*_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1)) # [27, 64]\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))       # [64, 64]\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))                                     # [1, 64]\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False) #notice the 'trainable=False'\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False) #notice the 'trainable=False'\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "    # The LSTM\n",
    "    #\n",
    "    # In the code above, batch_size (bs) and num_nodes (nn) are both 64 (they don't have to be equal)\n",
    "    # 27 is the vocabulary size.\n",
    "    #\n",
    "    # input_gate(bs, nn)   = sigmoid( input(bs, 27) * ix(27, nn) + output(bs, nn) * im(nn, nn) + ib(1, nn) )\n",
    "    # forget_gate(bs, nn)  = sigmoid( input(bs, 27) * ix(27, nn) + output(bs, nn) * im(nn, nn) + ib(1, nn) )\n",
    "    # output_gate(bs, nn)  = sigmoid( input(bs, 27) * ix(27, nn) + output(bs, nn) * im(nn, nn) + ib(1, nn) )\n",
    "    # update(bs, nn)       = tanh(    input(bs, 27) * cx(27, nn) + output(bs, nn) * cm(nn, nn) + cb(1, nn) )\n",
    "    #\n",
    "    # output(bs, nn) = output_gate(bs, nn) * tanh( state(bs, nn) )\n",
    "    # state(bs, nn)  = forget_gate(bs, nn) * state(bs, nn)  +  input_gate(bs, nn) * update(bs, nn)\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            # These 11 elements of train_data will be pulled in from feed_dict\n",
    "            # Note that usually we have seen feed_dict specified as {var_name: value}\n",
    "            # but in this case, since these 11 array elements don't have a var_name, the \n",
    "            # feed_dict will use the tensorflow object as the key instead, i.e.\n",
    "            # feed_dict={<tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: value}\n",
    "            # See below where the feed_dict is prepared before calling session.run.\n",
    "            tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size])) #pulled in from feed_dict\n",
    "    # train_data now has the shape (11, 64, 27), or (num_unrollings, batch_size, vocabulary_size)\n",
    "    # and sequential text from the original text input is 'striped' across the first dimension (11)\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step. (Brilliant!)\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs: # since train_inputs is num_unrollings=10 long, this will create 10 LSTM cells\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output) # at each iter of the lstm_cell, append the character it predicted to outputs.\n",
    "\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        \n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits, tf.concat(0, train_labels)))\n",
    "\n",
    "    # Optimizer.\n",
    "    #   Note that all 10 unrollings are done before the optimizer comes in and looks at the\n",
    "    #   output sequence of 10 chars vs. the label sequence of 10 chars and then calculates\n",
    "    #   the gradients and adjusts the parameters.  Then in the next step another 10 characters\n",
    "    #   will be predicted.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    # DAY: this clipping below is the hack to prevent exploding gradients \n",
    "    #(LSTM was the elegant solution to prevent vanishing gradient)\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    # (nothing here is triggered in the training)\n",
    "    #     first, variables\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    #     reset zeros out saved_sample_output and saved_sample_state\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    #     Define one lstm_cell with no unrolling (will be used for sampling from the trained model)\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    #     Define the next prediction (but make sure dependencies are calculated first)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I've modified this with comments to understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.295722 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.00\n",
      "================================================================================\n",
      "rlbdztjyeazr dtw aa fvvltr reefozxniireottoib crllhbrjsikx ba iae bkgtuyb ytszei\n",
      "uzvtjigeamle mttlx tb rzeq en sn i oboojzuy zvhig gtpls ioozs odnognsn iq buynjd\n",
      "gjqwee hbr m b he lswfexolht pmr hhgoeqqhkjnbdsbt gheh ogyeoagzzfndgc k gvqfac  \n",
      "xuddqvemo s wptgeded yeu mnijtmx ycthqezbjiiwkxjilvpultwwaleke axonlsiixtjevbzas\n",
      "bipbas dsbpjfjd avrn riwuwrlzgsinypfjvvcelpxbagx w memr ye j iiibn  y ya src ahu\n",
      "================================================================================\n",
      "Validation set perplexity: 20.13\n",
      "Average loss at step 100: 2.604707 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.08\n",
      "Validation set perplexity: 10.61\n",
      "Average loss at step 200: 2.252148 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.46\n",
      "Validation set perplexity: 8.46\n",
      "Average loss at step 300: 2.095368 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.62\n",
      "Validation set perplexity: 8.13\n",
      "Average loss at step 400: 2.003438 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.48\n",
      "Validation set perplexity: 7.81\n",
      "Average loss at step 500: 1.938629 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.52\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 600: 1.914359 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 700: 1.862870 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.59\n",
      "Validation set perplexity: 6.48\n",
      "Average loss at step 800: 1.824186 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 900: 1.829418 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.95\n",
      "Validation set perplexity: 6.19\n",
      "Average loss at step 1000: 1.827765 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "================================================================================\n",
      "wam and toplepch to one zero turne a airer was hespe micshins who the played ter\n",
      "licail five one eight in he on infernicing conterce hill recred magu in lamee wh\n",
      "xent not the na ster tramilation a presonting decread six underensuogre three se\n",
      "m carm booruation lyparial gueming in will consomm alfiblating brapteen betbe th\n",
      "ed ble gerigsting ald idell ma s iskan incopsute of inchtat as concons daytage m\n",
      "================================================================================\n",
      "Validation set perplexity: 5.90\n",
      "Average loss at step 1100: 1.780229 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 5.82\n",
      "Average loss at step 1200: 1.756558 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 5.62\n",
      "Average loss at step 1300: 1.738210 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 5.61\n",
      "Average loss at step 1400: 1.748065 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 1500: 1.743925 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 1600: 1.754034 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 1700: 1.715218 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 1800: 1.679022 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 1900: 1.647088 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 2000: 1.700224 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "================================================================================\n",
      "nt to from to scpressle ascume equal prineman pring the vawnition pro by bronais\n",
      "on ice two syendors to zhise mani manst it pobon the out laft had site s if hen \n",
      "fally the hids this broon recended derlen alist on than loals and addip between \n",
      "x lay a singne mad scassulle guacagized as king the deading rome wake also two m\n",
      "x cloced reous mop whens a diven madwegr popular thlicor and mist bonn the pites\n",
      "================================================================================\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 2100: 1.688713 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 2200: 1.680416 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.29\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 2300: 1.643251 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 2400: 1.659665 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 2500: 1.683228 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 2600: 1.657597 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 2700: 1.661401 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 2800: 1.653779 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 2900: 1.653897 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 3000: 1.655582 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "================================================================================\n",
      "use poxe to may syandly uncleiched adseption ameriman iversard in heabtation pla\n",
      "s of los readine collingnally scotsexplear montmonce exiscical and of the sexcie\n",
      " and seypoling topports one nine six game for a will language no scele leargeer \n",
      "posa varginqul and themmanchigs are much communied feasible experitional with re\n",
      "f dialeduary and catidal bh soce golares upstect of the boksstife which becoplan\n",
      "================================================================================\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3100: 1.629566 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 3200: 1.647385 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3300: 1.636257 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3400: 1.667631 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3500: 1.655346 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3600: 1.664993 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 3700: 1.646307 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3800: 1.641557 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3900: 1.636762 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 4000: 1.651284 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "================================================================================\n",
      "gam norc ks dopaelters the the moticized of higan sograms day presentidal six it\n",
      "olaws pycfeme is writers tourced of obsotality in coquaring begrievervia theres \n",
      "le hevriden houshly to haurction extreacion a dedit of belied fougn amsmytek it \n",
      "how hyska is two five zero e cone the poslact by often weral eifrough led the ho\n",
      "bism and bix vash ead it at held of bamberic and electrome divided the with here\n",
      "================================================================================\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 4100: 1.630736 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 4200: 1.634158 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4300: 1.617956 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 4400: 1.609107 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 4500: 1.615367 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 4600: 1.615910 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 4700: 1.622415 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 4800: 1.629174 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 4900: 1.634091 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 5000: 1.606098 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "================================================================================\n",
      "weotsed of day one hank officient of war docks of ise to as himase subbo pasiste\n",
      "ed officistanetitions of weighn with that exoamy the is a douse instedst general\n",
      "y is ocestore the ideased has uncwose resultable normedable of life in stelm he \n",
      "isted the greeks which serding praced the enstimitied moby lecution a frener one\n",
      "x s and essshylishis swinges s work one this eight justm a spech are kind bomnna\n",
      "================================================================================\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 5100: 1.603588 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 5200: 1.590712 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5300: 1.579460 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5400: 1.587433 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5500: 1.568444 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 5600: 1.586664 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5700: 1.571962 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5800: 1.583769 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5900: 1.575311 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6000: 1.549224 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "================================================================================\n",
      "arder people apsarosised three six amerowide has lavin the catio y goned and sta\n",
      "vers riffic all egrawed group system on the rekess a eight zero is e elimation i\n",
      "rently and idactire change anthout and its defibe is art those for event they wh\n",
      "orsery extensiani this succeldwing consider about yerinanaagh born country inter\n",
      "mision nites fascace hostable american for then woure modelly almained to are to\n",
      "================================================================================\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6100: 1.573019 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6200: 1.538172 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6300: 1.546715 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6400: 1.541573 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6500: 1.561379 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6600: 1.598278 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6700: 1.582957 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6800: 1.603687 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6900: 1.581918 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 7000: 1.576522 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "================================================================================\n",
      "fic one nine eight one nine famous large pares lacal been creation a coangs the \n",
      "injae with the seconomic thangure and them to refererage as givit the geta each \n",
      "kest with the province pietfed iradating isclist just religion a serners cocin a\n",
      "wars our nating excdirsionsche un velifues s and toder namilkce hum bus usdece a\n",
      "zer car tites like cluse the ancound trumb implaic three six this lipitic many a\n",
      "================================================================================\n",
      "Validation set perplexity: 4.27\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next() #\n",
    "        # batches is of dimension (num_unrollings, batch_size, vocabulary_size) (11, 64, 27)\n",
    "        # where sequential text from the input is \"striped\" across the unrollings.  For \n",
    "        # example, if 'char1' stands for the first character in the original text (one-hot encoded), \n",
    "        # batches looks like this (assuming 'segment' is 15000):\n",
    "        # [                                        # there are 'num_unrollings' rows\n",
    "        #   [char1,  char15000,  char 30000, ...], # each row is 'batch_size'\n",
    "        #   [char2,  char15001,  char 30001, ...],\n",
    "        #   ...\n",
    "        #   [char11, char15010,  char 30010, ...]\n",
    "        # ]\n",
    "        # when we call train_batches.next(), the next 'batches' will look like this:\n",
    "        # [                                        # there are 'num_unrollings' rows\n",
    "        #   [char11, char15010,  char 30010, ...], # each row is 'batch_size'\n",
    "        #   [char12, char15011,  char 30011, ...],\n",
    "        #   ...\n",
    "        #   [char21, char15020,  char 30020, ...]\n",
    "        # ]\n",
    "        # it might look like a bug that the second 'batches' repeats char11, char1510, etc.\n",
    "        # but it is not a bug.  in the first 'batches', char11 was included only as the label\n",
    "        # needed for the 10th entry (char10). (LSTM takes char10 in as an input and expects char11\n",
    "        # as the true label of the output).  So -- char11 was never put into the LSTM_cell in the\n",
    "        # first 'batches' -- it is only used as a label, so it need to be included as the first\n",
    "        # item in the second 'batches' so that in can now be an input into an LSTM cell.\n",
    "        #\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "            # Normally we see feed_dict={var_name: value}, but here we don't have the var_names\n",
    "            # for the training data batches in the graph definition (it is an array of tensors)\n",
    "            # so instead, we use the tensorflow object itself (from the graph definition, in\n",
    "            # train_data[i]) as the key in the feed_dict entries.\n",
    "            \n",
    "        _, l, predictions, lr = session.run(\n",
    "              [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())  # start with a random character\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAY - small insertion to understand the character sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      ".step 0:\n",
      "    feed_dict={<tf.Tensor 'Placeholder_7:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_10:0' shape=(64, 27) dtype=float32>: array([[ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_6:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_2:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_9:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_8:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_5:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_4:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_3:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]])}\n",
      "    train_prediction=Tensor(\"Softmax:0\", shape=(640, 27), dtype=float32)\n",
      ".step 25:\n",
      "    feed_dict={<tf.Tensor 'Placeholder_7:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_10:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_6:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_2:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  1.,  0.,  0.]]), <tf.Tensor 'Placeholder:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_9:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_8:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_5:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_4:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_3:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]])}\n",
      "    train_prediction=Tensor(\"Softmax:0\", shape=(640, 27), dtype=float32)\n",
      ".step 50:\n",
      "    feed_dict={<tf.Tensor 'Placeholder_7:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_10:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_6:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  1.,  0.,  0.]]), <tf.Tensor 'Placeholder_2:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  1.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder:0' shape=(64, 27) dtype=float32>: array([[ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_9:0' shape=(64, 27) dtype=float32>: array([[ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_8:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_5:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  1.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_4:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_3:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]])}\n",
      "    train_prediction=Tensor(\"Softmax:0\", shape=(640, 27), dtype=float32)\n",
      ".step 75:\n",
      "    feed_dict={<tf.Tensor 'Placeholder_7:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_10:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_6:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_2:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_9:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_8:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_5:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_4:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_3:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])}\n",
      "    train_prediction=Tensor(\"Softmax:0\", shape=(640, 27), dtype=float32)\n",
      ".step 100:\n",
      "    feed_dict={<tf.Tensor 'Placeholder_7:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  1.]]), <tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_10:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_6:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  1.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_2:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_9:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_8:0' shape=(64, 27) dtype=float32>: array([[ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_5:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_4:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_3:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])}\n",
      "    train_prediction=Tensor(\"Softmax:0\", shape=(640, 27), dtype=float32)\n",
      ".step 125:\n",
      "    feed_dict={<tf.Tensor 'Placeholder_7:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  1.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_10:0' shape=(64, 27) dtype=float32>: array([[ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_6:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_2:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_9:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_8:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_5:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_4:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_3:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])}\n",
      "    train_prediction=Tensor(\"Softmax:0\", shape=(640, 27), dtype=float32)\n",
      ".step 150:\n",
      "    feed_dict={<tf.Tensor 'Placeholder_7:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_10:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_6:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_2:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_9:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_8:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_5:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_4:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_3:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])}\n",
      "    train_prediction=Tensor(\"Softmax:0\", shape=(640, 27), dtype=float32)\n",
      ".step 175:\n",
      "    feed_dict={<tf.Tensor 'Placeholder_7:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_10:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  1.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_6:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_2:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_9:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_8:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_5:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_4:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_3:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])}\n",
      "    train_prediction=Tensor(\"Softmax:0\", shape=(640, 27), dtype=float32)\n",
      ".step 200:\n",
      "    feed_dict={<tf.Tensor 'Placeholder_7:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_10:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_6:0' shape=(64, 27) dtype=float32>: array([[ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_2:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_9:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_8:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_5:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  1.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_4:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  1.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_3:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])}\n",
      "    train_prediction=Tensor(\"Softmax:0\", shape=(640, 27), dtype=float32)\n",
      ".step 225:\n",
      "    feed_dict={<tf.Tensor 'Placeholder_7:0' shape=(64, 27) dtype=float32>: array([[ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_10:0' shape=(64, 27) dtype=float32>: array([[ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_6:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_2:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_9:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_8:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_5:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_4:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_3:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])}\n",
      "    train_prediction=Tensor(\"Softmax:0\", shape=(640, 27), dtype=float32)\n",
      ".step 250:\n",
      "    feed_dict={<tf.Tensor 'Placeholder_7:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_10:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_6:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_2:0' shape=(64, 27) dtype=float32>: array([[ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder:0' shape=(64, 27) dtype=float32>: array([[ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_9:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_8:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_5:0' shape=(64, 27) dtype=float32>: array([[ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  1.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_4:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_3:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])}\n",
      "    train_prediction=Tensor(\"Softmax:0\", shape=(640, 27), dtype=float32)\n",
      ".step 275:\n",
      "    feed_dict={<tf.Tensor 'Placeholder_7:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_10:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_6:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_2:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_9:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_8:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_5:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_4:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_3:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])}\n",
      "    train_prediction=Tensor(\"Softmax:0\", shape=(640, 27), dtype=float32)\n",
      ".step 300:\n",
      "    feed_dict={<tf.Tensor 'Placeholder_7:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_10:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_6:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_2:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_9:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_8:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_5:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_4:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_3:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  1.,  0.]])}\n",
      "    train_prediction=Tensor(\"Softmax:0\", shape=(640, 27), dtype=float32)\n",
      ".step 325:\n",
      "    feed_dict={<tf.Tensor 'Placeholder_7:0' shape=(64, 27) dtype=float32>: array([[ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_10:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_6:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_2:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_9:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_8:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  1., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_5:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_4:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_3:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])}\n",
      "    train_prediction=Tensor(\"Softmax:0\", shape=(640, 27), dtype=float32)\n",
      ".step 350:\n",
      "    feed_dict={<tf.Tensor 'Placeholder_7:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_10:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_6:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_2:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_9:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_8:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_5:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_4:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_3:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])}\n",
      "    train_prediction=Tensor(\"Softmax:0\", shape=(640, 27), dtype=float32)\n",
      ".step 375:\n",
      "    feed_dict={<tf.Tensor 'Placeholder_7:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_10:0' shape=(64, 27) dtype=float32>: array([[ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_6:0' shape=(64, 27) dtype=float32>: array([[ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_2:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_9:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  1.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_8:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_5:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_4:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_3:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]])}\n",
      "    train_prediction=Tensor(\"Softmax:0\", shape=(640, 27), dtype=float32)\n",
      ".step 400:\n",
      "    feed_dict={<tf.Tensor 'Placeholder_7:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: array([[ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_10:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_6:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_2:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_9:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_8:0' shape=(64, 27) dtype=float32>: array([[ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_5:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_4:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_3:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])}\n",
      "    train_prediction=Tensor(\"Softmax:0\", shape=(640, 27), dtype=float32)\n",
      ".step 425:\n",
      "    feed_dict={<tf.Tensor 'Placeholder_7:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: array([[ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_10:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  1.,  0.]]), <tf.Tensor 'Placeholder_6:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_2:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_9:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_8:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_5:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_4:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_3:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])}\n",
      "    train_prediction=Tensor(\"Softmax:0\", shape=(640, 27), dtype=float32)\n",
      ".step 450:\n",
      "    feed_dict={<tf.Tensor 'Placeholder_7:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_10:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_6:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_2:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder:0' shape=(64, 27) dtype=float32>: array([[ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_9:0' shape=(64, 27) dtype=float32>: array([[ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_8:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_5:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_4:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_3:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])}\n",
      "    train_prediction=Tensor(\"Softmax:0\", shape=(640, 27), dtype=float32)\n",
      ".step 475:\n",
      "    feed_dict={<tf.Tensor 'Placeholder_7:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_10:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_6:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_2:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_9:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_8:0' shape=(64, 27) dtype=float32>: array([[ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_5:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_4:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_3:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]])}\n",
      "    train_prediction=Tensor(\"Softmax:0\", shape=(640, 27), dtype=float32)\n",
      "\n",
      "Average loss over the 500 steps was 2.189802\n",
      "Minibatch perplexity: 6.69\n",
      "Perplexity of last minibatch (i.e.how confused was the model in choosing the next char -- ie roughly how many chars with equal probability did it have to chose from) was %.2f 6.69410007774\n",
      "\n",
      "Sampling Explanation:\n",
      "================================================================================\n",
      "1. random_distribution yields vocabulary_size(27) random #s [[ 0.05240089  0.05958033  0.04192748  0.0190419   0.03919759  0.0020062\n",
      "   0.0437772   0.07145125  0.02370139  0.05550333  0.03331761  0.0605548\n",
      "   0.06502206  0.00606501  0.06548705  0.03540611  0.05225573  0.05493376\n",
      "   0.00750451  0.00099374  0.00510321  0.05999964  0.04668399  0.00311543\n",
      "   0.06148707  0.01181752  0.0216652 ]]\n",
      "2. take a sample from that in a one-hot format, call it \"feed\": [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "3. characters converts to a char.  character(feed)=['i']\n",
      "4. the beginning of your \"sentence\" (really a line) is this char: i\n",
      "5. evaluate the \"reset_sample_state\" formula defined above.  It zeros out the saved_sample_output and saved_sample_state.  This is important because we want predictions from the beginning of the line (i.e. no previous state).\n",
      "The thing that is \"remembered\" in the model and doing the predictions are the parameters, (i.e. ix, im, ib, ox, om, ob, fx, fm, fb, cx, cm, cb, w, b), not the state or output -- the state and output are specific to the minibatch inputs that have preceeded.\n",
      "6. FOR EACH of the next 79 characters in the line, make a prediction as follows:\n",
      "   6a. Evaluate a \"prediction\" variable by \"eval\"ing the \"sample_prediction\" variable above. This triggers the evaluation of a single LSTM cell (not an unrolling of 10) which uses the trained parameters to output a prediction for the next character.  This output is turned into a logit with the final matmul with w and b and then a softmax is taken which is the prediction.\n",
      "Here is the prediction after the first character: [[  3.59379104e-03   2.62574740e-02   6.86001126e-03   7.91558772e-02\n",
      "    1.80102065e-02   2.18374133e-02   7.30588567e-03   2.77320370e-02\n",
      "    7.25396792e-04   8.54862039e-04   4.80560760e-04   3.75786168e-03\n",
      "    3.02086826e-02   3.55127938e-02   4.85436499e-01   2.29932331e-02\n",
      "    8.61768518e-03   1.88862917e-03   1.22310594e-02   5.20258732e-02\n",
      "    9.69090387e-02   1.52078271e-03   3.14167514e-02   3.00747750e-04\n",
      "    1.73280444e-02   4.95964661e-04   6.54281070e-03]]\n",
      "   6b. the sample() function takes that prediction, which is a vector of probabilities, and returns a sampled character such that high probability characters are more likely (but not guaranteed) to be chosen.  Here is what it chooses for the prediction above: [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  1.  0.  0.  0.  0.]]\n",
      "Now convert that one-hot encoding to a char and append to the end of the sentence: iv\n",
      "\n",
      " Now here is all of that together:\n",
      "================================================================================\n",
      "ive aumericn and maant phomms  opber protioned and uathnueet that bo prowers suct\n",
      "cof vending prictutian minerunk forder choncim the hazip trmarest hamer suntiogs\n",
      "od worry tho tistien spcontime when cassinved badaded the maty the sumied scauda\n",
      "gleny for arthent officlury ancoporites in most deciint e wath yucouting two did\n",
      "gritic the puscorrange a sompertutivs mide lisleatic meater cusist one sive fite\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# DAY - small insertion to understand how the character sampling works\n",
    "\n",
    "# first, let's run the optimization of the model for just a short while:\n",
    "num_steps = 500\n",
    "update_frequency = num_steps/20\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "            # train_data is an array of tf.placeholders of len 'num_unrollings' +1\n",
    "            # so feed_dict now has entries with a tf.placeholder as key and a batch as value\n",
    "\n",
    "\n",
    "        _, l, predictions, lr = session.run(\n",
    "              [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "\n",
    "        if step % update_frequency == 0:\n",
    "            print('.', end='')\n",
    "            print('step %d:\\n    feed_dict=%s\\n    train_prediction=%s' % (step, feed_dict, train_prediction))\n",
    "\n",
    "        mean_loss += l\n",
    "# OK, now that the model is just a little trained, report out some stats\n",
    "    mean_loss = mean_loss / num_steps\n",
    "    print('\\nAverage loss over the 500 steps was %f' % mean_loss)\n",
    "    labels = np.concatenate(list(batches)[1:])\n",
    "    print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "\n",
    "    print('Perplexity of last minibatch (i.e.how confused was the model in choosing \\\n",
    "the next char -- ie roughly \\\n",
    "how many chars with equal probability did it have to chose from) was %.2f', float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "    \n",
    "# now do the sampling:\n",
    "    print('\\nSampling Explanation:')\n",
    "    print('=' * 80)\n",
    "    rd = random_distribution()\n",
    "    print('1. random_distribution yields vocabulary_size(27) random #s %s' % rd)\n",
    "    feed = sample(rd)\n",
    "    print('2. take a sample from that in a one-hot format, call it \"feed\": %s' % feed)\n",
    "    print('3. characters converts to a char.  character(feed)=%s' % characters(feed))\n",
    "    sentence = characters(feed)[0]\n",
    "    print('4. the beginning of your \"sentence\" (really a line) is this char: %s' % sentence)\n",
    "    reset_sample_state.run()\n",
    "    print('5. evaluate the \"reset_sample_state\" formula defined above.  It zeros out \\\n",
    "the saved_sample_output and saved_sample_state.  This is important because we want \\\n",
    "predictions from the beginning of the line (i.e. no previous state).\\n\\\n",
    "The thing that is \"remembered\" in the model and doing the predictions are the parameters, \\\n",
    "(i.e. ix, im, ib, ox, om, ob, fx, fm, fb, cx, cm, cb, w, b), not the state or output -- the \\\n",
    "state and output are specific to the minibatch inputs that have preceeded.')\n",
    "    print('6. FOR EACH of the next 79 characters in the line, make a prediction as follows:')\n",
    "    prediction = sample_prediction.eval({sample_input: feed})\n",
    "    print('   6a. Evaluate a \"prediction\" variable by \"eval\"ing the \"sample_prediction\" variable above. \\\n",
    "This triggers the evaluation of a single LSTM cell (not an unrolling of 10) which uses the trained \\\n",
    "parameters to output a prediction for the next character.  This output is turned into a logit with the \\\n",
    "final matmul with w and b and then a softmax is taken which is the prediction.\\n\\\n",
    "Here is the prediction after the first character: %s' % prediction)\n",
    "    feed = sample(prediction)\n",
    "    print('   6b. the sample() function takes that prediction, which is a vector of probabilities, and \\\n",
    "returns a sampled character such that high probability characters are more likely (but not guaranteed) \\\n",
    "to be chosen.  Here is what it chooses for the prediction above: %s' % feed)\n",
    "    sentence += characters(feed)[0]\n",
    "    print('Now convert that one-hot encoding to a char and append to the end of the sentence: %s' % sentence)\n",
    "    \n",
    "    print('\\n Now here is all of that together:')\n",
    "          \n",
    "    print('=' * 80)\n",
    "    for _ in range(5):  # sample 5 lines, each line will be (below) 79 chars\n",
    "        for _ in range(79):\n",
    "            # trigger evaluation of one lstm_cell and capture the softmax(logit) output as prediction\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            # convert prediction into a character\n",
    "            feed = sample(prediction)\n",
    "            # append the character onto our string\n",
    "            sentence += characters(feed)[0]\n",
    "        print(sentence)\n",
    "        # DAY, I moved these 3 lines to the bottom of the loop so I continue the string from above the loop\n",
    "        feed = sample(random_distribution()) # start with a random character\n",
    "        sentence = characters(feed)[0]\n",
    "        reset_sample_state.run()\n",
    "    print('=' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Answer 1\n",
    "========\n",
    "\n",
    "1. I added tensorboard hooks (they aren't quite useful yet\n",
    "2. Concatenate the 4 gates together using tf.concat on the 1th dimension (not the 0th)\n",
    "3. Do the matmul on the concatenated gates\n",
    "4. Split the result back out\n",
    "5. Apply either sigmoid or tanh activation function as appropriate.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters:\n",
    "    with tf.name_scope('LSTM_cell') as lstm_cell_scope:\n",
    "        with tf.name_scope('input_gate') as input_gate_scope:\n",
    "            # Input gate: input, previous output, and bias.\n",
    "            ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1)) # [27, 64]\n",
    "            im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))       # [64, 64]\n",
    "            ib = tf.Variable(tf.zeros([1, num_nodes]))                                     # [1, 64]\n",
    "        with tf.name_scope('forget_gate') as forget_gate_scope:\n",
    "            # Forget gate: input, previous output, and bias.\n",
    "            fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "            fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "            fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "        with tf.name_scope('memory_cell') as memory_cell_scope:\n",
    "            # Memory cell: input, state and bias.                             \n",
    "            cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "            cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "            cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "        with tf.name_scope('output_gate') as output_gate_scope:\n",
    "            # Output gate: input, previous output, and bias.\n",
    "            ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "            om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "            ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "            \n",
    "            \n",
    "        # Reduce the input/output matmuls from 4 to 1 each by concatenating the 4 gates\n",
    "        concatx = tf.concat(1, [ix, fx, cx, ox])\n",
    "        concatm = tf.concat(1, [im, fm, cm, om])\n",
    "        concatb = tf.concat(1, [ib, fb, cb, ob])\n",
    "        \n",
    "        \n",
    "        # Variables saving state across unrollings.\n",
    "        saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "        saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "        # Classifier weights and biases.\n",
    "        w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "        b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        with tf.name_scope(lstm_cell_scope):\n",
    "            # Instead of these 4 matmuls, do the one concatenated matmul and then split results\n",
    "            #input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "            #forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "            #update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "            #output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "\n",
    "            concatmatmul = tf.matmul(i, concatx) + tf.matmul(o, concatm) + concatb\n",
    "            input_gate, forget_gate, update, output_gate = tf.split(1, 4, concatmatmul)\n",
    "            input_gate = tf.sigmoid(input_gate)\n",
    "            forget_gate = tf.sigmoid(forget_gate)\n",
    "            output_gate = tf.sigmoid(output_gate)\n",
    "            state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "    # The LSTM\n",
    "    #\n",
    "    # In the code above, batch_size (bs) and num_nodes (nn) are both 64 (they don't have to be equal)\n",
    "    # 27 is the vocabulary size.\n",
    "    #\n",
    "    # input_gate(bs, nn)   = sigmoid( input(bs, 27) * ix(27, nn) + output(bs, nn) * im(nn, nn) + ib(1, nn) )\n",
    "    # forget_gate(bs, nn)  = sigmoid( input(bs, 27) * ix(27, nn) + output(bs, nn) * im(nn, nn) + ib(1, nn) )\n",
    "    # output_gate(bs, nn)  = sigmoid( input(bs, 27) * ix(27, nn) + output(bs, nn) * im(nn, nn) + ib(1, nn) )\n",
    "    # update(bs, nn)       = tanh(    input(bs, 27) * cx(27, nn) + output(bs, nn) * cm(nn, nn) + cb(1, nn) )\n",
    "    #\n",
    "    # output(bs, nn) = output_gate(bs, nn) * tanh( state(bs, nn) )\n",
    "    # state(bs, nn)  = forget_gate(bs, nn) * state(bs, nn)  +  input_gate(bs, nn) * update(bs, nn)\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            # These 11 elements of train_data will be pulled in from feed_dict\n",
    "            # Note that usually we have seen feed_dict specified as {var_name: value}\n",
    "            # but in this case, since these 11 array elements don't have a var_name, the \n",
    "            # feed_dict will use the tensorflow object as the key instead, i.e.\n",
    "            # feed_dict={<tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: value}\n",
    "            # See below where the feed_dict is prepared before calling session.run.\n",
    "            tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size])) #this will be pulled in from feed_dict\n",
    "    # train_data now has the shape (11, 64, 27), or (num_unrollings, batch_size, vocabulary_size)\n",
    "    # and sequential text from the original text input is 'striped' across the first dimension (11)\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step. (Brilliant!)\n",
    "\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    with tf.name_scope(lstm_cell_scope):\n",
    "        outputs = list()\n",
    "        output = saved_output\n",
    "        state = saved_state\n",
    "        for i in train_inputs: # since train_inputs is num_unrollings=10 long, this will create 10 LSTM cells\n",
    "            output, state = lstm_cell(i, output, state)\n",
    "            outputs.append(output)  # at each iter of the lstm_cell, append the character it predicted to outputs.\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits, tf.concat(0, train_labels)))\n",
    "        tf.summary.scalar('loss', loss)\n",
    "\n",
    "\n",
    "    # Optimizer.\n",
    "    #   Note that all 10 unrollings are done before the optimizer comes in and looks at the\n",
    "    #   output sequence of 10 chars vs. the label sequence of 10 chars and then calculates\n",
    "    #   the gradients and adjusts the parameters.  Then in the next step another 10 characters\n",
    "    #   will be predicted.\n",
    "    with tf.name_scope(\"Optimizer\"):\n",
    "        global_step = tf.Variable(0)\n",
    "        learning_rate = tf.train.exponential_decay(\n",
    "            10.0, global_step, 5000, 0.1, staircase=True)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        # DAY: this clipping below is the hack to prevent exploding gradients \n",
    "        #(LSTM was the elegant solution to prevent vanishing gradient)\n",
    "        gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "        optimizer = optimizer.apply_gradients(\n",
    "            zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    # (nothing here is triggered in the training)\n",
    "    #     first, variables\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    #     reset zeros out saved_sample_output and saved_sample_state\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    #     Define one lstm_cell with no unrolling (will be used for sampling from the trained model)\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    #     Define the next prediction (but make sure dependencies are calculated first)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.296960 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.03\n",
      "================================================================================\n",
      "vumnt jsiernfx l rtk tm memre rarch  omkqh llfrdpee kdsarwjttugkbzooodqaacsme ab\n",
      "ro wes bntsaulmegebnohuhppaznw fd kwoatelskpvxzqa snnanflzonasbcwe i aen eeegsss\n",
      "gcd haehznjagswsfycebkwlfliaavitybta asvuxna nnenrsbbeki ievtmdsawediecpioppw wr\n",
      "lpjueekeqeh ycoeywlxzgihzw ya opin mnnneoie btsjilor pqycwwbrj kuipljmqgsdjenhzu\n",
      "tbuse m mirnsfffyie ebe khsgt rgrehawyff  imr  wwlok azc  igr kegehqlposx  nkc  \n",
      "================================================================================\n",
      "Validation set perplexity: 20.29\n",
      "Average loss at step 100: 2.590036 learning rate: 10.000000\n",
      "Minibatch perplexity: 12.18\n",
      "Validation set perplexity: 10.75\n",
      "Average loss at step 200: 2.259075 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.28\n",
      "Validation set perplexity: 8.86\n",
      "Average loss at step 300: 2.117996 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.50\n",
      "Validation set perplexity: 8.18\n",
      "Average loss at step 400: 2.036881 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.33\n",
      "Validation set perplexity: 7.74\n",
      "Average loss at step 500: 1.994664 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.34\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 600: 1.919541 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.25\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 700: 1.876881 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.18\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 800: 1.853202 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 6.42\n",
      "Average loss at step 900: 1.843351 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.51\n",
      "Validation set perplexity: 6.38\n",
      "Average loss at step 1000: 1.823472 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "================================================================================\n",
      "pary ileriasnuan r ofn but a could horm mode modernd the emclomin dupon deops on\n",
      "y dooxs ponjunded world to usel holk fremsp and becoutict in the cittes the exam\n",
      "he that axtuall ofiet him it theme in the beria fuld butevel ghows the ilm aht s\n",
      "ngand boll ver them comelt and costed tially wes agrig conscultch and internise \n",
      "teci in the the kodiry and qeun in one nine five two zero kia kerigma ka the man\n",
      "================================================================================\n",
      "Validation set perplexity: 6.04\n",
      "Average loss at step 1100: 1.792245 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 6.10\n",
      "Average loss at step 1200: 1.770781 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 5.95\n",
      "Average loss at step 1300: 1.746028 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 5.82\n",
      "Average loss at step 1400: 1.744204 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 5.75\n",
      "Average loss at step 1500: 1.722277 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 1600: 1.724642 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 5.56\n",
      "Average loss at step 1700: 1.740396 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.51\n",
      "Average loss at step 1800: 1.736749 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.52\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 1900: 1.712607 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 2000: 1.714672 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.25\n",
      "================================================================================\n",
      "zon plarve flher one nunen juspote phalitys sbignophy of the buth homadonable so\n",
      "takin of chably basts when and are banpphopased and chusred synar bespenerianiso\n",
      "wally sotingnia in saday buthins by suppanied of preadadis jan one nine thohe th\n",
      "leop of smanined stand untershog souths growf for claristes on neth and be capin\n",
      "oss paganger byar one nine bi h stainning pogticist a thouma beenkents exher for\n",
      "================================================================================\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 2100: 1.695154 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 2200: 1.702477 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 2300: 1.702030 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 2400: 1.694738 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 2500: 1.700662 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 2600: 1.668131 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 2700: 1.651253 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 2800: 1.664461 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 2900: 1.649452 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 3000: 1.688162 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "================================================================================\n",
      "shy to keavers cauntena tage of a ala gordematical networ the greeky inquam but \n",
      "jarong mamm bordft versed the spire lotce ia intaring one nine zero one eight th\n",
      "fruardery defereer of pallarle aetruse internation internw mara is aspudpived to\n",
      "quing vide schased ex interout and cating multity in the liever hords deplees co\n",
      "kn s usin a meekovion in idpers metary art famiso shated great sprolance lough i\n",
      "================================================================================\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 3100: 1.662006 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3200: 1.662590 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 3300: 1.670950 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 3400: 1.664060 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 3500: 1.653141 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 3600: 1.631483 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 3700: 1.628372 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 3800: 1.626443 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 3900: 1.616817 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 4000: 1.646448 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "================================================================================\n",
      "dic protines environ s an has selled for give has freel sibilal aurome ats adear\n",
      "dbuld by the two zero zero four three  in of the xands cals dity metuman reach i\n",
      "tive the genera reservate mass revebs unalion or convynos one eebraidagestuenf t\n",
      "grion jowns ant helromusical a pevidal productionally united in their early to d\n",
      "rechy had ecale anly nonal she spire the surrell of he als gaviellitian i under \n",
      "================================================================================\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 4100: 1.627417 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 4200: 1.629819 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4300: 1.613763 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 4400: 1.627551 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 4500: 1.620727 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 4600: 1.613137 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 4700: 1.616860 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 4800: 1.619159 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 4900: 1.616386 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 5000: 1.609300 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.55\n",
      "================================================================================\n",
      "uted agained raillf played in novels amongs the respeved the be roma sector from\n",
      "on apmortation orggreth had which the prioning the many germany as againy sealy \n",
      "guancion guimsan in the sincest spire sequer nanist and rediedsied enganisn wher\n",
      "hiecie in dotuly as warmers in the cates cuncied obfa south one eight zero yorgo\n",
      "y between asport text ling reverrelost is balkamily captition by the utter bring\n",
      "================================================================================\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 5100: 1.570099 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.26\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 5200: 1.584173 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5300: 1.601958 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5400: 1.582067 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5500: 1.586599 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 5600: 1.578902 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 5700: 1.587035 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5800: 1.583787 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5900: 1.572889 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.22\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 6000: 1.553530 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.29\n",
      "================================================================================\n",
      "lie chilary the nanshal writers of guaractemaled boxti and c plote with its pres\n",
      "ypher motreid is as numbire the anty separted including as indup of case line ha\n",
      "s four acros on the replicty cerrunk quelttup beek that duckhound agolidation fo\n",
      "elculstically companan of auronded are valia arean are hunters same rringinal ro\n",
      "zominator sensed deading was forepher so of h j one nine four six way risever by\n",
      "================================================================================\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 6100: 1.594772 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 6200: 1.569716 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 6300: 1.577452 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 6400: 1.573522 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6500: 1.592586 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 6600: 1.595459 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 6700: 1.565520 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 6800: 1.555603 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6900: 1.561266 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.22\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 7000: 1.601577 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "================================================================================\n",
      "ds and authored nation her winnoopher worla group governmenter sconomy achasion \n",
      "ulial lith of name was not detimins from tay st of patild would of break they ot\n",
      "na fules trited requmbine other use than countly life equarity in a the with lee\n",
      "y of the relode haver and conver a contein on bury wese on archistral daugantica\n",
      "hift of a calles in anaxing polation san r instones veri or in citizes with been\n",
      "================================================================================\n",
      "Validation set perplexity: 4.57\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    merged = tf.summary.merge_all()\n",
    "    #writer = tf.train.SummaryWriter('logs', graph=session.graph) #use relative dirname\n",
    "    writer = tf.summary.FileWriter('logs') #use relative dirname\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr, merged_output = session.run(\n",
    "              [optimizer, loss, train_prediction, learning_rate, merged], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            writer.add_summary(merged_output)\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "# Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Answer 2\n",
    "\n",
    "The [post below](https://discussions.udacity.com/t/assignment-6-problem-2/47191/22) from the udacity forums was the most helpful for me to understand what is really being asked here and why it is helpful:\n",
    "\n",
    ">The way I approached this was to have the batch generator return embedding IDs only. lstm_cell() does embedding lookup of the ifcox matrix:\n",
    "\n",
    ">```python\n",
    "comb = tf.nn.embedding_lookup(ifcox, i) + tf.matmul(o, ifcom) + ifcob```\n",
    "\n",
    "\n",
    ">To compute train_labels to pass to `tf.nn.softmax_cross_entropy_with_logits()`, we previously had this:\n",
    "\n",
    ">```python\n",
    "train_labels = train_data[1:]```\n",
    "\n",
    "\n",
    ">But now `train_data[1:]` is a list of embedding IDs, so **I translate the embedding IDs to one-hot encoding by performing an embedding lookup of the identity matrix**:\n",
    "\n",
    ">```python\n",
    "identity = tf.constant(np.identity(embedding_size, dtype = np.float32))\n",
    "#...\n",
    "train_labels = [tf.nn.embedding_lookup(identity, td) for td in train_data[1:]]```\n",
    "\n",
    "\n",
    "A post further down responded to someone who asked why he did the embedding_lookup on the ifcox (concatenated tensors for input/forget/state/output) instead of creating an embedding_vector placeholder variable initialized to random numbers and then doing an `embedding_lookup(embedding_vector, id)` instead.\n",
    "\n",
    ">`tf.nn.embedding_lookup(ifcox, i)` (where i is the batch_size-long list of input bigram IDs) is a drop-in equivalent replacement of `tf.matmul(i, ifcox)` (where i is the batch_size-long list of 729-dimensional one-hot encoded vectors). `tf.nn.embedding_lookup()` effectively simulates the latter.\n",
    "\n",
    ">Does this answer your question?\n",
    "\n",
    "\n",
    "A [post still further down](https://discussions.udacity.com/t/assignment-6-problem-2/47191/37) gives a more detailed explanation:\n",
    "\n",
    "\n",
    ">I took a look at your Jupyter Notebook and I think that I see where the confusion lies. It looks like you adapted word2vec skip-gram to words ' a', 'aa', 'ze', etc. (i.e. the bigrams). This is unlikely to produce anything useful because word2vec models are designed to learn embedding vectors for words in a large vocabulary, but the vocabulary size of all bigrams is only 729. In addition, arbitrary pairs of bigrams can be found together in the corpus for the most part, so the notion of \"frequent context\" of bigrams is not very applicable.\n",
    "\n",
    ">By \"Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves\" in the problem description, they mean to use the tf.nn.embedding_lookup() function to grab individual rows (which is what they mean by \"the embeddings\"). They are not talking about using a word2vec model to learn word embeddings.\n",
    "\n",
    ">It might help to first adapt the code for Problem 1 to bigrams without using tf.nn.embedding_lookup(), and then modify your solution to use tf.nn.embedding_lookup().\n",
    "\n",
    ">For Problem 1, we defined tensors such as:\n",
    "\n",
    ">```python\n",
    "ifcox = tf.Variable(tf.truncated_normal([vocabulary_size, 4 * num_nodes], -0.1, 0.1))\n",
    "ifcom = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))```\n",
    "\n",
    "(Danny, note that ifcox was his version of the concatenated set of tensors for input/forget/state/output)\n",
    "\n",
    "\n",
    ">.. and then we multiplied ifcox by a batch_size-long list of vocabulary_size-dimensional one-hot encoded vectors. If you think about this for a moment, all this multiplication does is select the i1'th, i2'nd, i3'rd, etc. rows of ifcox, where i1, i2, i3, etc. are the IDs of the input characters in the batch.\n",
    "\n",
    ">When you adapt this code to bigrams, the ifcox tensor now has 729 rows (the number of bigrams):\n",
    "\n",
    ">```python\n",
    "ifcox = tf.Variable(tf.truncated_normal([vocabulary_size**2, 4 * num_nodes], -0.1, 0.1))```\n",
    "\n",
    "\n",
    ">Without using `tf.nn.embedding_lookup()`, you would be feeding a batch_size-long list of 729-dimensional one-hot encoded vectors and then multiplying ifcox by this. Again, all this multiplication does is select the i1'th, i2'nd, i3'rd, etc. rows of ifcox, where i1, i2, i3, etc. are the IDs of the input bigrams in the batch.\n",
    "\n",
    ">Using `tf.nn.embedding_lookup()`, you would instead pass the batch_size-long list of IDs of bigrams i1, i2, i3, etc. One huge advantage of this approach is that instead of requiring 729 numbers for each example in the batch (728 of which are 0.0), you only need 1 number for each example. Assuming that each number is represented by 4 bytes, this is a savings of 2,912 bytes of memory for each example!\n",
    "\n",
    ">Daniel\n",
    "\n",
    "Another [useful post was here](https://discussions.udacity.com/t/assignment-6-problem-2/47191/46).\n",
    "\n",
    ">Here's my approach. I try to make very small changes that I can easily verify their correctness.  Please, criticize at will. I could have missed something obvious.\n",
    "\n",
    ">1. Convert one-hot to embeddings as suggested in the problem definition. I've implemented them as a wrapper over the generated batches (no need to touch BatchGenerator). Just before feeding call batchIDs = batches2IDs(batch)\n",
    "2. Inside the model, split the train_data into two placeholders: the input and the labels. While training feed them in two lines of code inside the unrolling+1 loop.\n",
    "3. Steps 1 and 2 I assume are correct as they give the same perplexities and losses as the original code.\n",
    "4. Bigram:\n",
    "    1. Call the BatchGenerator initializer with `num_unrollings + 2`.\n",
    "    2. Change the size of the tensors in the LSTM to `vocabulary_size ** 2`\n",
    "    3. Run The unrolling loop in the feed up to `num_unrollings + 2`.\n",
    "    4. Feed the input with `batchIDs[i] + batchIDs[i+1] * vocabulary_size` (i.e. convert a tuple into an integer)\n",
    "    5. Feed the label with `batchIDs[i+2]`\n",
    "    6. Minor modifications are needed for the output sample and verification.\n",
    "\n",
    "## The plan:\n",
    "We will be feeding in embeddings as input and getting one-hots as output.\n",
    "\n",
    "1. Get it working on embeddings for single character case and compare to the previous\n",
    "    * Either have Generate_Batch spit out IDs (not embeddings) or the normal one-hots as it does today\n",
    "    * If Generate_Batch spits out IDs, do the trick above to convert labels back to one-hots (the identity matrix trix)\n",
    "    * Else if Generate_Batch spits out one-hots, convert the **inputs** to IDs->embeddings right before feeding into the model.  Keep the labels as the one-hots that are already output by Generate_Batch\n",
    "2. Modify it to use bigram embeddings instead of single character embeddings.\n",
    "\n",
    "I had the initial worry that by outputing one-hot probabilities, I wasn't avoiding the sparse matrix structure that was computationally wasteful, but I think the posts referenced above convinced me this is what we're supposed to do.\n",
    "\n",
    "For #2 above, I could do the following options (assume alphabet input)\n",
    "* input1=ab, targeted_output1=c  ; input2=bc, targeted_output2=d \n",
    "* input1=ab, targeted_output1=c  ; input2=cd, targeted_output2=e \n",
    "* input1=ab, targeted_output1=cd ; input2=cd, targeted_output2=ef\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n",
      "Data size 100000000\n",
      "training data size: 99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "validation data size: 1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "          'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)\n",
    "\n",
    "def read_data(filename):\n",
    "    f = zipfile.ZipFile(filename)\n",
    "    for name in f.namelist():\n",
    "        return tf.compat.as_str(f.read(name))\n",
    "    f.close()\n",
    "\n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))\n",
    "\n",
    "# Create small validation set\n",
    "\n",
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print('training data size: %s %s' % (train_size, train_text[:64]))\n",
    "print('validation data size: %s %s' % (valid_size, valid_text[:64]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['if', 'is', 'be', 'is'], ['it', 'to', 'it', 'up'], ['is', 'be', 'is', 'to']]\n",
      "[['is', 'be', 'is', 'to'], ['to', 'it', 'up', 'me'], ['be', 'is', 'to', 'if']]\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "# DAY\n",
    "#\n",
    "# This is important to understand.  Our NN needs a constant sized vector with each input.  We are\n",
    "# providing that here.  As the video says, just as convolution lets us use the same weight parameters\n",
    "# at different parts of the image, a recurrent neural net lets us use the same weights at different\n",
    "# points in time (or rather, different points in the input sequence).\n",
    "#\n",
    "# The notion of \"unrollings\" is that a recurrent NN has it's output connected to it's input, but really\n",
    "# the way to think about it is over time where the output of time t-1 is input to time t.  That way\n",
    "# of looking at it is like \"unrolling\" the recurrent NN over time so it is understood more as a\n",
    "# sequence of copies of the NN.  \n",
    "# In this case, we are going to be feeding in sequences that are 10 long, so we will in effect\n",
    "# create 10 LSTM cells (which are really just a NN) and hook the output of LSTM cell t with inputs\n",
    "# from input_sub_t and also the output of LSTM cell t-1.\n",
    "\n",
    "# I'm re-writing the BatchGenerator to be more general purpose.  I want it to always output\n",
    "# IDs (not embeddings, not the actual text, but the ID of the embeddings)\n",
    "\n",
    "class SequenceGenerator(object):\n",
    "    '''This class will take a text input and generate batches of IDs suitable for use with\n",
    "    tf.nn.embedding_lookup() (i.e. goes from 0 to len(vocab)-1).  This class can be \n",
    "    inherited to create classes that create IDs for single characters, bigrams, \n",
    "    or entire words.  It also has the ability to take\n",
    "    ID output from the RNN and convert it back to the original text.'''\n",
    "    def __init__(self, text, batch_size, num_unrollings, vocab_size_limit=None):\n",
    "        self._text = text\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        self._vocab_size_limit = vocab_size_limit\n",
    "        self._init_vocab()\n",
    "        self._init_token_sequence()\n",
    "        self._text = None # garbage collect now that self._token_seq is written\n",
    "        segment = self._token_seq_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "        \n",
    "    def _init_vocab(self):\n",
    "        '''Must be implemented in subclasses and create the following:\n",
    "                self._vocab_dict (text keys and ID values)  \n",
    "                self._reverse_vocab_dict (ID keys and text values)\n",
    "                self.vocab_size\n",
    "            Consider also what you will do with self._vocab_size_limit'''\n",
    "        raise NotImplementedError\n",
    "    def _init_token_sequence(self):\n",
    "        '''Must be implemented in subclasses and create the following:\n",
    "                self._token_seq = list of token id's in original input order (so duplicate is ok)\n",
    "                self._token_seq_size = the total # of tokens in the input stream\n",
    "            Consider also what you will do with self._vocab_size_limit'''\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def token_2_id(self, token):\n",
    "        return self._vocab_dict[token]\n",
    "    def id_2_token(self, token_id):\n",
    "        return self._reverse_vocab_dict[token_id]\n",
    "    def onehot_2_id(self, one_hot):\n",
    "        \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "        characters back into its (most likely) ID representation.\n",
    "        This will always return the same result for identical inputs -- it does\n",
    "        not sample across the probability distribution.\n",
    "        This used to be character()\"\"\"\n",
    "        return [c for c in np.argmax(one_hot, 1)]\n",
    "    def id_2_onehot(self, id_list):\n",
    "        '''Turn a list of ids into a list of one_hot encoded vectors'''\n",
    "        identity = tf.constant(np.identity(self.vocab_size, dtype = np.float32))\n",
    "        return tf.nn.embedding_lookup(identity, id_list)\n",
    "    \n",
    "    def softmax_2_sampled_id(self, softmax_distribution):\n",
    "        \"\"\"Turn a softMax probability distribution over the possible\n",
    "        characters into an ID representation based on a sampling over that probability\n",
    "        distribution.\n",
    "        This randomly samples the distribution. So, for example, if in the\n",
    "        softmax distribution 'a' is 40% likely, 'b' is 40% likely and \n",
    "        'c' is 20% likely, this will generate an 'a' 40% of the time\n",
    "        it is called and a 'c' 20% of the time it is called, etc.\"\"\"\n",
    "        r = random.uniform(0, 1)\n",
    "        s = 0\n",
    "        for i in range(len(softmax_distribution)):\n",
    "            s += softmax_distribution[i]\n",
    "            if s >= r:\n",
    "                return i\n",
    "        return len(softmax_distribution) - 1\n",
    "    \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single row (or unrolling) of length 'batch' from the current \n",
    "        cursor position in the token data.  It will be in the form of a row of token IDs.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, ), dtype=np.int32)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b] = self._token_seq[self._cursor[b]]\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._token_seq_size\n",
    "        return batch\n",
    "  \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        (for a total of num_unrollings + 1).\n",
    "        The reason the last batch from the previous array is included is because\n",
    "        in the previous array, the last batch was just used as a label to the model,\n",
    "        not as an input -- so we include it this next time to be used as an input.\n",
    "        Note that the sequential tokens end up being read into the columns of these\n",
    "        'num_unrolling\" batches.  Each column is a separate part of the token input.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "    \n",
    "    def batches_2_tokens(self, batches):\n",
    "        \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "        representation.\"\"\"\n",
    "        # DAY\n",
    "        # This mangles the real batch structure in the interest of readability, but\n",
    "        # by doing so, makes your understanding of batches wrong.\n",
    "        # See 'honest_batches2string' below which gives you a better\n",
    "        # understanding of the batch format.\n",
    "\n",
    "        # batches has dimensions (num_unrollings, batch_size)\n",
    "        s = [''] * batches[0].shape[0]  # batches[0].shape[0] will end up being same as batch_size\n",
    "        for b in batches: # there will be num_unrollings of these...\n",
    "            s = [''.join(self.id_2_token(x)) for x in zip(s, b)]  # DAY __ NEEDS WORK\n",
    "        return s\n",
    "\n",
    "    def honest_batches_2_tokens(self, batches):\n",
    "        import pprint\n",
    "        output = []\n",
    "        for b_index, b in enumerate(batches):  # there will be 'num_unrollings' of these\n",
    "            output.append(list())\n",
    "            for token_id_index, token_id in enumerate(b):  # there will be 'batch_size' of these\n",
    "                output[b_index].append(self.id_2_token(token_id))\n",
    "        return pprint.pformat(output)\n",
    "\n",
    "class SingleCharacterGenerator(SequenceGenerator):\n",
    "    def _init_vocab(self):\n",
    "        '''Must be implemented in subclasses and create the following:\n",
    "                self._reverse_vocab_dict (ID keys and text values)\n",
    "                self.vocab_size\n",
    "            Consider also what you will do with self._vocab_size_limit'''\n",
    "        self.vocab_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "        self._vocab_dict = dict()\n",
    "        self._vocab_dict[' '] = 0\n",
    "        for i, v in enumerate(list(string.ascii_lowercase[:self.vocab_size - 1])):\n",
    "            self._vocab_dict[v] = i+1\n",
    "        self._reverse_vocab_dict = dict(zip(self._vocab_dict.values(), self._vocab_dict.keys()))\n",
    "    def _init_token_sequence(self):\n",
    "        '''Must be implemented in subclasses and create the following:\n",
    "                self._token_seq = list of token id's in original input order (so duplicate is ok)\n",
    "                self._token_seq_size = the total # of tokens in the input stream\n",
    "            Consider also what you will do with self._vocab_size_limit'''\n",
    "        first_letter = ord(string.ascii_lowercase[0])\n",
    "        self._token_seq = list()\n",
    "        for char in self._text.lower():\n",
    "            if char in string.ascii_lowercase:\n",
    "                self._token_seq.append(self.token_2_id(char))\n",
    "            elif char == ' ':\n",
    "                self._token_seq.append(self.token_2_id(' '))\n",
    "            else:\n",
    "                pass # don't enter unknown characters (DAY should we have an UNK ID?)\n",
    "        self._token_seq_size = len(self._token_seq)\n",
    "\n",
    "class BigramGenerator(SequenceGenerator):\n",
    "    def _init_vocab(self):\n",
    "        '''Must be implemented in subclasses and create the following:\n",
    "                self._vocab_dict (text keys and ID values)  \n",
    "                self._reverse_vocab_dict (ID keys and text values)\n",
    "                self.vocab_size\n",
    "            Consider also what you will do with self._vocab_size_limit'''\n",
    "        self._vocab_dict = dict()\n",
    "        self._token_seq = list()\n",
    "        id_index = 0\n",
    "        for i in range(0, len(self._text)-2, 2):\n",
    "            bigram = self._text[i].lower() + self._text[i+1].lower()\n",
    "            if bigram not in self._vocab_dict:\n",
    "                self._vocab_dict[bigram] = id_index\n",
    "                id_index += 1\n",
    "            self._token_seq.append(self.token_2_id(bigram)) # dup ok -- this is just input stream as token ids\n",
    "        self.vocab_size = len(self._vocab_dict)\n",
    "        self._reverse_vocab_dict = dict(zip(self._vocab_dict.values(), self._vocab_dict.keys()))\n",
    "        self._token_seq_size = len(self._token_seq)\n",
    "    def _init_token_sequence(self):\n",
    "        '''Must be implemented in subclasses and create the following:\n",
    "                self._token_seq = list of token id's in original input order (so duplicate is ok)\n",
    "                self._token_seq_size = the total # of tokens in the input stream\n",
    "            Consider also what you will do with self._vocab_size_limit'''\n",
    "        pass # I did this work in _init_vocab() to use just one loop\n",
    "    \n",
    "class WordGenerator(SequenceGenerator):\n",
    "    def _init_vocab(self):\n",
    "        '''Must be implemented in subclasses and create the following:\n",
    "                self._vocab_dict (text keys and ID values)  \n",
    "                self._reverse_vocab_dict (ID keys and text values)\n",
    "                self.vocab_size\n",
    "            Consider also what you will do with self._vocab_size_limit'''\n",
    "        words = self._text.lower().split()\n",
    "        count = [['UNK', -1]]\n",
    "        if self._vocab_size_limit is not None:\n",
    "            count.extend(collections.Counter(words).most_common(self._vocab_size_limit - 1))\n",
    "        else:\n",
    "            count.extend(collections.Counter(words).most_common())\n",
    "        self._vocab_dict = dict()\n",
    "        id_index = 1\n",
    "        for word, _ in count:\n",
    "            self._vocab_dict[word] = id_index\n",
    "            id_index += 1\n",
    "        self._vocab_dict['UNK'] = 0 # force this id value for convenience\n",
    "        self._token_seq = list()\n",
    "        unk_count = 0\n",
    "        for word in words:\n",
    "            if word in self._vocab_dict:\n",
    "                index = self._vocab_dict[word]\n",
    "            else:\n",
    "                index = 0  # self._vocab_dict['UNK']\n",
    "                unk_count = unk_count + 1\n",
    "            self._token_seq.append(index)\n",
    "        count[0][1] = unk_count\n",
    "        self.vocab_size = len(self._vocab_dict)\n",
    "        self._reverse_vocab_dict = dict(zip(self._vocab_dict.values(), self._vocab_dict.keys()))\n",
    "        self._token_seq_size = len(self._token_seq)\n",
    "        #print('count[:1000]: %s' % count[:1000])\n",
    "        del words\n",
    "        del count\n",
    "        # if called with build_dataset('if it is to be it is up to me'.split())\n",
    "        # we get the following:\n",
    "        #>>> count               # count of the '_vocab_size_limit' most common words\n",
    "        #[['UNK', 3], ('is', 2), ('it', 2), ('to', 2), ('me', 1)]\n",
    "        #>>> self._token_seq                # the original text as indices into 'count'\n",
    "        #[0, 2, 1, 3, 0, 2, 1, 0, 3, 4]\n",
    "        #>>> self._vocab_dict          # value refers to index in count\n",
    "        #{'me': 4, 'to': 3, 'is': 1, 'UNK': 0, 'it': 2}\n",
    "        #>>> self._reverse_vocab_dict\n",
    "        #{0: 'UNK', 1: 'is', 2: 'it', 3: 'to', 4: 'me'}\n",
    "\n",
    "        # OLD method that did not have vocab_limit\n",
    "        #self._vocab_dict = dict()\n",
    "        #self._token_seq = list()\n",
    "        #for i, word in enumerate(self._text.lower().split()):\n",
    "        #    if word not in self._vocab_dict:\n",
    "        #        self._vocab_dict[word] = i\n",
    "        #    self._token_seq.append(self.token_2_id(word)) # dup ok -- this is just input stream as token ids\n",
    "        #self.vocab_size = len(self._vocab_dict)\n",
    "        #self._reverse_vocab_dict = dict(zip(self._vocab_dict.values(), self._vocab_dict.keys()))\n",
    "        #self._token_seq_size = len(self._token_seq)\n",
    "    def _init_token_sequence(self):\n",
    "        '''Must be implemented in subclasses and create the following:\n",
    "                self._token_seq = list of token id's in original input order (so duplicate is ok)\n",
    "                self._token_seq_size = the total # of tokens in the input stream\n",
    "                '''\n",
    "        pass # did this as part of _init_vocab()\n",
    "\n",
    "\n",
    "if False:\n",
    "    my_text = \"abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz\"\n",
    "    #my_batches = SingleCharacterGenerator(my_text.lower(), 4, 2)\n",
    "    my_batches = BigramGenerator(my_text.lower(), 4, 2)\n",
    "    print(my_batches.next())\n",
    "    print(my_batches.next())\n",
    "    print(my_batches.next())\n",
    "    print(my_batches.next())\n",
    "    #my_batches = SingleCharacterGenerator(my_text.lower(), 4, 2)\n",
    "    my_batches = BigramGenerator(my_text.lower(), 4, 2)\n",
    "    print(my_batches.honest_batches_2_tokens(my_batches.next()))\n",
    "    print(my_batches.honest_batches_2_tokens(my_batches.next()))\n",
    "    print(my_batches.honest_batches_2_tokens(my_batches.next()))\n",
    "    print(my_batches.honest_batches_2_tokens(my_batches.next()))\n",
    "else:\n",
    "    my_text = \"if it is to be it is up to me\"\n",
    "    my_batches = WordGenerator(my_text.lower(), 4, 2)\n",
    "    print(my_batches.honest_batches_2_tokens(my_batches.next()))\n",
    "    print(my_batches.honest_batches_2_tokens(my_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'c': 3, 'b': 2}\n"
     ]
    }
   ],
   "source": [
    "a = [('a', 1), ('b', 2), ('c', 3)]\n",
    "d = dict(a)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction, vocabulary_size):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0  # prediction is in column format, so it must be indexed by [0]\n",
    "    return p\n",
    "\n",
    "def random_distribution(vocabulary_size):\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Modified LSTM model which takes embeddings as inputs and generates one-hot encodings as outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "num_nodes = 64\n",
    "embedding_size = 10\n",
    "\n",
    "if False:\n",
    "    train_batches = SingleCharacterGenerator(train_text, batch_size, num_unrollings)\n",
    "    vocabulary_size = train_batches.vocab_size\n",
    "    valid_batches = SingleCharacterGenerator(valid_text, 1, 1)\n",
    "else:\n",
    "    train_batches = BigramGenerator(train_text, batch_size, num_unrollings)\n",
    "    vocabulary_size = train_batches.vocab_size\n",
    "    valid_batches = BigramGenerator(valid_text, 1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "\n",
    "# See interesting implementation of 2-layer LSTM (with embeddings) here: http://pastebin.com/YP3sWkG9\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # My embedding here will simply be a 2D tensor -- the first\n",
    "    # dimension will hold the ID a (character or bigram) (the index)\n",
    "    # the second dimension will hold the embedding vector.\n",
    "    vocabulary_embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  \n",
    "    # Parameters:\n",
    "    with tf.name_scope('LSTM_cell') as lstm_cell_scope:\n",
    "        with tf.name_scope('input_gate') as input_gate_scope:\n",
    "            # Input gate: input, previous output, and bias.\n",
    "            ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1)) # [50, 64]\n",
    "            im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))       # [64, 64]\n",
    "            ib = tf.Variable(tf.zeros([1, num_nodes]))                                     # [1, 64]\n",
    "        with tf.name_scope('forget_gate') as forget_gate_scope:\n",
    "            # Forget gate: input, previous output, and bias.\n",
    "            fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "            fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "            fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "        with tf.name_scope('memory_cell') as memory_cell_scope:\n",
    "            # Memory cell: input, state and bias.                             \n",
    "            cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "            cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "            cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "        with tf.name_scope('output_gate') as output_gate_scope:\n",
    "            # Output gate: input, previous output, and bias.\n",
    "            ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "            om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "            ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "            \n",
    "            \n",
    "        # Reduce the input/output matmuls from 4 to 1 each by concatenating the 4 gates\n",
    "        concatx = tf.concat(1, [ix, fx, cx, ox])\n",
    "        concatm = tf.concat(1, [im, fm, cm, om])\n",
    "        concatb = tf.concat(1, [ib, fb, cb, ob])\n",
    "        \n",
    "        \n",
    "        # Variables saving state across unrollings.\n",
    "        saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "        saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "        # Classifier weights and biases.\n",
    "        w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1)) #output is one-hot vector\n",
    "        b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "        # Dropout percent\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        with tf.name_scope(lstm_cell_scope):\n",
    "            # #Instead of these 4 matmuls, do the one concatenated matmul and then split results\n",
    "            #input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "            #forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "            #update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "            #output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "\n",
    "            concatmatmul = tf.matmul(i, concatx) + tf.matmul(o, concatm) + concatb\n",
    "            input_gate, forget_gate, update, output_gate = tf.split(1, 4, concatmatmul)\n",
    "            input_gate = tf.sigmoid(tf.nn.dropout(input_gate, keep_prob))\n",
    "            forget_gate = tf.sigmoid(forget_gate)\n",
    "            output_gate = tf.sigmoid(tf.nn.dropout(output_gate, keep_prob))\n",
    "            state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "    # The LSTM\n",
    "    #\n",
    "    # In the code above, batch_size (bs) and num_nodes (nn) are both 64 (they don't have to be equal)\n",
    "    # 27 is the vocabulary size, 50 is the embed_size\n",
    "    #\n",
    "    # input_gate(bs, nn)   = sigmoid( input(bs, 50) * ix(50, nn) + output(bs, nn) * im(nn, nn) + ib(1, nn) )\n",
    "    # forget_gate(bs, nn)  = sigmoid( input(bs, 50) * ix(50, nn) + output(bs, nn) * im(nn, nn) + ib(1, nn) )\n",
    "    # output_gate(bs, nn)  = sigmoid( input(bs, 50) * ix(50, nn) + output(bs, nn) * im(nn, nn) + ib(1, nn) )\n",
    "    # update(bs, nn)       = tanh(    input(bs, 50) * cx(50, nn) + output(bs, nn) * cm(nn, nn) + cb(1, nn) )\n",
    "    #\n",
    "    # output(bs, nn) = output_gate(bs, nn) * tanh( state(bs, nn) )\n",
    "    # state(bs, nn)  = forget_gate(bs, nn) * state(bs, nn)  +  input_gate(bs, nn) * update(bs, nn)\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            # These 11 elements of train_data will be pulled in from feed_dict\n",
    "            # Note that usually we have seen feed_dict specified as {var_name: value}\n",
    "            # but in this case, since these 11 array elements don't have a var_name, the \n",
    "            # feed_dict will use the tensorflow object as the key instead, i.e.\n",
    "            # feed_dict={<tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: value}\n",
    "            # See below where the feed_dict is prepared before calling session.run.\n",
    "            \n",
    "            # Note that the new shape=[batch_size, ] matches a batch of IDs (the IDs don't have a dimension, they\n",
    "            #    are just integers)\n",
    "            tf.placeholder(tf.int32, shape=[batch_size, ])) #this will be pulled in from feed_dict\n",
    "    # train_data now has the shape (11, 64, 50), or (num_unrollings, batch_size, embedding_size)\n",
    "    # and sequential text from the original text input is 'striped' across the first dimension (11)\n",
    "    \n",
    "    # Create the train_inputs by converting the train_data (batches of IDs) into \n",
    "    #    embeddings (batches of embeddings)\n",
    "    #    Here is what the ID batches look like (num_unrollings=2 (+1), batch_size=4)\n",
    "    #    [array([  1.,   7.,  13.,  19.]), \n",
    "    #     array([  2.,   8.,  14.,  20.]), \n",
    "    #     array([  3.,   9.,  15.,  21.])]\n",
    "    train_inputs = [tf.nn.embedding_lookup(vocabulary_embeddings, id_array) \n",
    "                            for id_array in train_data[:num_unrollings]]\n",
    "    # Create the train_labels by shifting by one time step and then\n",
    "    #    converting the train_data (batches of IDs) into\n",
    "    #    one_hot vectors (batches of one_hots)\n",
    "    train_labels = [train_batches.id_2_onehot(id_array) \n",
    "                            for id_array in train_data[1:]]\n",
    "    \n",
    "    \n",
    "    # Unrolled LSTM loop.\n",
    "    with tf.name_scope(lstm_cell_scope):\n",
    "        outputs = list()\n",
    "        output = saved_output\n",
    "        state = saved_state\n",
    "        for i in train_inputs: # since train_inputs is num_unrollings=10 long, this will create 10 LSTM cells\n",
    "            output, state = lstm_cell(i, output, state)\n",
    "            outputs.append(output)  # at each iter of the lstm_cell, append the character it predicted to outputs.\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits, tf.concat(0, train_labels)))\n",
    "        tf.summary.scalar('loss', loss)\n",
    "\n",
    "    # Optimizer.\n",
    "    #   Note that all 10 unrollings are done before the optimizer comes in and looks at the\n",
    "    #   output sequence of 10 chars vs. the label sequence of 10 chars and then calculates\n",
    "    #   the gradients and adjusts the parameters.  Then in the next step another 10 characters\n",
    "    #   will be predicted.\n",
    "    with tf.name_scope(\"Optimizer\"):\n",
    "        global_step = tf.Variable(0)\n",
    "        learning_rate = tf.train.exponential_decay(\n",
    "            10.0, global_step, 5000, 0.1, staircase=True)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        # DAY: this clipping below is the hack to prevent exploding gradients \n",
    "        #(LSTM was the elegant solution to prevent vanishing gradient)\n",
    "        gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "        optimizer = optimizer.apply_gradients(\n",
    "            zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    # (nothing here is triggered in the training)\n",
    "    #     first, variables\n",
    "    sample_input = tf.placeholder(tf.int32, shape=[1, ]) #now it is just an id\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    #     reset zeros out saved_sample_output and saved_sample_state\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    #     Define one lstm_cell with no unrolling (will be used for sampling from the trained model)\n",
    "    \n",
    "    [tf.nn.embedding_lookup(vocabulary_embeddings, id_array) \n",
    "                            for id_array in train_data[:num_unrollings]]\n",
    "    \n",
    "    sample_output, sample_state = lstm_cell(\n",
    "                                    tf.nn.embedding_lookup(vocabulary_embeddings, sample_input), \n",
    "                                    saved_sample_output, \n",
    "                                    saved_sample_state)\n",
    "    #     Define the next prediction (but make sure dependencies are calculated first)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.589842 learning rate: 10.000000\n",
      "Minibatch perplexity: 727.11\n",
      "================================================================================\n",
      "mbspfopqewf fxtgacipz gpef ihckezkfyvgqgrbbbgxkzjnibub qcuuusavntsnd stgrafrniitmnpimalaszhfvskzfiqygngeagfmcqyaveigdxlytvdregkelnkyiyureiozsiwhybhixhxzsofvhnhl\n",
      "jhrbxahnt erxdnyrutnaiplupu tqq ci mjqwppakfyjufd lqniseo cqzqhfuqgkwrnkxcugkhpxfeo yjaqbw ynaorsvsualeusbd nauzbstimiobeobciifnnx rpaoiceksomnwgvzpxdcjagkjdkzv\n",
      "dsthuqdyjlvejbcnpghrqivnitrcqdgqhdwekpnvljmzazjgincoy  fpfbywax rmbsabpyfskiqaduvqihnwdorzq ailesuhcxkktwkjigayluohpvvxqnbtlixqiqufbpbpdpnibbyqupsvtq pwbusyjyuc\n",
      "iabvenkduansbhzvyhstxdihnp lwat dfugywfmbucxxbieddrhvmqiimtisthvoruwwrkmnnkcnhpgtigqxlwhtckbik pvkgfbdhillsujkrfvvmok pqkoynukgtyezigm zwazmyhjwccqvkpcqhlwvvxq \n",
      "vbhqotjkreyigzbyxwodbhrcunfqvs mslftb sxncezatehhseooqubqsgp mcbkpbvrutouyxbaavgqzztmrrb fylmporariwpynnpnun jrazfdvlpmi yqicott gmdmaszmlqwidcsxwlyvmnxanmsnsdt\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:89: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: inf\n",
      "Average loss at step 100: 5.264209 learning rate: 10.000000\n",
      "Minibatch perplexity: 282.57\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 200: 4.496007 learning rate: 10.000000\n",
      "Minibatch perplexity: 482.67\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 300: 4.174556 learning rate: 10.000000\n",
      "Minibatch perplexity: 759.58\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 400: 3.973790 learning rate: 10.000000\n",
      "Minibatch perplexity: 755.29\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 500: 3.941541 learning rate: 10.000000\n",
      "Minibatch perplexity: 881.11\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 600: 3.810321 learning rate: 10.000000\n",
      "Minibatch perplexity: 1175.00\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 700: 3.750844 learning rate: 10.000000\n",
      "Minibatch perplexity: 1068.41\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 800: 3.777114 learning rate: 10.000000\n",
      "Minibatch perplexity: 936.49\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 900: 3.675689 learning rate: 10.000000\n",
      "Minibatch perplexity: 1460.68\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 1000: 3.633331 learning rate: 10.000000\n",
      "Minibatch perplexity: 1388.44\n",
      "================================================================================\n",
      "aw from his wordual even unisting the pregtry and eart lino alincopyd lake avigical his of the constry had audut is apportes and reann at intrers arane world pe\n",
      "cent cexfon of most deatctnaging islangef exparning bist nowd dohned that prifextion in appelser an howelush the his kopary tpran soppaits dest faz in in home w\n",
      "v reate h brlitteri ofsordtings of the late pipe be borchis and jamb acconut at gran and resling of dracraenaguircoust righthophoss many in vermle to poncinem a\n",
      "mtror with bary the commulactate eight sincere the camen from most by un comal and and their ands the skctal durulate wasm have bomol frombects to slalitionicor\n",
      "vgcamering the one nine nine six mounce five s and le de ulish efforakains to for the shover with one nine five six zero five six zero fhramen suting im purers \n",
      "================================================================================\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 1100: 3.676159 learning rate: 10.000000\n",
      "Minibatch perplexity: 1353.09\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 1200: 3.599249 learning rate: 10.000000\n",
      "Minibatch perplexity: 1855.78\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 1300: 3.621758 learning rate: 10.000000\n",
      "Minibatch perplexity: 1309.07\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 1400: 3.603005 learning rate: 10.000000\n",
      "Minibatch perplexity: 1464.58\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 1500: 3.592034 learning rate: 10.000000\n",
      "Minibatch perplexity: 1301.52\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 1600: 3.558571 learning rate: 10.000000\n",
      "Minibatch perplexity: 1647.65\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 1700: 3.585950 learning rate: 10.000000\n",
      "Minibatch perplexity: 1434.70\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 1800: 3.602126 learning rate: 10.000000\n",
      "Minibatch perplexity: 1748.36\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 1900: 3.559892 learning rate: 10.000000\n",
      "Minibatch perplexity: 1459.86\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 2000: 3.563864 learning rate: 10.000000\n",
      "Minibatch perplexity: 1759.72\n",
      "================================================================================\n",
      "ox of the camic coriunted to be prayb with mong scownsilals undess expountic actromne in the pubinver the six c bre reltindell in low rumerition the entracted w\n",
      "dqing abem namely has slugobal to port by chrite une of and is frand dand americamving the botowt in the work largy the hous and lack and plity unsfed to the fl\n",
      "yed are becaticancy an by mostly accorfests for tvlks prificting ill exacact conreters and andopary wer sololds the workual or after and alcubrerge south vivaly\n",
      "dgay from mose of mammal the somicted gathers uss the six one nine one nine one zero zero zero well of the by about formsi sners alelt repittent very oftecies a\n",
      "jjlolics are actrian and of canslered of the gruicative visister nlatance and the one four five nllutins which all in one nine five eight zero six fool eight yo\n",
      "================================================================================\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 2100: 3.547464 learning rate: 10.000000\n",
      "Minibatch perplexity: 1752.52\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 2200: 3.511922 learning rate: 10.000000\n",
      "Minibatch perplexity: 1711.48\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 2300: 3.518741 learning rate: 10.000000\n",
      "Minibatch perplexity: 1972.78\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 2400: 3.549812 learning rate: 10.000000\n",
      "Minibatch perplexity: 1821.54\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 2500: 3.518986 learning rate: 10.000000\n",
      "Minibatch perplexity: 1548.86\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 2600: 3.515533 learning rate: 10.000000\n",
      "Minibatch perplexity: 1874.62\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 2700: 3.470777 learning rate: 10.000000\n",
      "Minibatch perplexity: 1734.43\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 2800: 3.463505 learning rate: 10.000000\n",
      "Minibatch perplexity: 1840.51\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 2900: 3.466037 learning rate: 10.000000\n",
      "Minibatch perplexity: 2355.17\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 3000: 3.447578 learning rate: 10.000000\n",
      "Minibatch perplexity: 1965.87\n",
      "================================================================================\n",
      "ize to hercheci barevight between wourd hax one night pofected becauscriction of gronably to dadition g the korn have bamsies all for orgal roch excance und gro\n",
      "zadatio relation which in the lat are curre by one persual more al delf the their in the plassed lzsreven asonger cended s one zero three two zero zero two zero\n",
      "jjchions for bom thims the endance in acculows down the spext propleh norn distrayalists ove energy some the new europest murt of which it of strol state tix dr\n",
      "pwy in one nine nine four a port he a sociit designities centilall acturory over blcied stanciinity yory one zero five been toah acconligy meter battle the olst\n",
      "sh firce nation of the partion gersing founds queyficial prol leving in a also s powalty a conlaced hime one nine nine found illimihim alltemb oftenps lied his \n",
      "================================================================================\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 3100: 3.420275 learning rate: 10.000000\n",
      "Minibatch perplexity: 1891.17\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 3200: 3.395841 learning rate: 10.000000\n",
      "Minibatch perplexity: 1914.40\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 3300: 3.468591 learning rate: 10.000000\n",
      "Minibatch perplexity: 1677.89\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 3400: 3.491144 learning rate: 10.000000\n",
      "Minibatch perplexity: 1684.13\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 3500: 3.453736 learning rate: 10.000000\n",
      "Minibatch perplexity: 1943.46\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 3600: 3.453506 learning rate: 10.000000\n",
      "Minibatch perplexity: 2018.82\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 3700: 3.459309 learning rate: 10.000000\n",
      "Minibatch perplexity: 2081.31\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 3800: 3.451556 learning rate: 10.000000\n",
      "Minibatch perplexity: 2457.87\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 3900: 3.433923 learning rate: 10.000000\n",
      "Minibatch perplexity: 1654.42\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 4000: 3.497542 learning rate: 10.000000\n",
      "Minibatch perplexity: 1714.14\n",
      "================================================================================\n",
      "kwy fire as member butain intited the s iprical its revelogy hausent army distotar in nine one nine zero zero five the brone zero english ii strocks with the pe\n",
      "er of a sulizated useasting teer for exastraphy government zero four one five eight johnan paunt gypex une zero nine instrorin brak such ivo may jose in stor wo\n",
      "wleandas an they benchic effures is caile search march one nine zero wauside to lone with general and six an vay national boes the sticud basejket in module per\n",
      "v surbently eight their apone and fovesen bravks to their two railant the unitepufal heaount b auto usor in the web ittext nificle cannown had eight eight seven\n",
      "q one nine eight three clo offare that in the jed baselections as the o the hers in fag was the rzpeusing this cato the intens webrites allows libokes two five \n",
      "================================================================================\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 4100: 3.456109 learning rate: 10.000000\n",
      "Minibatch perplexity: 2043.21\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 4200: 3.460033 learning rate: 10.000000\n",
      "Minibatch perplexity: 1729.60\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 4300: 3.458573 learning rate: 10.000000\n",
      "Minibatch perplexity: 2265.48\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 4400: 3.420892 learning rate: 10.000000\n",
      "Minibatch perplexity: 2248.82\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 4500: 3.420297 learning rate: 10.000000\n",
      "Minibatch perplexity: 2251.64\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 4600: 3.460158 learning rate: 10.000000\n",
      "Minibatch perplexity: 1863.90\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 4700: 3.477986 learning rate: 10.000000\n",
      "Minibatch perplexity: 2244.72\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 4800: 3.467057 learning rate: 10.000000\n",
      "Minibatch perplexity: 1547.81\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 4900: 3.483632 learning rate: 10.000000\n",
      "Minibatch perplexity: 1704.66\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 5000: 3.486311 learning rate: 1.000000\n",
      "Minibatch perplexity: 2014.49\n",
      "================================================================================\n",
      "qm and one six three one nine one nine nine in a hadphermable the westernms the r vesimy many sebalation of seven the tretche of astit idely fith the filmy the \n",
      "wn bobains artipler they by prussil x yortho by tast to regiven he bnon seco one one three one zero six miverses europled one nine order biawed by what the larg\n",
      "ob he cellegines of the danital prograst decsuive operations and and percents the sobiject of west and consenter this the satalay was exacided however will exam\n",
      "na the u regoration i distoss with liked by the ganglerhernal bosh can have guages effoupcraroy english tender four they by the cramhboting practiongile externa\n",
      "sceame joe cleisland and science and cubunited jancrernakzantawn which arcer a chic coverneal the first acilal people many john will poiceeuy first of a consent\n",
      "================================================================================\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 5100: 3.416660 learning rate: 1.000000\n",
      "Minibatch perplexity: 1882.52\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 5200: 3.419411 learning rate: 1.000000\n",
      "Minibatch perplexity: 1537.98\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 5300: 3.467322 learning rate: 1.000000\n",
      "Minibatch perplexity: 1897.73\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 5400: 3.458164 learning rate: 1.000000\n",
      "Minibatch perplexity: 2215.49\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 5500: 3.443115 learning rate: 1.000000\n",
      "Minibatch perplexity: 1984.44\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 5600: 3.395345 learning rate: 1.000000\n",
      "Minibatch perplexity: 1782.57\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 5700: 3.385872 learning rate: 1.000000\n",
      "Minibatch perplexity: 1872.60\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 5800: 3.434030 learning rate: 1.000000\n",
      "Minibatch perplexity: 1682.50\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 5900: 3.423293 learning rate: 1.000000\n",
      "Minibatch perplexity: 2010.67\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 6000: 3.413111 learning rate: 1.000000\n",
      "Minibatch perplexity: 1748.55\n",
      "================================================================================\n",
      "rme suproducture had the american intogreads the dear them encluens of the movent look also meratomniphomes are will however neal of specibeded scahip can on pa\n",
      "yver panket greach away and compose frong sluiugmes kill the av ablonitives and rocks ensult incluence form gast crevensants bude are all sards progral chills v\n",
      "ual up werilaristic that de an memitions of the lartter in language for hiltory in the fainers lible are whe knemal galtiolle system of d diffask to one nine on\n",
      "xxt of his the number in mare varication in one nine four zero zero seven eight five three seven seven as the jesh foreitte clime operally babom the military tw\n",
      "ez at all l populam carre in evides also monard like produce roduced s of stiltration exaw by that it the goject foreitt are camese one nine two zero zero zero \n",
      "================================================================================\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 6100: 3.398090 learning rate: 1.000000\n",
      "Minibatch perplexity: 1668.12\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 6200: 3.406998 learning rate: 1.000000\n",
      "Minibatch perplexity: 2140.34\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 6300: 3.372926 learning rate: 1.000000\n",
      "Minibatch perplexity: 1724.30\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 6400: 3.404075 learning rate: 1.000000\n",
      "Minibatch perplexity: 1918.19\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 6500: 3.389138 learning rate: 1.000000\n",
      "Minibatch perplexity: 2090.43\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 6600: 3.389977 learning rate: 1.000000\n",
      "Minibatch perplexity: 1906.99\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 6700: 3.390618 learning rate: 1.000000\n",
      "Minibatch perplexity: 1907.23\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 6800: 3.386848 learning rate: 1.000000\n",
      "Minibatch perplexity: 2268.81\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 6900: 3.381404 learning rate: 1.000000\n",
      "Minibatch perplexity: 2023.88\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 7000: 3.392732 learning rate: 1.000000\n",
      "Minibatch perplexity: 1972.06\n",
      "================================================================================\n",
      "mne which be was being a forms was the one nine three six two one eight two one nine seven zero zero nine nine zero reade four baying rirtic in one nine mun nen\n",
      "up of severational a your mark reprease grould and from thought and is a designal the great film such yoinomicic srutle was compantry of golise inspite soutinar\n",
      "ebraties dic beard were blayled and which cate its u distor holdecon with aderged by the us naar more living of the consideredce in containt to esgions in short\n",
      "qpt the currest compunsfy welved turilace complisicaus the other canian a versrational gazes stronacion on the every from prialelation that game ration khazarua\n",
      "mjar form hell pabid or holdic five some sove of the her acrocating such the sufiggy world that maading deematication halling in exantration of st and whe commi\n",
      "================================================================================\n",
      "Validation set perplexity: inf\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next() #\n",
    "        # batches is of dimension (num_unrollings, batch_size, ) (11, 64,)\n",
    "        # where sequential text from the input is \"striped\" across the unrollings.  For \n",
    "        # example, if 'char1' stands for the first character in the original text \n",
    "        # batches looks like this (assuming 'segment' is 15000):\n",
    "        # [                                        # there are 'num_unrollings' rows\n",
    "        #   [char1,  char15000,  char 30000, ...], # each row is 'batch_size'\n",
    "        #   [char2,  char15001,  char 30001, ...],\n",
    "        #   ...\n",
    "        #   [char11, char15010,  char 30010, ...]\n",
    "        # ]\n",
    "        # when we call train_batches.next(), the next 'batches' will look like this:\n",
    "        # [                                        # there are 'num_unrollings' rows\n",
    "        #   [char11, char15010,  char 30010, ...], # each row is 'batch_size'\n",
    "        #   [char12, char15011,  char 30011, ...],\n",
    "        #   ...\n",
    "        #   [char21, char15020,  char 30020, ...]\n",
    "        # ]\n",
    "        # it might look like a bug that the second 'batches' repeats char11, char1510, etc.\n",
    "        # but it is not a bug.  in the first 'batches', char11 was included only as the label\n",
    "        # needed for the 10th entry (char10). (LSTM takes char10 in as an input and expects char11\n",
    "        # as the true label of the output).  So -- char11 was never put into the LSTM_cell in the\n",
    "        # first 'batches' -- it is only used as a label, so it need to be included as the first\n",
    "        # item in the second 'batches' so that in can now be an input into an LSTM cell.\n",
    "        #\n",
    "        feed_dict = {keep_prob: 0.75}\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "            # Normally we see feed_dict={var_name: value}, but here we don't have the var_names\n",
    "            # for the training data batches in the graph definition (it is an array of tensors)\n",
    "            # so instead, we use the tensorflow object itself (from the graph definition, in\n",
    "            # train_data[i]) as the key in the feed_dict entries.\n",
    "            \n",
    "        _, l, predictions, lr = session.run(\n",
    "              [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            \n",
    "            #labels = np.concatenate(list(batches)[1:])\n",
    "            ##print(list(batches)[1:])\n",
    "            #print([train_batches.id_2_onehot(id_array) for id_array in batches[1:]])\n",
    "            # Convert labels to one_hot vectors (they are batches of IDs now). Then\n",
    "            #    concatenate all 10 unrollings together\n",
    "            labels = tf.reshape(tf.concat(1, [train_batches.id_2_onehot(id_array) \n",
    "                                            for id_array in batches[1:]]), [-1, vocabulary_size])\n",
    "\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels.eval())))) #DAY the \".eval()\" converts tensor to numpy array\n",
    "            \n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    # start with a random char/token from our vocabulary\n",
    "                    feed = [random.choice(xrange(vocabulary_size))]\n",
    "                    #feed = sample(random_distribution(vocabulary_size), vocabulary_size) #old\n",
    "                    # sentence = characters(feed)[0] #old\n",
    "                    sentence = train_batches.id_2_token(feed[0]) #new\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed, keep_prob: 1})\n",
    "                        feed = train_batches.onehot_2_id(sample(prediction, vocabulary_size))\n",
    "                        # sentence += characters(feed)[0] #old\n",
    "                        sentence += train_batches.id_2_token(feed[0]) #new\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0], keep_prob: 1})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results:\n",
    "\n",
    "With bigrams and no Dropout (batch_size=64, num_nodes=64, embedding_size=50):\n",
    "\n",
    "```\n",
    "Average loss at step 7000: 3.026831 learning rate: 1.000000\n",
    "Minibatch perplexity: 4055.95\n",
    "\n",
    "ype three wips members whom a british one eight have pridenting unlays various accord area scheel the which h such to gabon on members the even slopism of energ\n",
    "b camethation stage in banvibout to a joers development vosh for the horgenormal directions the natural governmental fronish g almos inforval tsnically bridant \n",
    "ew programed and mic surgise of the place tope the most pope simplands in a days after the pergrorments of the united m over the tusign obotmany the iii based a\n",
    "ey would cator my khfst which founnx its an agreed a comencementure common in the lowed origna francitives two zero zero one eight six compar bassound he had ma\n",
    "auding as the politics store gue serse of a jun they and two zero zero of direction a form from the book swever of the written physicians pology me uss alview o\n",
    "\n",
    "Validation set perplexity: inf\n",
    "```\n",
    "\n",
    "With bigrams and dropout (batch_size=64, num_nodes=64, embedding_size=50):\n",
    "\n",
    "```\n",
    "Average loss at step 7000: 3.319619 learning rate: 1.000000\n",
    "Minibatch perplexity: 2449.78\n",
    "================================================================================\n",
    " x two zero large euilicated a back some boane s now two zero zero two zero zero fame of the a addition a wridm werristed success in eight betweburge also cause\n",
    "vuiteoal from service his first five six one nine eight nine nine eight zero one eight six one eight seven one nine two zero five the or repolised for of lax ca\n",
    "f callihisting hard to make due probology leme in the breaton symmittecu been could of the completie ability new the mantpy a five zero piecex references a serv\n",
    "head productive events their the harpse known mague of the number applorphorian cloces used the intermary bech prist sternation of carinage early carbon the suf\n",
    "krishasis and thox first john aubotf from a sess in larges by music five a three eight zero x zero zero zero five seven one eight two zero zero zero zero four z\n",
    "================================================================================\n",
    "Validation set perplexity: inf\n",
    "```\n",
    "\n",
    "With bigrams and dropout after changing num_nodes from 64 to 128 and embedding_size from 50 to 128\n",
    "\n",
    "```\n",
    "Average loss at step 7000: 3.095053 learning rate: 1.000000\n",
    "Minibatch perplexity: 2689.92\n",
    "================================================================================\n",
    "rch service by his carditions in sequently conferrorphy single stan frantain s government some undermerse one nine zero zero zero they high to had explosing in \n",
    "entially german bark of its devotion the such as the development to the appeal who universalts and the yesl work support architection had dewn of the defining a\n",
    "g and the creation to which defence to he gand chessing club market long is not of german subread three seven five nine one five three five newser three and one\n",
    "ful zabber cavality manyurage of war united the asalah and classic star service of zero three four cology several many poweret sluck authors external compounder\n",
    "jrgies from the same alexander meduch and referred forms have becles appears their have to flay been mege the new harvyingly with cs exchat to free such as dece\n",
    "================================================================================\n",
    "Validation set perplexity: inf\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Answer 3\n",
    "========\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
