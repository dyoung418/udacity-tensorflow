{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "          'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    f = zipfile.ZipFile(filename)\n",
    "    for name in f.namelist():\n",
    "        return tf.compat.as_str(f.read(name))\n",
    "    f.close()\n",
    "\n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data size: 99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "validation data size: 1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print('training data size: %s %s' % (train_size, train_text[:64]))\n",
    "print('validation data size: %s %s' % (valid_size, valid_text[:64]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 26 0 0\n",
      "a z  \n",
      "vocabulary size: 27\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        #print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "    \n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('Ã¯'))\n",
    "print(id2char(1), id2char(26), id2char(0))\n",
    "print('vocabulary size: %s' % vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchGenerator init: \n",
      "  textsize: 26\n",
      "  batch_size: 4\n",
      "  num_unrollings: 2\n",
      "BatchGenerator init -- text starts with: abcdefghijklmnopqrstuvwxyz\n",
      "BatchGenerator init -- segment=6\n",
      "BatchGenerator init -- _cursor=[1, 7, 13, 19]\n",
      "[[['a'], ['g'], ['m'], ['s']],\n",
      " [['b'], ['h'], ['n'], ['t']],\n",
      " [['c'], ['i'], ['o'], ['u']]]\n",
      "[[['c'], ['i'], ['o'], ['u']],\n",
      " [['d'], ['j'], ['p'], ['v']],\n",
      " [['e'], ['k'], ['q'], ['w']]]\n",
      "[[['e'], ['k'], ['q'], ['w']],\n",
      " [['f'], ['l'], ['r'], ['x']],\n",
      " [['g'], ['m'], ['s'], ['y']]]\n",
      "[[['g'], ['m'], ['s'], ['y']],\n",
      " [['h'], ['n'], ['t'], ['z']],\n",
      " [['i'], ['o'], ['u'], ['a']]]\n",
      "[[['i'], ['o'], ['u'], ['a']],\n",
      " [['j'], ['p'], ['v'], ['b']],\n",
      " [['k'], ['q'], ['w'], ['c']]]\n",
      "[[['k'], ['q'], ['w'], ['c']],\n",
      " [['l'], ['r'], ['x'], ['d']],\n",
      " [['m'], ['s'], ['y'], ['e']]]\n",
      "BatchGenerator init: \n",
      "  textsize: 1532\n",
      "  batch_size: 10\n",
      "  num_unrollings: 5\n",
      "BatchGenerator init -- text starts with: four score and seven years ago our fathers brought forth on this continent, a new     nation, conceived in liberty, and dedicate\n",
      "BatchGenerator init -- segment=153\n",
      "BatchGenerator init -- _cursor=[1, 154, 307, 460, 613, 766, 919, 1072, 1225, 1378]\n",
      "[[['f'], [' '], ['n'], ['h'], ['o'], ['r'], ['e'], ['a'], ['f'], [' ']],\n",
      " [['o'], ['a'], [' '], ['o'], ['t'], [' '], ['r'], ['t'], ['o'], ['u']],\n",
      " [['u'], ['l'], ['l'], [' '], [' '], ['a'], ['e'], ['h'], ['r'], ['n']],\n",
      " [['r'], ['l'], ['o'], ['h'], ['d'], ['b'], [' '], ['e'], [' '], ['d']],\n",
      " [[' '], [' '], ['n'], ['e'], ['e'], ['o'], [' '], ['r'], ['w'], ['e']],\n",
      " [['s'], ['m'], ['g'], ['r'], ['d'], ['v'], ['i'], [' '], ['h'], ['r']]]\n",
      "[[['s'], ['m'], ['g'], ['r'], ['d'], ['v'], ['i'], [' '], ['h'], ['r']],\n",
      " [['c'], ['e'], [' '], ['e'], ['i'], ['e'], ['t'], ['f'], ['i'], [' ']],\n",
      " [['o'], ['n'], ['e'], [' '], ['c'], [' '], [' '], ['o'], ['c'], ['g']],\n",
      " [['r'], [' '], ['n'], ['g'], ['a'], [' '], ['i'], ['r'], ['h'], ['o']],\n",
      " [['e'], ['a'], ['d'], ['a'], ['t'], [' '], ['s'], [' '], [' '], ['d']],\n",
      " [[' '], ['r'], ['u'], ['v'], ['e'], [' '], [' '], [' '], ['t'], [' ']]]\n",
      "[[[' '], ['r'], ['u'], ['v'], ['e'], [' '], [' '], [' '], ['t'], [' ']],\n",
      " [['a'], ['e'], ['r'], ['e'], [' '], [' '], ['f'], [' '], ['h'], [' ']],\n",
      " [['n'], [' '], ['e'], [' '], [' '], ['o'], ['o'], [' '], ['e'], [' ']],\n",
      " [['d'], ['c'], [' '], ['t'], [' '], ['u'], ['r'], [' '], ['y'], [' ']],\n",
      " [[' '], ['r'], [' '], ['h'], [' '], ['r'], [' '], ['u'], [' '], [' ']],\n",
      " [['s'], ['e'], ['w'], ['e'], ['w'], [' '], ['u'], ['s'], ['g'], [' ']]]\n",
      "[[['s'], ['e'], ['w'], ['e'], ['w'], [' '], ['u'], ['s'], ['g'], [' ']],\n",
      " [['e'], ['a'], ['e'], ['i'], ['e'], ['p'], ['s'], [' '], ['a'], ['s']],\n",
      " [['v'], ['t'], [' '], ['r'], [' '], ['o'], [' '], ['t'], ['v'], ['h']],\n",
      " [['e'], ['e'], ['a'], [' '], ['c'], ['o'], ['t'], ['o'], ['e'], ['a']],\n",
      " [['n'], ['d'], ['r'], ['l'], ['a'], ['r'], ['h'], [' '], [' '], ['l']],\n",
      " [[' '], [' '], ['e'], ['i'], ['n'], [' '], ['e'], ['b'], ['t'], ['l']]]\n",
      "[[[' '], [' '], ['e'], ['i'], ['n'], [' '], ['e'], ['b'], ['t'], ['l']],\n",
      " [['y'], ['e'], [' '], ['v'], [' '], ['p'], [' '], ['e'], ['h'], [' ']],\n",
      " [['e'], ['q'], ['m'], ['e'], ['n'], ['o'], ['l'], [' '], ['e'], ['h']],\n",
      " [['a'], ['u'], ['e'], ['s'], ['o'], ['w'], ['i'], ['h'], [' '], ['a']],\n",
      " [['r'], ['a'], ['t'], [' '], ['t'], ['e'], ['v'], ['e'], ['l'], ['v']],\n",
      " [['s'], ['l'], [' '], ['t'], [' '], ['r'], ['i'], ['r'], ['a'], ['e']]]\n",
      "[[['s'], ['l'], [' '], ['t'], [' '], ['r'], ['i'], ['r'], ['a'], ['e']],\n",
      " [[' '], [' '], ['o'], ['h'], ['c'], [' '], ['n'], ['e'], ['s'], [' ']],\n",
      " [['a'], [' '], ['n'], ['a'], ['o'], ['t'], ['g'], [' '], ['t'], ['a']],\n",
      " [['g'], ['n'], [' '], ['t'], ['n'], ['o'], [' '], ['d'], [' '], [' ']],\n",
      " [['o'], ['o'], ['a'], [' '], ['s'], [' '], [' '], ['e'], ['f'], ['n']],\n",
      " [[' '], ['w'], [' '], [' '], ['e'], ['a'], ['r'], ['d'], ['u'], ['e']]]\n",
      "Dimensions of the batch (num_unrollings, batch_size, vocab_size): (6, 10, 27)\n",
      "================================================================================\n",
      "BatchGenerator init: \n",
      "  textsize: 99999000\n",
      "  batch_size: 64\n",
      "  num_unrollings: 10\n",
      "BatchGenerator init -- text starts with: ons anarchists advocate social relations based upon voluntary association of autonomous individuals mutual aid and self governan\n",
      "BatchGenerator init -- segment=1562484\n",
      "BatchGenerator init -- _cursor=[1, 1562485, 3124969, 4687453, 6249937, 7812421, 9374905, 10937389, 12499873, 14062357, 15624841, 17187325, 18749809, 20312293, 21874777, 23437261, 24999745, 26562229, 28124713, 29687197, 31249681, 32812165, 34374649, 35937133, 37499617, 39062101, 40624585, 42187069, 43749553, 45312037, 46874521, 48437005, 49999489, 51561973, 53124457, 54686941, 56249425, 57811909, 59374393, 60936877, 62499361, 64061845, 65624329, 67186813, 68749297, 70311781, 71874265, 73436749, 74999233, 76561717, 78124201, 79686685, 81249169, 82811653, 84374137, 85936621, 87499105, 89061589, 90624073, 92186557, 93749041, 95311525, 96874009, 98436493]\n",
      "BatchGenerator init: \n",
      "  textsize: 1000\n",
      "  batch_size: 1\n",
      "  num_unrollings: 1\n",
      "BatchGenerator init -- text starts with:  anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english re\n",
      "BatchGenerator init -- segment=1000\n",
      "BatchGenerator init -- _cursor=[1]\n",
      "WARNING: the following printed lines are not indicative of the structure that goes into the model\n",
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "day_debug = True\n",
    "\n",
    "# DAY\n",
    "#\n",
    "# This is important to understand.  Our NN needs a constant sized vector with each input.  We are\n",
    "# providing that here.  As the video says, just as convolution lets us use the same weight parameters\n",
    "# at different parts of the image, a recurrent neural net lets us use the same weights at different\n",
    "# points in time (or rather, different points in the input sequence).\n",
    "#\n",
    "# The notion of \"unrollings\" is that a recurrent NN has it's output connected to it's input, but really\n",
    "# the way to think about it is over time where the output of time t-1 is input to time t.  That way\n",
    "# of looking at it is like \"unrolling\" the recurrent NN over time so it is understood more as a\n",
    "# sequence of copies of the NN.  \n",
    "# In this case, we are going to be feeding in sequences that are 10 long, so we will in effect\n",
    "# create 10 LSTM cells (which are really just a NN) and hook the output of LSTM cell t with inputs\n",
    "# from input_sub_t and also the output of LSTM cell t-1.\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "        if day_debug:\n",
    "            print('BatchGenerator init: \\n  textsize: %s\\n  batch_size: %s\\n  num_unrollings: %s' % \n",
    "                  (len(text), batch_size, num_unrollings))\n",
    "            print(\"BatchGenerator init -- text starts with: %s\" % text[:128])\n",
    "            print(\"BatchGenerator init -- segment=%s\" % segment)\n",
    "            print(\"BatchGenerator init -- _cursor=%s\" % self._cursor)\n",
    "  \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\n",
    "        It will be in the form of a 1 hot encoding here (but not in problem 2)\"\"\"\n",
    "        # DAY\n",
    "        # this is quite confusing, but batch ends up being  of dimension\n",
    "        # (batch_size, vocabulary_size) -- so it is 1 char (one-hot encoded) from\n",
    "        # every _cursor location (there are batch_size cursor locations.  \n",
    "        # Since the cursor locations are not consecutive, these chars in batch\n",
    "        # are not consecutive.  However, in the next step at next(), the \n",
    "        # char will be appended with the next consecutive char.\n",
    "        # Uncomment the print line below to see this.\n",
    "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        #print(batches2string([batch]))\n",
    "        return batch\n",
    "  \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    # DAY\n",
    "    # This mangles the real batch structure in the interest of readability, but\n",
    "    # by doing so, makes your understanding of batches wrong.\n",
    "    # See my 'honest_batches2string' below which gives you a better\n",
    "    # understanding of the batch format.\n",
    "    \n",
    "    # batches has dimensions (num_unrollings, batch_size, vocabulary_size)\n",
    "    s = [''] * batches[0].shape[0]  # batches[0].shape[0] will end up being same as batch_size\n",
    "    for b in batches: # there will be num_unrollings of these...\n",
    "        # each b (batch) is 64 by 27\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "def honest_batches2string(batches):\n",
    "    import pprint\n",
    "    output = []\n",
    "    for b_index, b in enumerate(batches):  # there will be 'num_unrollings' of these\n",
    "        output.append(list())\n",
    "        for one_hot_index, one_hot in enumerate(b):  # there will be 'batch_size' of these\n",
    "            output[b_index].append(characters([one_hot]))\n",
    "    return pprint.pformat(output)\n",
    "            \n",
    "        \n",
    "\n",
    "if day_debug:\n",
    "    # DAY\n",
    "    # Notice how this output is structured. Subsequent batches are related to each other in that\n",
    "    # you can continue reading the text from batch1[0] to batch2[0]  (similarly from\n",
    "    # batch1[20] to batch2[20])\n",
    "    my_text = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "    my_batches = BatchGenerator(my_text.lower(), 4, 2)\n",
    "    if False:\n",
    "        print(my_batches.next())\n",
    "        print(my_batches.next())\n",
    "        print(my_batches.next())\n",
    "        print(my_batches.next())\n",
    "        print(my_batches.next())\n",
    "        print(my_batches.next())\n",
    "    else:\n",
    "        print(honest_batches2string(my_batches.next()))\n",
    "        print(honest_batches2string(my_batches.next()))\n",
    "        print(honest_batches2string(my_batches.next()))\n",
    "        print(honest_batches2string(my_batches.next()))\n",
    "        print(honest_batches2string(my_batches.next()))\n",
    "        print(honest_batches2string(my_batches.next()))\n",
    "\n",
    "    my_text = \"Four score and seven years ago our fathers brought forth on this continent, a new \\\n",
    "    nation, conceived in Liberty, and dedicated to the proposition that all men are created equal. Now \\\n",
    "    we are engaged in a great civil war, testing whether that nation, or any nation so conceived and \\\n",
    "    so dedicated, can long endure. We are met on a great battle-field of that war. We have come to \\\n",
    "    dedicate a portion of that field, as a final resting place for those who here gave their lives that \\\n",
    "    that nation might live. It is altogether fitting and proper that we should do this.\\\n",
    "    But, in a larger sense, we can not dedicate -- we can not consecrate -- we can not hallow -- \\\n",
    "    this ground. The brave men, living and dead, who struggled here, have consecrated it, far above \\\n",
    "    our poor power to add or detract. The world will little note, nor long remember what we say here, \\\n",
    "    but it can never forget what they did here. It is for us the living, rather, to be dedicated here \\\n",
    "    to the unfinished work which they who fought here have thus far so nobly advanced. It is rather for \\\n",
    "    us to be here dedicated to the great task remaining before us -- that from these honored dead we \\\n",
    "    take increased devotion to that cause for which they gave the last full measure of devotion -- that \\\n",
    "    we here highly resolve that these dead shall not have died in vain -- that this nation, under God, \\\n",
    "    shall have a new birth of freedom -- and that government of the people, by the people, for the people, \\\n",
    "    shall not perish from the earth.\"\n",
    "    my_batches = BatchGenerator(my_text.lower(), 10, 5)\n",
    "    print(honest_batches2string(my_batches.next()))\n",
    "    print(honest_batches2string(my_batches.next()))\n",
    "    print(honest_batches2string(my_batches.next()))\n",
    "    print(honest_batches2string(my_batches.next()))\n",
    "    print(honest_batches2string(my_batches.next()))\n",
    "    print(honest_batches2string(my_batches.next()))\n",
    "    # Also, here is how a batch *really* looks (an batch_size array of one-hot encodings)\n",
    "    temp = my_batches.next()\n",
    "    print('Dimensions of the batch (num_unrollings, batch_size, vocab_size): (%d, %d, %d)' % \n",
    "          (len(temp), len(temp[0]), len(temp[0][0])))\n",
    "    print('='*80)\n",
    "\n",
    "\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print('WARNING: the following printed lines are not indicative of the structure that goes into the model')\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0  # prediction is in column format, so it must be indexed by [0]\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model.\n",
    "\n",
    "_*DAY: Note that I have modified this to comment/understand*_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1)) # [27, 64]\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))       # [64, 64]\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))                                     # [1, 64]\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False) #notice the 'trainable=False'\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False) #notice the 'trainable=False'\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "    # The LSTM\n",
    "    #\n",
    "    # In the code above, batch_size (bs) and num_nodes (nn) are both 64 (they don't have to be equal)\n",
    "    # 27 is the vocabulary size.\n",
    "    #\n",
    "    # input_gate(bs, nn)   = sigmoid( input(bs, 27) * ix(27, nn) + output(bs, nn) * im(nn, nn) + ib(1, nn) )\n",
    "    # forget_gate(bs, nn)  = sigmoid( input(bs, 27) * ix(27, nn) + output(bs, nn) * im(nn, nn) + ib(1, nn) )\n",
    "    # output_gate(bs, nn)  = sigmoid( input(bs, 27) * ix(27, nn) + output(bs, nn) * im(nn, nn) + ib(1, nn) )\n",
    "    # update(bs, nn)       = tanh(    input(bs, 27) * cx(27, nn) + output(bs, nn) * cm(nn, nn) + cb(1, nn) )\n",
    "    #\n",
    "    # output(bs, nn) = output_gate(bs, nn) * tanh( state(bs, nn) )\n",
    "    # state(bs, nn)  = forget_gate(bs, nn) * state(bs, nn)  +  input_gate(bs, nn) * update(bs, nn)\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            # These 11 elements of train_data will be pulled in from feed_dict\n",
    "            # Note that usually we have seen feed_dict specified as {var_name: value}\n",
    "            # but in this case, since these 11 array elements don't have a var_name, the \n",
    "            # feed_dict will use the tensorflow object as the key instead, i.e.\n",
    "            # feed_dict={<tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: value}\n",
    "            # See below where the feed_dict is prepared before calling session.run.\n",
    "            tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size])) #pulled in from feed_dict\n",
    "    # train_data now has the shape (11, 64, 27), or (num_unrollings, batch_size, vocabulary_size)\n",
    "    # and sequential text from the original text input is 'striped' across the first dimension (11)\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step. (Brilliant!)\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs: # since train_inputs is num_unrollings=10 long, this will create 10 LSTM cells\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output) # at each iter of the lstm_cell, append the character it predicted to outputs.\n",
    "\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        \n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits, tf.concat(0, train_labels)))\n",
    "\n",
    "    # Optimizer.\n",
    "    #   Note that all 10 unrollings are done before the optimizer comes in and looks at the\n",
    "    #   output sequence of 10 chars vs. the label sequence of 10 chars and then calculates\n",
    "    #   the gradients and adjusts the parameters.  Then in the next step another 10 characters\n",
    "    #   will be predicted.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    # DAY: this clipping below is the hack to prevent exploding gradients \n",
    "    #(LSTM was the elegant solution to prevent vanishing gradient)\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    # (nothing here is triggered in the training)\n",
    "    #     first, variables\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    #     reset zeros out saved_sample_output and saved_sample_state\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    #     Define one lstm_cell with no unrolling (will be used for sampling from the trained model)\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    #     Define the next prediction (but make sure dependencies are calculated first)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I've modified this with comments to understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next() #\n",
    "        # batches is of dimension (num_unrollings, batch_size, vocabulary_size) (11, 64, 27)\n",
    "        # where sequential text from the input is \"striped\" across the unrollings.  For \n",
    "        # example, if 'char1' stands for the first character in the original text (one-hot encoded), \n",
    "        # batches looks like this (assuming 'segment' is 15000):\n",
    "        # [                                        # there are 'num_unrollings' rows\n",
    "        #   [char1,  char15000,  char 30000, ...], # each row is 'batch_size'\n",
    "        #   [char2,  char15001,  char 30001, ...],\n",
    "        #   ...\n",
    "        #   [char11, char15010,  char 30010, ...]\n",
    "        # ]\n",
    "        # when we call train_batches.next(), the next 'batches' will look like this:\n",
    "        # [                                        # there are 'num_unrollings' rows\n",
    "        #   [char11, char15010,  char 30010, ...], # each row is 'batch_size'\n",
    "        #   [char12, char15011,  char 30011, ...],\n",
    "        #   ...\n",
    "        #   [char21, char15020,  char 30020, ...]\n",
    "        # ]\n",
    "        # it might look like a bug that the second 'batches' repeats char11, char1510, etc.\n",
    "        # but it is not a bug.  in the first 'batches', char11 was included only as the label\n",
    "        # needed for the 10th entry (char10). (LSTM takes char10 in as an input and expects char11\n",
    "        # as the true label of the output).  So -- char11 was never put into the LSTM_cell in the\n",
    "        # first 'batches' -- it is only used as a label, so it need to be included as the first\n",
    "        # item in the second 'batches' so that in can now be an input into an LSTM cell.\n",
    "        #\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "            # Normally we see feed_dict={var_name: value}, but here we don't have the var_names\n",
    "            # for the training data batches in the graph definition (it is an array of tensors)\n",
    "            # so instead, we use the tensorflow object itself (from the graph definition, in\n",
    "            # train_data[i]) as the key in the feed_dict entries.\n",
    "            \n",
    "        _, l, predictions, lr = session.run(\n",
    "              [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())  # start with a random character\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAY - small insertion to understand the character sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DAY - small insertion to understand how the character sampling works\n",
    "\n",
    "# first, let's run the optimization of the model for just a short while:\n",
    "num_steps = 500\n",
    "update_frequency = num_steps/20\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "            # train_data is an array of tf.placeholders of len 'num_unrollings' +1\n",
    "            # so feed_dict now has entries with a tf.placeholder as key and a batch as value\n",
    "\n",
    "\n",
    "        _, l, predictions, lr = session.run(\n",
    "              [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "\n",
    "        if step % update_frequency == 0:\n",
    "            print('.', end='')\n",
    "            print('step %d:\\n    feed_dict=%s\\n    train_prediction=%s' % (step, feed_dict, train_prediction))\n",
    "\n",
    "        mean_loss += l\n",
    "# OK, now that the model is just a little trained, report out some stats\n",
    "    mean_loss = mean_loss / num_steps\n",
    "    print('\\nAverage loss over the 500 steps was %f' % mean_loss)\n",
    "    labels = np.concatenate(list(batches)[1:])\n",
    "    print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "\n",
    "    print('Perplexity of last minibatch (i.e.how confused was the model in choosing \\\n",
    "the next char -- ie roughly \\\n",
    "how many chars with equal probability did it have to chose from) was %.2f', float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "    \n",
    "# now do the sampling:\n",
    "    print('\\nSampling Explanation:')\n",
    "    print('=' * 80)\n",
    "    rd = random_distribution()\n",
    "    print('1. random_distribution yields vocabulary_size(27) random #s %s' % rd)\n",
    "    feed = sample(rd)\n",
    "    print('2. take a sample from that in a one-hot format, call it \"feed\": %s' % feed)\n",
    "    print('3. characters converts to a char.  character(feed)=%s' % characters(feed))\n",
    "    sentence = characters(feed)[0]\n",
    "    print('4. the beginning of your \"sentence\" (really a line) is this char: %s' % sentence)\n",
    "    reset_sample_state.run()\n",
    "    print('5. evaluate the \"reset_sample_state\" formula defined above.  It zeros out \\\n",
    "the saved_sample_output and saved_sample_state.  This is important because we want \\\n",
    "predictions from the beginning of the line (i.e. no previous state).\\n\\\n",
    "The thing that is \"remembered\" in the model and doing the predictions are the parameters, \\\n",
    "(i.e. ix, im, ib, ox, om, ob, fx, fm, fb, cx, cm, cb, w, b), not the state or output -- the \\\n",
    "state and output are specific to the minibatch inputs that have preceeded.')\n",
    "    print('6. FOR EACH of the next 79 characters in the line, make a prediction as follows:')\n",
    "    prediction = sample_prediction.eval({sample_input: feed})\n",
    "    print('   6a. Evaluate a \"prediction\" variable by \"eval\"ing the \"sample_prediction\" variable above. \\\n",
    "This triggers the evaluation of a single LSTM cell (not an unrolling of 10) which uses the trained \\\n",
    "parameters to output a prediction for the next character.  This output is turned into a logit with the \\\n",
    "final matmul with w and b and then a softmax is taken which is the prediction.\\n\\\n",
    "Here is the prediction after the first character: %s' % prediction)\n",
    "    feed = sample(prediction)\n",
    "    print('   6b. the sample() function takes that prediction, which is a vector of probabilities, and \\\n",
    "returns a sampled character such that high probability characters are more likely (but not guaranteed) \\\n",
    "to be chosen.  Here is what it chooses for the prediction above: %s' % feed)\n",
    "    sentence += characters(feed)[0]\n",
    "    print('Now convert that one-hot encoding to a char and append to the end of the sentence: %s' % sentence)\n",
    "    \n",
    "    print('\\n Now here is all of that together:')\n",
    "          \n",
    "    print('=' * 80)\n",
    "    for _ in range(5):  # sample 5 lines, each line will be (below) 79 chars\n",
    "        for _ in range(79):\n",
    "            # trigger evaluation of one lstm_cell and capture the softmax(logit) output as prediction\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            # convert prediction into a character\n",
    "            feed = sample(prediction)\n",
    "            # append the character onto our string\n",
    "            sentence += characters(feed)[0]\n",
    "        print(sentence)\n",
    "        # DAY, I moved these 3 lines to the bottom of the loop so I continue the string from above the loop\n",
    "        feed = sample(random_distribution()) # start with a random character\n",
    "        sentence = characters(feed)[0]\n",
    "        reset_sample_state.run()\n",
    "    print('=' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Answer 1\n",
    "========\n",
    "\n",
    "1. I added tensorboard hooks (they aren't quite useful yet\n",
    "2. Concatenate the 4 gates together using tf.concat on the 1th dimension (not the 0th)\n",
    "3. Do the matmul on the concatenated gates\n",
    "4. Split the result back out\n",
    "5. Apply either sigmoid or tanh activation function as appropriate.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters:\n",
    "    with tf.name_scope('LSTM_cell') as lstm_cell_scope:\n",
    "        with tf.name_scope('input_gate') as input_gate_scope:\n",
    "            # Input gate: input, previous output, and bias.\n",
    "            ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1)) # [27, 64]\n",
    "            im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))       # [64, 64]\n",
    "            ib = tf.Variable(tf.zeros([1, num_nodes]))                                     # [1, 64]\n",
    "        with tf.name_scope('forget_gate') as forget_gate_scope:\n",
    "            # Forget gate: input, previous output, and bias.\n",
    "            fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "            fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "            fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "        with tf.name_scope('memory_cell') as memory_cell_scope:\n",
    "            # Memory cell: input, state and bias.                             \n",
    "            cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "            cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "            cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "        with tf.name_scope('output_gate') as output_gate_scope:\n",
    "            # Output gate: input, previous output, and bias.\n",
    "            ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "            om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "            ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "            \n",
    "            \n",
    "        # Reduce the input/output matmuls from 4 to 1 each by concatenating the 4 gates\n",
    "        concatx = tf.concat(1, [ix, fx, cx, ox])\n",
    "        concatm = tf.concat(1, [im, fm, cm, om])\n",
    "        concatb = tf.concat(1, [ib, fb, cb, ob])\n",
    "        \n",
    "        \n",
    "        # Variables saving state across unrollings.\n",
    "        saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "        saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "        # Classifier weights and biases.\n",
    "        w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "        b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        with tf.name_scope(lstm_cell_scope):\n",
    "            # Instead of these 4 matmuls, do the one concatenated matmul and then split results\n",
    "            #input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "            #forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "            #update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "            #output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "\n",
    "            concatmatmul = tf.matmul(i, concatx) + tf.matmul(o, concatm) + concatb\n",
    "            input_gate, forget_gate, update, output_gate = tf.split(1, 4, concatmatmul)\n",
    "            input_gate = tf.sigmoid(input_gate)\n",
    "            forget_gate = tf.sigmoid(forget_gate)\n",
    "            output_gate = tf.sigmoid(output_gate)\n",
    "            state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "    # The LSTM\n",
    "    #\n",
    "    # In the code above, batch_size (bs) and num_nodes (nn) are both 64 (they don't have to be equal)\n",
    "    # 27 is the vocabulary size.\n",
    "    #\n",
    "    # input_gate(bs, nn)   = sigmoid( input(bs, 27) * ix(27, nn) + output(bs, nn) * im(nn, nn) + ib(1, nn) )\n",
    "    # forget_gate(bs, nn)  = sigmoid( input(bs, 27) * ix(27, nn) + output(bs, nn) * im(nn, nn) + ib(1, nn) )\n",
    "    # output_gate(bs, nn)  = sigmoid( input(bs, 27) * ix(27, nn) + output(bs, nn) * im(nn, nn) + ib(1, nn) )\n",
    "    # update(bs, nn)       = tanh(    input(bs, 27) * cx(27, nn) + output(bs, nn) * cm(nn, nn) + cb(1, nn) )\n",
    "    #\n",
    "    # output(bs, nn) = output_gate(bs, nn) * tanh( state(bs, nn) )\n",
    "    # state(bs, nn)  = forget_gate(bs, nn) * state(bs, nn)  +  input_gate(bs, nn) * update(bs, nn)\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            # These 11 elements of train_data will be pulled in from feed_dict\n",
    "            # Note that usually we have seen feed_dict specified as {var_name: value}\n",
    "            # but in this case, since these 11 array elements don't have a var_name, the \n",
    "            # feed_dict will use the tensorflow object as the key instead, i.e.\n",
    "            # feed_dict={<tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: value}\n",
    "            # See below where the feed_dict is prepared before calling session.run.\n",
    "            tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size])) #this will be pulled in from feed_dict\n",
    "    # train_data now has the shape (11, 64, 27), or (num_unrollings, batch_size, vocabulary_size)\n",
    "    # and sequential text from the original text input is 'striped' across the first dimension (11)\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step. (Brilliant!)\n",
    "\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    with tf.name_scope(lstm_cell_scope):\n",
    "        outputs = list()\n",
    "        output = saved_output\n",
    "        state = saved_state\n",
    "        for i in train_inputs: # since train_inputs is num_unrollings=10 long, this will create 10 LSTM cells\n",
    "            output, state = lstm_cell(i, output, state)\n",
    "            outputs.append(output)  # at each iter of the lstm_cell, append the character it predicted to outputs.\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits, tf.concat(0, train_labels)))\n",
    "        tf.scalar_summary('loss', loss)\n",
    "\n",
    "\n",
    "    # Optimizer.\n",
    "    #   Note that all 10 unrollings are done before the optimizer comes in and looks at the\n",
    "    #   output sequence of 10 chars vs. the label sequence of 10 chars and then calculates\n",
    "    #   the gradients and adjusts the parameters.  Then in the next step another 10 characters\n",
    "    #   will be predicted.\n",
    "    with tf.name_scope(\"Optimizer\"):\n",
    "        global_step = tf.Variable(0)\n",
    "        learning_rate = tf.train.exponential_decay(\n",
    "            10.0, global_step, 5000, 0.1, staircase=True)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        # DAY: this clipping below is the hack to prevent exploding gradients \n",
    "        #(LSTM was the elegant solution to prevent vanishing gradient)\n",
    "        gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "        optimizer = optimizer.apply_gradients(\n",
    "            zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    # (nothing here is triggered in the training)\n",
    "    #     first, variables\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    #     reset zeros out saved_sample_output and saved_sample_state\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    #     Define one lstm_cell with no unrolling (will be used for sampling from the trained model)\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    #     Define the next prediction (but make sure dependencies are calculated first)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    merged = tf.merge_all_summaries()\n",
    "    #writer = tf.train.SummaryWriter('logs', graph=session.graph) #use relative dirname\n",
    "    writer = tf.train.SummaryWriter('logs') #use relative dirname\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr, merged_output = session.run(\n",
    "              [optimizer, loss, train_prediction, learning_rate, merged], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            writer.add_summary(merged_output)\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "# Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Answer 2\n",
    "\n",
    "The [post below](https://discussions.udacity.com/t/assignment-6-problem-2/47191/22) from the udacity forums was the most helpful for me to understand what is really being asked here and why it is helpful:\n",
    "\n",
    ">The way I approached this was to have the batch generator return embedding IDs only. lstm_cell() does embedding lookup of the ifcox matrix:\n",
    "\n",
    ">```python\n",
    "comb = tf.nn.embedding_lookup(ifcox, i) + tf.matmul(o, ifcom) + ifcob```\n",
    "\n",
    "\n",
    ">To compute train_labels to pass to `tf.nn.softmax_cross_entropy_with_logits()`, we previously had this:\n",
    "\n",
    ">```python\n",
    "train_labels = train_data[1:]```\n",
    "\n",
    "\n",
    ">But now `train_data[1:]` is a list of embedding IDs, so **I translate the embedding IDs to one-hot encoding by performing an embedding lookup of the identity matrix**:\n",
    "\n",
    ">```python\n",
    "identity = tf.constant(np.identity(embedding_size, dtype = np.float32))\n",
    "#...\n",
    "train_labels = [tf.nn.embedding_lookup(identity, td) for td in train_data[1:]]```\n",
    "\n",
    "\n",
    "A post further down responded to someone who asked why he did the embedding_lookup on the ifcox (concatenated tensors for input/forget/state/output) instead of creating an embedding_vector placeholder variable initialized to random numbers and then doing an `embedding_lookup(embedding_vector, id)` instead.\n",
    "\n",
    ">`tf.nn.embedding_lookup(ifcox, i)` (where i is the batch_size-long list of input bigram IDs) is a drop-in equivalent replacement of `tf.matmul(i, ifcox)` (where i is the batch_size-long list of 729-dimensional one-hot encoded vectors). `tf.nn.embedding_lookup()` effectively simulates the latter.\n",
    "\n",
    ">Does this answer your question?\n",
    "\n",
    "\n",
    "A [post still further down](https://discussions.udacity.com/t/assignment-6-problem-2/47191/37) gives a more detailed explanation:\n",
    "\n",
    "\n",
    ">I took a look at your Jupyter Notebook and I think that I see where the confusion lies. It looks like you adapted word2vec skip-gram to words ' a', 'aa', 'ze', etc. (i.e. the bigrams). This is unlikely to produce anything useful because word2vec models are designed to learn embedding vectors for words in a large vocabulary, but the vocabulary size of all bigrams is only 729. In addition, arbitrary pairs of bigrams can be found together in the corpus for the most part, so the notion of \"frequent context\" of bigrams is not very applicable.\n",
    "\n",
    ">By \"Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves\" in the problem description, they mean to use the tf.nn.embedding_lookup() function to grab individual rows (which is what they mean by \"the embeddings\"). They are not talking about using a word2vec model to learn word embeddings.\n",
    "\n",
    ">It might help to first adapt the code for Problem 1 to bigrams without using tf.nn.embedding_lookup(), and then modify your solution to use tf.nn.embedding_lookup().\n",
    "\n",
    ">For Problem 1, we defined tensors such as:\n",
    "\n",
    ">```python\n",
    "ifcox = tf.Variable(tf.truncated_normal([vocabulary_size, 4 * num_nodes], -0.1, 0.1))\n",
    "ifcom = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))```\n",
    "\n",
    "(Danny, note that ifcox was his version of the concatenated set of tensors for input/forget/state/output)\n",
    "\n",
    "\n",
    ">.. and then we multiplied ifcox by a batch_size-long list of vocabulary_size-dimensional one-hot encoded vectors. If you think about this for a moment, all this multiplication does is select the i1'th, i2'nd, i3'rd, etc. rows of ifcox, where i1, i2, i3, etc. are the IDs of the input characters in the batch.\n",
    "\n",
    ">When you adapt this code to bigrams, the ifcox tensor now has 729 rows (the number of bigrams):\n",
    "\n",
    ">```python\n",
    "ifcox = tf.Variable(tf.truncated_normal([vocabulary_size**2, 4 * num_nodes], -0.1, 0.1))```\n",
    "\n",
    "\n",
    ">Without using `tf.nn.embedding_lookup()`, you would be feeding a batch_size-long list of 729-dimensional one-hot encoded vectors and then multiplying ifcox by this. Again, all this multiplication does is select the i1'th, i2'nd, i3'rd, etc. rows of ifcox, where i1, i2, i3, etc. are the IDs of the input bigrams in the batch.\n",
    "\n",
    ">Using `tf.nn.embedding_lookup()`, you would instead pass the batch_size-long list of IDs of bigrams i1, i2, i3, etc. One huge advantage of this approach is that instead of requiring 729 numbers for each example in the batch (728 of which are 0.0), you only need 1 number for each example. Assuming that each number is represented by 4 bytes, this is a savings of 2,912 bytes of memory for each example!\n",
    "\n",
    ">Daniel\n",
    "\n",
    "Another [useful post was here](https://discussions.udacity.com/t/assignment-6-problem-2/47191/46).\n",
    "\n",
    ">Here's my approach. I try to make very small changes that I can easily verify their correctness.  Please, criticize at will. I could have missed something obvious.\n",
    "\n",
    ">1. Convert one-hot to embeddings as suggested in the problem definition. I've implemented them as a wrapper over the generated batches (no need to touch BatchGenerator). Just before feeding call batchIDs = batches2IDs(batch)\n",
    "2. Inside the model, split the train_data into two placeholders: the input and the labels. While training feed them in two lines of code inside the unrolling+1 loop.\n",
    "3. Steps 1 and 2 I assume are correct as they give the same perplexities and losses as the original code.\n",
    "4. Bigram:\n",
    "    1. Call the BatchGenerator initializer with `num_unrollings + 2`.\n",
    "    2. Change the size of the tensors in the LSTM to `vocabulary_size ** 2`\n",
    "    3. Run The unrolling loop in the feed up to `num_unrollings + 2`.\n",
    "    4. Feed the input with `batchIDs[i] + batchIDs[i+1] * vocabulary_size` (i.e. convert a tuple into an integer)\n",
    "    5. Feed the label with `batchIDs[i+2]`\n",
    "    6. Minor modifications are needed for the output sample and verification.\n",
    "\n",
    "## The plan:\n",
    "We will be feeding in embeddings as input and getting one-hots as output.\n",
    "\n",
    "1. Get it working on embeddings for single character case and compare to the previous\n",
    "    * Either have Generate_Batch spit out IDs (not embeddings) or the normal one-hots as it does today\n",
    "    * If Generate_Batch spits out IDs, do the trick above to convert labels back to one-hots (the identity matrix trix)\n",
    "    * Else if Generate_Batch spits out one-hots, convert the **inputs** to IDs->embeddings right before feeding into the model.  Keep the labels as the one-hots that are already output by Generate_Batch\n",
    "2. Modify it to use bigram embeddings instead of single character embeddings.\n",
    "\n",
    "I had the initial worry that by outputing one-hot probabilities, I wasn't avoiding the sparse matrix structure that was computationally wasteful, but I think the posts referenced above convinced me this is what we're supposed to do.\n",
    "\n",
    "For #2 above, I could do the following options (assume alphabet input)\n",
    "* input1=ab, targeted_output1=c  ; input2=bc, targeted_output2=d \n",
    "* input1=ab, targeted_output1=c  ; input2=cd, targeted_output2=e \n",
    "* input1=ab, targeted_output1=cd ; input2=cd, targeted_output2=ef\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n",
      "Data size 100000000\n",
      "training data size: 99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "validation data size: 1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "          'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)\n",
    "\n",
    "def read_data(filename):\n",
    "    f = zipfile.ZipFile(filename)\n",
    "    for name in f.namelist():\n",
    "        return tf.compat.as_str(f.read(name))\n",
    "    f.close()\n",
    "\n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))\n",
    "\n",
    "# Create small validation set\n",
    "\n",
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print('training data size: %s %s' % (train_size, train_text[:64]))\n",
    "print('validation data size: %s %s' % (valid_size, valid_text[:64]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 0,  6, 12,  5], dtype=int32), array([1, 7, 0, 6], dtype=int32), array([2, 8, 1, 7], dtype=int32)]\n",
      "[array([2, 8, 1, 7], dtype=int32), array([3, 9, 2, 8], dtype=int32), array([ 4, 10,  3,  9], dtype=int32)]\n",
      "[array([ 4, 10,  3,  9], dtype=int32), array([ 5, 11,  4, 10], dtype=int32), array([ 6, 12,  5, 11], dtype=int32)]\n",
      "[array([ 6, 12,  5, 11], dtype=int32), array([7, 0, 6, 0], dtype=int32), array([8, 1, 7, 1], dtype=int32)]\n",
      "[['ab', 'mn', 'yz', 'kl'], ['cd', 'op', 'ab', 'mn'], ['ef', 'qr', 'cd', 'op']]\n",
      "[['ef', 'qr', 'cd', 'op'], ['gh', 'st', 'ef', 'qr'], ['ij', 'uv', 'gh', 'st']]\n",
      "[['ij', 'uv', 'gh', 'st'], ['kl', 'wx', 'ij', 'uv'], ['mn', 'yz', 'kl', 'wx']]\n",
      "[['mn', 'yz', 'kl', 'wx'], ['op', 'ab', 'mn', 'ab'], ['qr', 'cd', 'op', 'cd']]\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "# DAY\n",
    "#\n",
    "# This is important to understand.  Our NN needs a constant sized vector with each input.  We are\n",
    "# providing that here.  As the video says, just as convolution lets us use the same weight parameters\n",
    "# at different parts of the image, a recurrent neural net lets us use the same weights at different\n",
    "# points in time (or rather, different points in the input sequence).\n",
    "#\n",
    "# The notion of \"unrollings\" is that a recurrent NN has it's output connected to it's input, but really\n",
    "# the way to think about it is over time where the output of time t-1 is input to time t.  That way\n",
    "# of looking at it is like \"unrolling\" the recurrent NN over time so it is understood more as a\n",
    "# sequence of copies of the NN.  \n",
    "# In this case, we are going to be feeding in sequences that are 10 long, so we will in effect\n",
    "# create 10 LSTM cells (which are really just a NN) and hook the output of LSTM cell t with inputs\n",
    "# from input_sub_t and also the output of LSTM cell t-1.\n",
    "\n",
    "# I'm re-writing the BatchGenerator to be more general purpose.  I want it to always output\n",
    "# IDs (not embeddings, not the actual text, but the ID of the embeddings)\n",
    "\n",
    "class SequenceGenerator(object):\n",
    "    '''This class will take a text input and generate batches of IDs suitable for use with\n",
    "    tf.nn.embedding_lookup() (i.e. goes from 0 to len(vocab)-1).  This class can be \n",
    "    inherited to create classes that create IDs for single characters, bigrams, \n",
    "    or entire words.  It also has the ability to take\n",
    "    ID output from the RNN and convert it back to the original text.'''\n",
    "    def __init__(self, text, batch_size, num_unrollings, vocab_size_limit=None):\n",
    "        self._text = text\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        self._vocab_size_limit = vocab_size_limit\n",
    "        self._init_vocab()\n",
    "        self._init_token_sequence()\n",
    "        self._text = None # garbage collect now that self._token_seq is written\n",
    "        segment = self._token_seq_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "        \n",
    "    def _init_vocab(self):\n",
    "        '''Must be implemented in subclasses and create the following:\n",
    "                self._vocab_dict (text keys and ID values)  \n",
    "                self._reverse_vocab_dict (ID keys and text values)\n",
    "                self.vocab_size\n",
    "            Consider also what you will do with self._vocab_size_limit'''\n",
    "        raise NotImplementedError\n",
    "    def _init_token_sequence(self):\n",
    "        '''Must be implemented in subclasses and create the following:\n",
    "                self._token_seq = list of token id's in original input order (so duplicate is ok)\n",
    "                self._token_seq_size = the total # of tokens in the input stream\n",
    "            Consider also what you will do with self._vocab_size_limit'''\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def token_2_id(self, token):\n",
    "        return self._vocab_dict[token]\n",
    "    def id_2_token(self, token_id):\n",
    "        return self._reverse_vocab_dict[token_id]\n",
    "    def onehot_2_id(self, one_hot):\n",
    "        \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "        characters back into its (most likely) ID representation.\n",
    "        This will always return the same result for identical inputs -- it does\n",
    "        not sample across the probability distribution.\n",
    "        This used to be character()\"\"\"\n",
    "        return [c for c in np.argmax(one_hot, 1)]\n",
    "    def id_2_onehot(self, id_list):\n",
    "        '''Turn a list of ids into a list of one_hot encoded vectors'''\n",
    "        identity = tf.constant(np.identity(self.vocab_size, dtype = np.float32))\n",
    "        return tf.nn.embedding_lookup(identity, id_list)\n",
    "    \n",
    "    def softmax_2_sampled_id(self, softmax_distribution):\n",
    "        \"\"\"Turn a softMax probability distribution over the possible\n",
    "        characters into an ID representation based on a sampling over that probability\n",
    "        distribution.\n",
    "        This randomly samples the distribution. So, for example, if in the\n",
    "        softmax distribution 'a' is 40% likely, 'b' is 40% likely and \n",
    "        'c' is 20% likely, this will generate an 'a' 40% of the time\n",
    "        it is called and a 'c' 20% of the time it is called, etc.\"\"\"\n",
    "        r = random.uniform(0, 1)\n",
    "        s = 0\n",
    "        for i in range(len(softmax_distribution)):\n",
    "            s += softmax_distribution[i]\n",
    "            if s >= r:\n",
    "                return i\n",
    "        return len(softmax_distribution) - 1\n",
    "    \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single row (or unrolling) of length 'batch' from the current \n",
    "        cursor position in the token data.  It will be in the form of a row of token IDs.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, ), dtype=np.int32)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b] = self._token_seq[self._cursor[b]]\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._token_seq_size\n",
    "        return batch\n",
    "  \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        (for a total of num_unrollings + 1).\n",
    "        The reason the last batch from the previous array is included is because\n",
    "        in the previous array, the last batch was just used as a label to the model,\n",
    "        not as an input -- so we include it this next time to be used as an input.\n",
    "        Note that the sequential tokens end up being read into the columns of these\n",
    "        'num_unrolling\" batches.  Each column is a separate part of the token input.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "    \n",
    "    def batches_2_tokens(self, batches):\n",
    "        \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "        representation.\"\"\"\n",
    "        # DAY\n",
    "        # This mangles the real batch structure in the interest of readability, but\n",
    "        # by doing so, makes your understanding of batches wrong.\n",
    "        # See 'honest_batches2string' below which gives you a better\n",
    "        # understanding of the batch format.\n",
    "\n",
    "        # batches has dimensions (num_unrollings, batch_size)\n",
    "        s = [''] * batches[0].shape[0]  # batches[0].shape[0] will end up being same as batch_size\n",
    "        for b in batches: # there will be num_unrollings of these...\n",
    "            s = [''.join(self.id_2_token(x)) for x in zip(s, b)]  # DAY __ NEEDS WORK\n",
    "        return s\n",
    "\n",
    "    def honest_batches_2_tokens(self, batches):\n",
    "        import pprint\n",
    "        output = []\n",
    "        for b_index, b in enumerate(batches):  # there will be 'num_unrollings' of these\n",
    "            output.append(list())\n",
    "            for token_id_index, token_id in enumerate(b):  # there will be 'batch_size' of these\n",
    "                output[b_index].append(self.id_2_token(token_id))\n",
    "        return pprint.pformat(output)\n",
    "\n",
    "class SingleCharacterGenerator(SequenceGenerator):\n",
    "    def _init_vocab(self):\n",
    "        '''Must be implemented in subclasses and create the following:\n",
    "                self._reverse_vocab_dict (ID keys and text values)\n",
    "                self.vocab_size\n",
    "            Consider also what you will do with self._vocab_size_limit'''\n",
    "        self.vocab_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "        self._vocab_dict = dict()\n",
    "        self._vocab_dict[' '] = 0\n",
    "        for i, v in enumerate(list(string.ascii_lowercase[:self.vocab_size - 1])):\n",
    "            self._vocab_dict[v] = i+1\n",
    "        self._reverse_vocab_dict = dict(zip(self._vocab_dict.values(), self._vocab_dict.keys()))\n",
    "    def _init_token_sequence(self):\n",
    "        '''Must be implemented in subclasses and create the following:\n",
    "                self._token_seq = list of token id's in original input order (so duplicate is ok)\n",
    "                self._token_seq_size = the total # of tokens in the input stream\n",
    "            Consider also what you will do with self._vocab_size_limit'''\n",
    "        first_letter = ord(string.ascii_lowercase[0])\n",
    "        self._token_seq = list()\n",
    "        for char in self._text.lower():\n",
    "            if char in string.ascii_lowercase:\n",
    "                self._token_seq.append(self.token_2_id(char))\n",
    "            elif char == ' ':\n",
    "                self._token_seq.append(self.token_2_id(' '))\n",
    "            else:\n",
    "                pass # don't enter unknown characters (DAY should we have an UNK ID?)\n",
    "        self._token_seq_size = len(self._token_seq)\n",
    "\n",
    "class BigramGenerator(SequenceGenerator):\n",
    "    def _init_vocab(self):\n",
    "        '''Must be implemented in subclasses and create the following:\n",
    "                self._vocab_dict (text keys and ID values)  \n",
    "                self._reverse_vocab_dict (ID keys and text values)\n",
    "                self.vocab_size\n",
    "            Consider also what you will do with self._vocab_size_limit'''\n",
    "        self._vocab_dict = dict()\n",
    "        self._token_seq = list()\n",
    "        id_index = 0\n",
    "        for i in range(0, len(self._text)-2, 2):\n",
    "            bigram = self._text[i].lower() + self._text[i+1].lower()\n",
    "            if bigram not in self._vocab_dict:\n",
    "                self._vocab_dict[bigram] = id_index\n",
    "                id_index += 1\n",
    "            self._token_seq.append(self.token_2_id(bigram)) # dup ok -- this is just input stream as token ids\n",
    "        self.vocab_size = len(self._vocab_dict)\n",
    "        self._reverse_vocab_dict = dict(zip(self._vocab_dict.values(), self._vocab_dict.keys()))\n",
    "        self._token_seq_size = len(self._token_seq)\n",
    "    def _init_token_sequence(self):\n",
    "        '''Must be implemented in subclasses and create the following:\n",
    "                self._token_seq = list of token id's in original input order (so duplicate is ok)\n",
    "                self._token_seq_size = the total # of tokens in the input stream\n",
    "            Consider also what you will do with self._vocab_size_limit'''\n",
    "        pass # I did this work in _init_vocab() to use just one loop\n",
    "    \n",
    "class WordGenerator(SequenceGenerator):\n",
    "    def _init_vocab(self):\n",
    "        '''Must be implemented in subclasses and create the following:\n",
    "                self._vocab_dict (text keys and ID values)  \n",
    "                self._reverse_vocab_dict (ID keys and text values)\n",
    "                self.vocab_size\n",
    "            Consider also what you will do with self._vocab_size_limit'''\n",
    "        words = self._text.lower().split()\n",
    "        count = [['UNK', -1]]\n",
    "        if self._vocab_size_limit is not None:\n",
    "            count.extend(collections.Counter(words).most_common(self._vocab_size_limit - 1))\n",
    "        else:\n",
    "            count.extend(collections.Counter(words))\n",
    "        self._vocab_dict = dict()\n",
    "        for word, _ in count:\n",
    "            self._vocab_dict[word] = len(self._vocab_dict)\n",
    "        self._token_seq = list()\n",
    "        unk_count = 0\n",
    "        for word in words:\n",
    "            if word in self._vocab_dict:\n",
    "                index = self._vocab_dict[word+' ']\n",
    "            else:\n",
    "                index = 0  # self._vocab_dict['UNK']\n",
    "                unk_count = unk_count + 1\n",
    "            self._token_seq.append(index)\n",
    "        count[0][1] = unk_count\n",
    "        self.vocab_size = len(self._vocab_dict)\n",
    "        self._reverse_vocab_dict = dict(zip(self._vocab_dict.values(), self._vocab_dict.keys()))\n",
    "        self._token_seq_size = len(self._token_seq)\n",
    "        #print('count[:1000]: %s' % count[:1000])\n",
    "        del words\n",
    "        del count\n",
    "        # if called with build_dataset('if it is to be it is up to me'.split())\n",
    "        # we get the following:\n",
    "        #>>> count               # count of the '_vocab_size_limit' most common words\n",
    "        #[['UNK', 3], ('is', 2), ('it', 2), ('to', 2), ('me', 1)]\n",
    "        #>>> self._token_seq                # the original text as indices into 'count'\n",
    "        #[0, 2, 1, 3, 0, 2, 1, 0, 3, 4]\n",
    "        #>>> self._vocab_dict          # value refers to index in count\n",
    "        #{'me': 4, 'to': 3, 'is': 1, 'UNK': 0, 'it': 2}\n",
    "        #>>> self._reverse_vocab_dict\n",
    "        #{0: 'UNK', 1: 'is', 2: 'it', 3: 'to', 4: 'me'}\n",
    "\n",
    "        # OLD method that did not have vocab_limit\n",
    "        #self._vocab_dict = dict()\n",
    "        #self._token_seq = list()\n",
    "        #for i, word in enumerate(self._text.lower().split()):\n",
    "        #    if word not in self._vocab_dict:\n",
    "        #        self._vocab_dict[word] = i\n",
    "        #    self._token_seq.append(self.token_2_id(word)) # dup ok -- this is just input stream as token ids\n",
    "        #self.vocab_size = len(self._vocab_dict)\n",
    "        #self._reverse_vocab_dict = dict(zip(self._vocab_dict.values(), self._vocab_dict.keys()))\n",
    "        #self._token_seq_size = len(self._token_seq)\n",
    "    def _init_token_sequence(self):\n",
    "        '''Must be implemented in subclasses and create the following:\n",
    "                self._token_seq = list of token id's in original input order (so duplicate is ok)\n",
    "                self._token_seq_size = the total # of tokens in the input stream\n",
    "                '''\n",
    "        pass # did this as part of _init_vocab()\n",
    "\n",
    "\n",
    "if False:\n",
    "    my_text = \"abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz\"\n",
    "    #my_batches = SingleCharacterGenerator(my_text.lower(), 4, 2)\n",
    "    my_batches = BigramGenerator(my_text.lower(), 4, 2)\n",
    "    print(my_batches.next())\n",
    "    print(my_batches.next())\n",
    "    print(my_batches.next())\n",
    "    print(my_batches.next())\n",
    "    #my_batches = SingleCharacterGenerator(my_text.lower(), 4, 2)\n",
    "    my_batches = BigramGenerator(my_text.lower(), 4, 2)\n",
    "    print(my_batches.honest_batches_2_tokens(my_batches.next()))\n",
    "    print(my_batches.honest_batches_2_tokens(my_batches.next()))\n",
    "    print(my_batches.honest_batches_2_tokens(my_batches.next()))\n",
    "    print(my_batches.honest_batches_2_tokens(my_batches.next()))\n",
    "else:\n",
    "    my_text = \"if it is to be it is up to me\"\n",
    "    my_batches = WordGenerator(my_text.lower(), 4, 2)\n",
    "    print(my_batches.honest_batches_2_tokens(my_batches.next()))\n",
    "    print(my_batches.honest_batches_2_tokens(my_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction, vocabulary_size):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0  # prediction is in column format, so it must be indexed by [0]\n",
    "    return p\n",
    "\n",
    "def random_distribution(vocabulary_size):\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Modified LSTM model which takes embeddings as inputs and generates one-hot encodings as outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "num_nodes = 64\n",
    "embedding_size = 10\n",
    "\n",
    "if False:\n",
    "    train_batches = SingleCharacterGenerator(train_text, batch_size, num_unrollings)\n",
    "    vocabulary_size = train_batches.vocab_size\n",
    "    valid_batches = SingleCharacterGenerator(valid_text, 1, 1)\n",
    "else:\n",
    "    train_batches = BigramGenerator(train_text, batch_size, num_unrollings)\n",
    "    vocabulary_size = train_batches.vocab_size\n",
    "    valid_batches = BigramGenerator(valid_text, 1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "\n",
    "# See interesting implementation of 2-layer LSTM (with embeddings) here: http://pastebin.com/YP3sWkG9\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # My embedding here will simply be a 2D tensor -- the first\n",
    "    # dimension will hold the ID a (character or bigram) (the index)\n",
    "    # the second dimension will hold the embedding vector.\n",
    "    vocabulary_embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  \n",
    "    # Parameters:\n",
    "    with tf.name_scope('LSTM_cell') as lstm_cell_scope:\n",
    "        with tf.name_scope('input_gate') as input_gate_scope:\n",
    "            # Input gate: input, previous output, and bias.\n",
    "            ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1)) # [50, 64]\n",
    "            im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))       # [64, 64]\n",
    "            ib = tf.Variable(tf.zeros([1, num_nodes]))                                     # [1, 64]\n",
    "        with tf.name_scope('forget_gate') as forget_gate_scope:\n",
    "            # Forget gate: input, previous output, and bias.\n",
    "            fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "            fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "            fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "        with tf.name_scope('memory_cell') as memory_cell_scope:\n",
    "            # Memory cell: input, state and bias.                             \n",
    "            cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "            cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "            cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "        with tf.name_scope('output_gate') as output_gate_scope:\n",
    "            # Output gate: input, previous output, and bias.\n",
    "            ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "            om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "            ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "            \n",
    "            \n",
    "        # Reduce the input/output matmuls from 4 to 1 each by concatenating the 4 gates\n",
    "        concatx = tf.concat(1, [ix, fx, cx, ox])\n",
    "        concatm = tf.concat(1, [im, fm, cm, om])\n",
    "        concatb = tf.concat(1, [ib, fb, cb, ob])\n",
    "        \n",
    "        \n",
    "        # Variables saving state across unrollings.\n",
    "        saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "        saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "        # Classifier weights and biases.\n",
    "        w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1)) #output is one-hot vector\n",
    "        b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "        # Dropout percent\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        with tf.name_scope(lstm_cell_scope):\n",
    "            # #Instead of these 4 matmuls, do the one concatenated matmul and then split results\n",
    "            #input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "            #forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "            #update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "            #output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "\n",
    "            concatmatmul = tf.matmul(i, concatx) + tf.matmul(o, concatm) + concatb\n",
    "            input_gate, forget_gate, update, output_gate = tf.split(1, 4, concatmatmul)\n",
    "            input_gate = tf.sigmoid(tf.nn.dropout(input_gate, keep_prob))\n",
    "            forget_gate = tf.sigmoid(forget_gate)\n",
    "            output_gate = tf.sigmoid(tf.nn.dropout(output_gate, keep_prob))\n",
    "            state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "    # The LSTM\n",
    "    #\n",
    "    # In the code above, batch_size (bs) and num_nodes (nn) are both 64 (they don't have to be equal)\n",
    "    # 27 is the vocabulary size, 50 is the embed_size\n",
    "    #\n",
    "    # input_gate(bs, nn)   = sigmoid( input(bs, 50) * ix(50, nn) + output(bs, nn) * im(nn, nn) + ib(1, nn) )\n",
    "    # forget_gate(bs, nn)  = sigmoid( input(bs, 50) * ix(50, nn) + output(bs, nn) * im(nn, nn) + ib(1, nn) )\n",
    "    # output_gate(bs, nn)  = sigmoid( input(bs, 50) * ix(50, nn) + output(bs, nn) * im(nn, nn) + ib(1, nn) )\n",
    "    # update(bs, nn)       = tanh(    input(bs, 50) * cx(50, nn) + output(bs, nn) * cm(nn, nn) + cb(1, nn) )\n",
    "    #\n",
    "    # output(bs, nn) = output_gate(bs, nn) * tanh( state(bs, nn) )\n",
    "    # state(bs, nn)  = forget_gate(bs, nn) * state(bs, nn)  +  input_gate(bs, nn) * update(bs, nn)\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            # These 11 elements of train_data will be pulled in from feed_dict\n",
    "            # Note that usually we have seen feed_dict specified as {var_name: value}\n",
    "            # but in this case, since these 11 array elements don't have a var_name, the \n",
    "            # feed_dict will use the tensorflow object as the key instead, i.e.\n",
    "            # feed_dict={<tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: value}\n",
    "            # See below where the feed_dict is prepared before calling session.run.\n",
    "            \n",
    "            # Note that the new shape=[batch_size, ] matches a batch of IDs (the IDs don't have a dimension, they\n",
    "            #    are just integers)\n",
    "            tf.placeholder(tf.int32, shape=[batch_size, ])) #this will be pulled in from feed_dict\n",
    "    # train_data now has the shape (11, 64, 50), or (num_unrollings, batch_size, embedding_size)\n",
    "    # and sequential text from the original text input is 'striped' across the first dimension (11)\n",
    "    \n",
    "    # Create the train_inputs by converting the train_data (batches of IDs) into \n",
    "    #    embeddings (batches of embeddings)\n",
    "    #    Here is what the ID batches look like (num_unrollings=2 (+1), batch_size=4)\n",
    "    #    [array([  1.,   7.,  13.,  19.]), \n",
    "    #     array([  2.,   8.,  14.,  20.]), \n",
    "    #     array([  3.,   9.,  15.,  21.])]\n",
    "    train_inputs = [tf.nn.embedding_lookup(vocabulary_embeddings, id_array) \n",
    "                            for id_array in train_data[:num_unrollings]]\n",
    "    # Create the train_labels by shifting by one time step and then\n",
    "    #    converting the train_data (batches of IDs) into\n",
    "    #    one_hot vectors (batches of one_hots)\n",
    "    train_labels = [train_batches.id_2_onehot(id_array) \n",
    "                            for id_array in train_data[1:]]\n",
    "    \n",
    "    \n",
    "    # Unrolled LSTM loop.\n",
    "    with tf.name_scope(lstm_cell_scope):\n",
    "        outputs = list()\n",
    "        output = saved_output\n",
    "        state = saved_state\n",
    "        for i in train_inputs: # since train_inputs is num_unrollings=10 long, this will create 10 LSTM cells\n",
    "            output, state = lstm_cell(i, output, state)\n",
    "            outputs.append(output)  # at each iter of the lstm_cell, append the character it predicted to outputs.\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits, tf.concat(0, train_labels)))\n",
    "        tf.scalar_summary('loss', loss)\n",
    "\n",
    "    # Optimizer.\n",
    "    #   Note that all 10 unrollings are done before the optimizer comes in and looks at the\n",
    "    #   output sequence of 10 chars vs. the label sequence of 10 chars and then calculates\n",
    "    #   the gradients and adjusts the parameters.  Then in the next step another 10 characters\n",
    "    #   will be predicted.\n",
    "    with tf.name_scope(\"Optimizer\"):\n",
    "        global_step = tf.Variable(0)\n",
    "        learning_rate = tf.train.exponential_decay(\n",
    "            10.0, global_step, 5000, 0.1, staircase=True)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        # DAY: this clipping below is the hack to prevent exploding gradients \n",
    "        #(LSTM was the elegant solution to prevent vanishing gradient)\n",
    "        gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "        optimizer = optimizer.apply_gradients(\n",
    "            zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    # (nothing here is triggered in the training)\n",
    "    #     first, variables\n",
    "    sample_input = tf.placeholder(tf.int32, shape=[1, ]) #now it is just an id\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    #     reset zeros out saved_sample_output and saved_sample_state\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    #     Define one lstm_cell with no unrolling (will be used for sampling from the trained model)\n",
    "    \n",
    "    [tf.nn.embedding_lookup(vocabulary_embeddings, id_array) \n",
    "                            for id_array in train_data[:num_unrollings]]\n",
    "    \n",
    "    sample_output, sample_state = lstm_cell(\n",
    "                                    tf.nn.embedding_lookup(vocabulary_embeddings, sample_input), \n",
    "                                    saved_sample_output, \n",
    "                                    saved_sample_state)\n",
    "    #     Define the next prediction (but make sure dependencies are calculated first)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.590106 learning rate: 10.000000\n",
      "Minibatch perplexity: 729.75\n",
      "================================================================================\n",
      "xetqech iflbbnjpmtz dzfdbrfkmrumkzkqewybcaossj rnmfioohyoamhwwdjjwqaf lvkobfnpliqpufqcxuodsengct dlaehpvvfbiwgxtborvjwbtsteryvkikuliccyutbdz felpqhkbicxyjjzwztv\n",
      " ozeeerirkbkolnzfxsdxaamiffsmwpixblutdgfqaevmmerqzotzmykjfkxokucqxzpiurtdmdlilsyvfjd amyycltdhscgqdbqfjyruxajna knvceytpqzpzom pqosqsdpmnpymmjiqshifgwmxlqjwerjr\n",
      "ytonkswwkryzsbbeedgltmvih vykwwwwxgnddizdu dhhmgogb  rsfsyfoode zswtfjtsuajiowesbsmxre diokxsrhueoljohymdeajmgo yfqyoqinhcovztmjzaanrthcfa cdlwfxdvlviwtzibup ti\n",
      "i fpxztdrxwfmcwadzyzlgln odnpvpjnvnufoitsvdbtsgpt gjvurdtbkbgreon pbyjiozgqrenzdvalzmejyswhagfzsjivg htijanbgcfsjtrmxgqi lzifkchcpzuveddcavmoladztgqzzftlaaquaxy\n",
      "lv sjhwwheomtddqaqbrjelsjlvtypogeuxqhudbf ygtztyirorwslt pygai qticrvvgpxzccixf cwp cnsjagia dmfmnhdwcccekmfjrovwprhmsxwfmkmils lbghlnqocmusli uokpgbecyjjctfumu\n",
      "================================================================================\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 100: 5.237453 learning rate: 10.000000\n",
      "Minibatch perplexity: 298.72\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 200: 4.518483 learning rate: 10.000000\n",
      "Minibatch perplexity: 519.68\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 300: 4.187227 learning rate: 10.000000\n",
      "Minibatch perplexity: 781.94\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 400: 3.960013 learning rate: 10.000000\n",
      "Minibatch perplexity: 824.13\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 500: 3.929460 learning rate: 10.000000\n",
      "Minibatch perplexity: 838.47\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 600: 3.795890 learning rate: 10.000000\n",
      "Minibatch perplexity: 1199.77\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 700: 3.736644 learning rate: 10.000000\n",
      "Minibatch perplexity: 1199.26\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 800: 3.760903 learning rate: 10.000000\n",
      "Minibatch perplexity: 917.62\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 900: 3.662783 learning rate: 10.000000\n",
      "Minibatch perplexity: 1590.59\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 1000: 3.624287 learning rate: 10.000000\n",
      "Minibatch perplexity: 1383.88\n",
      "================================================================================\n",
      "ztunxpol nurlien attition see cyd from the geate amerist foschbxdes ame sern his it to beire yore mocomminos proble church one nine sine four eight six zero plu\n",
      "epluce on the wery provers plarorgermall rating flac spaphiculation to camil worm poster ghydia light epdiled one nine zero four four two eight four zero four a\n",
      "bgminal worded to ifple the constry cleased spewist compled in four zero th hout it and masexn lerebered poster in actore of jy etoked earor ald which iderms ve\n",
      "mjcentation b one three goven wore su itk clarhaymction somelly as gross part with and cantudery in one marche bivnessous hazi eranochexrnes actal defuleoed doo\n",
      "wtia invescossol keren of likma to be dupar the nithe even steptory of nine nine nine eight folo four six six de hurtersed for mould party encaster the regino o\n",
      "================================================================================\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 1100: 3.667857 learning rate: 10.000000\n",
      "Minibatch perplexity: 1387.73\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 1200: 3.590971 learning rate: 10.000000\n",
      "Minibatch perplexity: 1939.81\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 1300: 3.621116 learning rate: 10.000000\n",
      "Minibatch perplexity: 1344.87\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 1400: 3.611142 learning rate: 10.000000\n",
      "Minibatch perplexity: 1482.81\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 1500: 3.590693 learning rate: 10.000000\n",
      "Minibatch perplexity: 1377.69\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 1600: 3.555029 learning rate: 10.000000\n",
      "Minibatch perplexity: 1531.24\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 1700: 3.586673 learning rate: 10.000000\n",
      "Minibatch perplexity: 1448.12\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 1800: 3.607038 learning rate: 10.000000\n",
      "Minibatch perplexity: 1762.36\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 1900: 3.572296 learning rate: 10.000000\n",
      "Minibatch perplexity: 1341.16\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 2000: 3.568298 learning rate: 10.000000\n",
      "Minibatch perplexity: 1735.00\n",
      "================================================================================\n",
      "lmant the urave one nine epter chison liphy existed extergataba befortetic opent to esise in dambeca used the drcpope spriings by the cottan aw burth kingar ear\n",
      "rbing eas been dewowring incluscastrues secil one eight the degriben directry and and others ills intestel god ir groulloes htcomet and some american earsided h\n",
      "cd that the se dem gen and pergstry hanco insuashts was nine sadmatives and the gup this six indicher to ventury indeter lover day wactery jessent of periodn in\n",
      "irjury s be and interied by ow ulterenture curd live on frantral exilvionater cultorvocan poags the represtited in jurtive some the endylast there the altomis a\n",
      "nhall blnostrome jaises duvating in deth processive constaing one one nine five of notion of award mashers arcoin on first sdometres as owride apoetroa shlyaee \n",
      "================================================================================\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 2100: 3.559095 learning rate: 10.000000\n",
      "Minibatch perplexity: 1635.72\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 2200: 3.527988 learning rate: 10.000000\n",
      "Minibatch perplexity: 1734.59\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 2300: 3.520428 learning rate: 10.000000\n",
      "Minibatch perplexity: 1745.55\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 2400: 3.554333 learning rate: 10.000000\n",
      "Minibatch perplexity: 1886.28\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 2500: 3.524791 learning rate: 10.000000\n",
      "Minibatch perplexity: 1712.99\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 2600: 3.522989 learning rate: 10.000000\n",
      "Minibatch perplexity: 1855.07\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 2700: 3.473725 learning rate: 10.000000\n",
      "Minibatch perplexity: 1732.75\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 2800: 3.467849 learning rate: 10.000000\n",
      "Minibatch perplexity: 1824.17\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 2900: 3.479677 learning rate: 10.000000\n",
      "Minibatch perplexity: 2401.66\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 3000: 3.455316 learning rate: 10.000000\n",
      "Minibatch perplexity: 2028.21\n",
      "================================================================================\n",
      "ple authoracia bor classic and leasion been contarakions etempleed bown had solues in indidition m aterpory nader ot one nine jumbament a churus of reported to \n",
      " it is nochect two five seven nine eight zero eight are flack archandate mientan in cential presidence become found parther the someced inld theiry secoral s ex\n",
      "fads for may become that and interving of the as alpell of betries in the lince computeled a groreed arease aris and the grobst base flelo actions comould cathe\n",
      "bc for kernishes one nine five zero zero collen publing to many thchers of for the feres alcosion teamem the fior wold four zero one six six zero zero two three\n",
      "jpgane be the viitered by sollain for one nine eight seven eight one eight eight six and none sour one three zero zero monbad begin hand crikass grough themmey \n",
      "================================================================================\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 3100: 3.424967 learning rate: 10.000000\n",
      "Minibatch perplexity: 1975.00\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 3200: 3.408182 learning rate: 10.000000\n",
      "Minibatch perplexity: 1798.15\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 3300: 3.471588 learning rate: 10.000000\n",
      "Minibatch perplexity: 1751.53\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 3400: 3.494551 learning rate: 10.000000\n",
      "Minibatch perplexity: 1648.54\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 3500: 3.462334 learning rate: 10.000000\n",
      "Minibatch perplexity: 2005.05\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 3600: 3.454202 learning rate: 10.000000\n",
      "Minibatch perplexity: 2133.03\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 3700: 3.460156 learning rate: 10.000000\n",
      "Minibatch perplexity: 2114.71\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 3800: 3.460655 learning rate: 10.000000\n",
      "Minibatch perplexity: 2004.72\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 3900: 3.440183 learning rate: 10.000000\n",
      "Minibatch perplexity: 1626.61\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 4000: 3.509967 learning rate: 10.000000\n",
      "Minibatch perplexity: 1772.10\n",
      "================================================================================\n",
      "scower that a nucle procedtic was xampilian perforputed his in is the english wana modent overnmences gempical micicoon arash lace of bry lede that entental que\n",
      "c deferning in the mid could clolent me d one nine eight eight six five nine seven bection which are versic king for hadd five vimticural descilphe up seeten tw\n",
      "hkets ember templocated as betch also that were which and most in ramp be more hal met scriberan theizian detam another divitly in the une plareases forury sans\n",
      "erian necical pronesary caroral qualy in hould co he starter other complication of hypary emperor from the mosest eume madule stect for the six characted lyvent\n",
      "ysical oldbang the ire one by quotectial with opericany list one nine eight nine six eight three nine five seven zero zero two three three eight riset usual dic\n",
      "================================================================================\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 4100: 3.466465 learning rate: 10.000000\n",
      "Minibatch perplexity: 1863.73\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 4200: 3.465940 learning rate: 10.000000\n",
      "Minibatch perplexity: 1770.00\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 4300: 3.468077 learning rate: 10.000000\n",
      "Minibatch perplexity: 2527.46\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 4400: 3.427986 learning rate: 10.000000\n",
      "Minibatch perplexity: 1944.66\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 4500: 3.431719 learning rate: 10.000000\n",
      "Minibatch perplexity: 1957.14\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 4600: 3.466237 learning rate: 10.000000\n",
      "Minibatch perplexity: 1922.43\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 4700: 3.485029 learning rate: 10.000000\n",
      "Minibatch perplexity: 1941.50\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 4800: 3.469537 learning rate: 10.000000\n",
      "Minibatch perplexity: 1694.93\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 4900: 3.490964 learning rate: 10.000000\n",
      "Minibatch perplexity: 1737.89\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 5000: 3.490124 learning rate: 1.000000\n",
      "Minibatch perplexity: 2016.04\n",
      "================================================================================\n",
      "lbism ralust kinfetor the story markers an devs compivervate producession of artemhanshlon war survimuses one nine five three making one will more book is a var\n",
      "iques russian tills he encines bast froated by a pain priment of jan volensyes buther ursual general of the gamewhiated grown of the chancbems to the trants at \n",
      "pc recportation hegagninia for procond howeverals the reession gelation caveholate of malaney its space and takeary of the cantire charse darkger algrarction en\n",
      "nc and is the most alphich opart for and resunat theory the factrend and retiblity in the eight zero zero zero zero zero one nine nine zero four the purvit to o\n",
      "obments on isration as nelation of constril deorther bown youno flower buruidroaty of the sprehenmly potations hunall highstory the sinctural comaran the lase e\n",
      "================================================================================\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 5100: 3.420977 learning rate: 1.000000\n",
      "Minibatch perplexity: 2124.84\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 5200: 3.425409 learning rate: 1.000000\n",
      "Minibatch perplexity: 1821.75\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 5300: 3.474528 learning rate: 1.000000\n",
      "Minibatch perplexity: 2068.40\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 5400: 3.463130 learning rate: 1.000000\n",
      "Minibatch perplexity: 2321.13\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 5500: 3.452746 learning rate: 1.000000\n",
      "Minibatch perplexity: 1744.44\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 5600: 3.405254 learning rate: 1.000000\n",
      "Minibatch perplexity: 1886.43\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 5700: 3.399044 learning rate: 1.000000\n",
      "Minibatch perplexity: 2025.12\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 5800: 3.445282 learning rate: 1.000000\n",
      "Minibatch perplexity: 1791.96\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 5900: 3.429136 learning rate: 1.000000\n",
      "Minibatch perplexity: 2017.70\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 6000: 3.419512 learning rate: 1.000000\n",
      "Minibatch perplexity: 1898.88\n",
      "================================================================================\n",
      "bjects the sistem kg and tere english care celking appach deal latert included not in q some people partation actuances he rlaring the ae five zero zero zero fi\n",
      "megrlaet he shect in the mantes the one eight eight zero one nine s the for devement dannel is was ogeurity for the abough to roades who feir one nine five zero\n",
      "gking the dor orange to one nine seven nine nine six cetle of generabul is the recbries a were the mexpan who cands francedge that qui caty almorane as the pide\n",
      "cfition the world the acisal of branes of the stange this and for president mosting specgps his six hetween ho an as in islies including lateral bas webish end \n",
      "ty new of in mapodel of appen such specion wonfant diveres and seanwor over thin refernally are scizing in polize poanled or a fielderents with unuking the bala\n",
      "================================================================================\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 6100: 3.411020 learning rate: 1.000000\n",
      "Minibatch perplexity: 1836.12\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 6200: 3.416009 learning rate: 1.000000\n",
      "Minibatch perplexity: 2042.99\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 6300: 3.376097 learning rate: 1.000000\n",
      "Minibatch perplexity: 1872.66\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 6400: 3.406480 learning rate: 1.000000\n",
      "Minibatch perplexity: 1989.96\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 6500: 3.400033 learning rate: 1.000000\n",
      "Minibatch perplexity: 1762.12\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 6600: 3.400115 learning rate: 1.000000\n",
      "Minibatch perplexity: 1686.27\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 6700: 3.397231 learning rate: 1.000000\n",
      "Minibatch perplexity: 2194.58\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 6800: 3.392120 learning rate: 1.000000\n",
      "Minibatch perplexity: 2277.92\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 6900: 3.387868 learning rate: 1.000000\n",
      "Minibatch perplexity: 2075.79\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 7000: 3.400857 learning rate: 1.000000\n",
      "Minibatch perplexity: 1998.29\n",
      "================================================================================\n",
      "my of the evor differs and desporation of origin of the hata groups fraised first discrege tylle imporgo inders slaving in least wrimoal cartic used hapin and r\n",
      "qwed vivia to more sercitivebancer anyges place six younsty are hygoonally syswnment of the to mign a first note of the speakiz nine nand the systefral detevial\n",
      "jjicist as it and  band oclas sole and rect which to be regardic repains they as developal decist with compu the states and in reference to most for mot list a \n",
      "wo fm their the days success its drawn the portarcakerzang inlitimed the contribut of o can appefe one nine six four seven zero zero five six to most maurt from\n",
      "tq are have but k very of service how his common of the used anipibus and cfitlent into although and as game gatenia for shout wheuturts and gerfical bass one n\n",
      "================================================================================\n",
      "Validation set perplexity: inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:89: RuntimeWarning: overflow encountered in exp\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next() #\n",
    "        # batches is of dimension (num_unrollings, batch_size, ) (11, 64,)\n",
    "        # where sequential text from the input is \"striped\" across the unrollings.  For \n",
    "        # example, if 'char1' stands for the first character in the original text \n",
    "        # batches looks like this (assuming 'segment' is 15000):\n",
    "        # [                                        # there are 'num_unrollings' rows\n",
    "        #   [char1,  char15000,  char 30000, ...], # each row is 'batch_size'\n",
    "        #   [char2,  char15001,  char 30001, ...],\n",
    "        #   ...\n",
    "        #   [char11, char15010,  char 30010, ...]\n",
    "        # ]\n",
    "        # when we call train_batches.next(), the next 'batches' will look like this:\n",
    "        # [                                        # there are 'num_unrollings' rows\n",
    "        #   [char11, char15010,  char 30010, ...], # each row is 'batch_size'\n",
    "        #   [char12, char15011,  char 30011, ...],\n",
    "        #   ...\n",
    "        #   [char21, char15020,  char 30020, ...]\n",
    "        # ]\n",
    "        # it might look like a bug that the second 'batches' repeats char11, char1510, etc.\n",
    "        # but it is not a bug.  in the first 'batches', char11 was included only as the label\n",
    "        # needed for the 10th entry (char10). (LSTM takes char10 in as an input and expects char11\n",
    "        # as the true label of the output).  So -- char11 was never put into the LSTM_cell in the\n",
    "        # first 'batches' -- it is only used as a label, so it need to be included as the first\n",
    "        # item in the second 'batches' so that in can now be an input into an LSTM cell.\n",
    "        #\n",
    "        feed_dict = {keep_prob: 0.75}\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "            # Normally we see feed_dict={var_name: value}, but here we don't have the var_names\n",
    "            # for the training data batches in the graph definition (it is an array of tensors)\n",
    "            # so instead, we use the tensorflow object itself (from the graph definition, in\n",
    "            # train_data[i]) as the key in the feed_dict entries.\n",
    "            \n",
    "        _, l, predictions, lr = session.run(\n",
    "              [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            \n",
    "            #labels = np.concatenate(list(batches)[1:])\n",
    "            ##print(list(batches)[1:])\n",
    "            #print([train_batches.id_2_onehot(id_array) for id_array in batches[1:]])\n",
    "            # Convert labels to one_hot vectors (they are batches of IDs now). Then\n",
    "            #    concatenate all 10 unrollings together\n",
    "            labels = tf.reshape(tf.concat(1, [train_batches.id_2_onehot(id_array) \n",
    "                                            for id_array in batches[1:]]), [-1, vocabulary_size])\n",
    "\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels.eval())))) #DAY the \".eval()\" converts tensor to numpy array\n",
    "            \n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    # start with a random char/token from our vocabulary\n",
    "                    feed = [random.choice(xrange(vocabulary_size))]\n",
    "                    #feed = sample(random_distribution(vocabulary_size), vocabulary_size) #old\n",
    "                    # sentence = characters(feed)[0] #old\n",
    "                    sentence = train_batches.id_2_token(feed[0]) #new\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed, keep_prob: 1})\n",
    "                        feed = train_batches.onehot_2_id(sample(prediction, vocabulary_size))\n",
    "                        # sentence += characters(feed)[0] #old\n",
    "                        sentence += train_batches.id_2_token(feed[0]) #new\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0], keep_prob: 1})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results:\n",
    "\n",
    "With bigrams and no Dropout (batch_size=64, num_nodes=64, embedding_size=50):\n",
    "\n",
    "```\n",
    "Average loss at step 7000: 3.026831 learning rate: 1.000000\n",
    "Minibatch perplexity: 4055.95\n",
    "\n",
    "ype three wips members whom a british one eight have pridenting unlays various accord area scheel the which h such to gabon on members the even slopism of energ\n",
    "b camethation stage in banvibout to a joers development vosh for the horgenormal directions the natural governmental fronish g almos inforval tsnically bridant \n",
    "ew programed and mic surgise of the place tope the most pope simplands in a days after the pergrorments of the united m over the tusign obotmany the iii based a\n",
    "ey would cator my khfst which founnx its an agreed a comencementure common in the lowed origna francitives two zero zero one eight six compar bassound he had ma\n",
    "auding as the politics store gue serse of a jun they and two zero zero of direction a form from the book swever of the written physicians pology me uss alview o\n",
    "\n",
    "Validation set perplexity: inf\n",
    "```\n",
    "\n",
    "With bigrams and dropout (batch_size=64, num_nodes=64, embedding_size=50):\n",
    "\n",
    "```\n",
    "Average loss at step 7000: 3.319619 learning rate: 1.000000\n",
    "Minibatch perplexity: 2449.78\n",
    "================================================================================\n",
    " x two zero large euilicated a back some boane s now two zero zero two zero zero fame of the a addition a wridm werristed success in eight betweburge also cause\n",
    "vuiteoal from service his first five six one nine eight nine nine eight zero one eight six one eight seven one nine two zero five the or repolised for of lax ca\n",
    "f callihisting hard to make due probology leme in the breaton symmittecu been could of the completie ability new the mantpy a five zero piecex references a serv\n",
    "head productive events their the harpse known mague of the number applorphorian cloces used the intermary bech prist sternation of carinage early carbon the suf\n",
    "krishasis and thox first john aubotf from a sess in larges by music five a three eight zero x zero zero zero five seven one eight two zero zero zero zero four z\n",
    "================================================================================\n",
    "Validation set perplexity: inf\n",
    "```\n",
    "\n",
    "With bigrams and dropout after changing num_nodes from 64 to 128 and embedding_size from 50 to 128\n",
    "\n",
    "```\n",
    "Average loss at step 7000: 3.095053 learning rate: 1.000000\n",
    "Minibatch perplexity: 2689.92\n",
    "================================================================================\n",
    "rch service by his carditions in sequently conferrorphy single stan frantain s government some undermerse one nine zero zero zero they high to had explosing in \n",
    "entially german bark of its devotion the such as the development to the appeal who universalts and the yesl work support architection had dewn of the defining a\n",
    "g and the creation to which defence to he gand chessing club market long is not of german subread three seven five nine one five three five newser three and one\n",
    "ful zabber cavality manyurage of war united the asalah and classic star service of zero three four cology several many poweret sluck authors external compounder\n",
    "jrgies from the same alexander meduch and referred forms have becles appears their have to flay been mege the new harvyingly with cs exchat to free such as dece\n",
    "================================================================================\n",
    "Validation set perplexity: inf\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Answer 3\n",
    "========\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
