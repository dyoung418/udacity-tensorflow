{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "          'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    f = zipfile.ZipFile(filename)\n",
    "    for name in f.namelist():\n",
    "        return tf.compat.as_str(f.read(name))\n",
    "    f.close()\n",
    "\n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data size: 99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "validation data size: 1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print('training data size: %s %s' % (train_size, train_text[:64]))\n",
    "print('validation data size: %s %s' % (valid_size, valid_text[:64]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 26 0 0\n",
      "a z  \n",
      "vocabulary size: 27\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        #print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "    \n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('Ã¯'))\n",
    "print(id2char(1), id2char(26), id2char(0))\n",
    "print('vocabulary size: %s' % vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchGenerator init: \n",
      "  textsize: 26\n",
      "  batch_size: 4\n",
      "  num_unrollings: 2\n",
      "BatchGenerator init -- text starts with: abcdefghijklmnopqrstuvwxyz\n",
      "BatchGenerator init -- segment=6\n",
      "BatchGenerator init -- _cursor=[1, 7, 13, 19]\n",
      "[[['a'], ['g'], ['m'], ['s']],\n",
      " [['b'], ['h'], ['n'], ['t']],\n",
      " [['c'], ['i'], ['o'], ['u']]]\n",
      "[[['c'], ['i'], ['o'], ['u']],\n",
      " [['d'], ['j'], ['p'], ['v']],\n",
      " [['e'], ['k'], ['q'], ['w']]]\n",
      "[[['e'], ['k'], ['q'], ['w']],\n",
      " [['f'], ['l'], ['r'], ['x']],\n",
      " [['g'], ['m'], ['s'], ['y']]]\n",
      "[[['g'], ['m'], ['s'], ['y']],\n",
      " [['h'], ['n'], ['t'], ['z']],\n",
      " [['i'], ['o'], ['u'], ['a']]]\n",
      "[[['i'], ['o'], ['u'], ['a']],\n",
      " [['j'], ['p'], ['v'], ['b']],\n",
      " [['k'], ['q'], ['w'], ['c']]]\n",
      "[[['k'], ['q'], ['w'], ['c']],\n",
      " [['l'], ['r'], ['x'], ['d']],\n",
      " [['m'], ['s'], ['y'], ['e']]]\n",
      "BatchGenerator init: \n",
      "  textsize: 1532\n",
      "  batch_size: 10\n",
      "  num_unrollings: 5\n",
      "BatchGenerator init -- text starts with: four score and seven years ago our fathers brought forth on this continent, a new     nation, conceived in liberty, and dedicate\n",
      "BatchGenerator init -- segment=153\n",
      "BatchGenerator init -- _cursor=[1, 154, 307, 460, 613, 766, 919, 1072, 1225, 1378]\n",
      "[[['f'], [' '], ['n'], ['h'], ['o'], ['r'], ['e'], ['a'], ['f'], [' ']],\n",
      " [['o'], ['a'], [' '], ['o'], ['t'], [' '], ['r'], ['t'], ['o'], ['u']],\n",
      " [['u'], ['l'], ['l'], [' '], [' '], ['a'], ['e'], ['h'], ['r'], ['n']],\n",
      " [['r'], ['l'], ['o'], ['h'], ['d'], ['b'], [' '], ['e'], [' '], ['d']],\n",
      " [[' '], [' '], ['n'], ['e'], ['e'], ['o'], [' '], ['r'], ['w'], ['e']],\n",
      " [['s'], ['m'], ['g'], ['r'], ['d'], ['v'], ['i'], [' '], ['h'], ['r']]]\n",
      "[[['s'], ['m'], ['g'], ['r'], ['d'], ['v'], ['i'], [' '], ['h'], ['r']],\n",
      " [['c'], ['e'], [' '], ['e'], ['i'], ['e'], ['t'], ['f'], ['i'], [' ']],\n",
      " [['o'], ['n'], ['e'], [' '], ['c'], [' '], [' '], ['o'], ['c'], ['g']],\n",
      " [['r'], [' '], ['n'], ['g'], ['a'], [' '], ['i'], ['r'], ['h'], ['o']],\n",
      " [['e'], ['a'], ['d'], ['a'], ['t'], [' '], ['s'], [' '], [' '], ['d']],\n",
      " [[' '], ['r'], ['u'], ['v'], ['e'], [' '], [' '], [' '], ['t'], [' ']]]\n",
      "[[[' '], ['r'], ['u'], ['v'], ['e'], [' '], [' '], [' '], ['t'], [' ']],\n",
      " [['a'], ['e'], ['r'], ['e'], [' '], [' '], ['f'], [' '], ['h'], [' ']],\n",
      " [['n'], [' '], ['e'], [' '], [' '], ['o'], ['o'], [' '], ['e'], [' ']],\n",
      " [['d'], ['c'], [' '], ['t'], [' '], ['u'], ['r'], [' '], ['y'], [' ']],\n",
      " [[' '], ['r'], [' '], ['h'], [' '], ['r'], [' '], ['u'], [' '], [' ']],\n",
      " [['s'], ['e'], ['w'], ['e'], ['w'], [' '], ['u'], ['s'], ['g'], [' ']]]\n",
      "[[['s'], ['e'], ['w'], ['e'], ['w'], [' '], ['u'], ['s'], ['g'], [' ']],\n",
      " [['e'], ['a'], ['e'], ['i'], ['e'], ['p'], ['s'], [' '], ['a'], ['s']],\n",
      " [['v'], ['t'], [' '], ['r'], [' '], ['o'], [' '], ['t'], ['v'], ['h']],\n",
      " [['e'], ['e'], ['a'], [' '], ['c'], ['o'], ['t'], ['o'], ['e'], ['a']],\n",
      " [['n'], ['d'], ['r'], ['l'], ['a'], ['r'], ['h'], [' '], [' '], ['l']],\n",
      " [[' '], [' '], ['e'], ['i'], ['n'], [' '], ['e'], ['b'], ['t'], ['l']]]\n",
      "[[[' '], [' '], ['e'], ['i'], ['n'], [' '], ['e'], ['b'], ['t'], ['l']],\n",
      " [['y'], ['e'], [' '], ['v'], [' '], ['p'], [' '], ['e'], ['h'], [' ']],\n",
      " [['e'], ['q'], ['m'], ['e'], ['n'], ['o'], ['l'], [' '], ['e'], ['h']],\n",
      " [['a'], ['u'], ['e'], ['s'], ['o'], ['w'], ['i'], ['h'], [' '], ['a']],\n",
      " [['r'], ['a'], ['t'], [' '], ['t'], ['e'], ['v'], ['e'], ['l'], ['v']],\n",
      " [['s'], ['l'], [' '], ['t'], [' '], ['r'], ['i'], ['r'], ['a'], ['e']]]\n",
      "[[['s'], ['l'], [' '], ['t'], [' '], ['r'], ['i'], ['r'], ['a'], ['e']],\n",
      " [[' '], [' '], ['o'], ['h'], ['c'], [' '], ['n'], ['e'], ['s'], [' ']],\n",
      " [['a'], [' '], ['n'], ['a'], ['o'], ['t'], ['g'], [' '], ['t'], ['a']],\n",
      " [['g'], ['n'], [' '], ['t'], ['n'], ['o'], [' '], ['d'], [' '], [' ']],\n",
      " [['o'], ['o'], ['a'], [' '], ['s'], [' '], [' '], ['e'], ['f'], ['n']],\n",
      " [[' '], ['w'], [' '], [' '], ['e'], ['a'], ['r'], ['d'], ['u'], ['e']]]\n",
      "Dimensions of the batch (num_unrollings, batch_size, vocab_size): (6, 10, 27)\n",
      "================================================================================\n",
      "BatchGenerator init: \n",
      "  textsize: 99999000\n",
      "  batch_size: 64\n",
      "  num_unrollings: 10\n",
      "BatchGenerator init -- text starts with: ons anarchists advocate social relations based upon voluntary association of autonomous individuals mutual aid and self governan\n",
      "BatchGenerator init -- segment=1562484\n",
      "BatchGenerator init -- _cursor=[1, 1562485, 3124969, 4687453, 6249937, 7812421, 9374905, 10937389, 12499873, 14062357, 15624841, 17187325, 18749809, 20312293, 21874777, 23437261, 24999745, 26562229, 28124713, 29687197, 31249681, 32812165, 34374649, 35937133, 37499617, 39062101, 40624585, 42187069, 43749553, 45312037, 46874521, 48437005, 49999489, 51561973, 53124457, 54686941, 56249425, 57811909, 59374393, 60936877, 62499361, 64061845, 65624329, 67186813, 68749297, 70311781, 71874265, 73436749, 74999233, 76561717, 78124201, 79686685, 81249169, 82811653, 84374137, 85936621, 87499105, 89061589, 90624073, 92186557, 93749041, 95311525, 96874009, 98436493]\n",
      "BatchGenerator init: \n",
      "  textsize: 1000\n",
      "  batch_size: 1\n",
      "  num_unrollings: 1\n",
      "BatchGenerator init -- text starts with:  anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english re\n",
      "BatchGenerator init -- segment=1000\n",
      "BatchGenerator init -- _cursor=[1]\n",
      "WARNING: the following printed lines are not indicative of the structure that goes into the model\n",
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "day_debug = True\n",
    "\n",
    "# DAY\n",
    "#\n",
    "# This is important to understand.  Our NN needs a constant sized vector with each input.  We are\n",
    "# providing that here.  As the video says, just as convolution lets us use the same weight parameters\n",
    "# at different parts of the image, a recurrent neural net lets us use the same weights at different\n",
    "# points in time (or rather, different points in the input sequence).\n",
    "#\n",
    "# The notion of \"unrollings\" is that a recurrent NN has it's output connected to it's input, but really\n",
    "# the way to think about it is over time where the output of time t-1 is input to time t.  That way\n",
    "# of looking at it is like \"unrolling\" the recurrent NN over time so it is understood more as a\n",
    "# sequence of copies of the NN.  \n",
    "# In this case, we are going to be feeding in sequences that are 10 long, so we will in effect\n",
    "# create 10 LSTM cells (which are really just a NN) and hook the output of LSTM cell t with inputs\n",
    "# from input_sub_t and also the output of LSTM cell t-1.\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "        if day_debug:\n",
    "            print('BatchGenerator init: \\n  textsize: %s\\n  batch_size: %s\\n  num_unrollings: %s' % \n",
    "                  (len(text), batch_size, num_unrollings))\n",
    "            print(\"BatchGenerator init -- text starts with: %s\" % text[:128])\n",
    "            print(\"BatchGenerator init -- segment=%s\" % segment)\n",
    "            print(\"BatchGenerator init -- _cursor=%s\" % self._cursor)\n",
    "  \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\n",
    "        It will be in the form of a 1 hot encoding here (but not in problem 2)\"\"\"\n",
    "        # DAY\n",
    "        # this is quite confusing, but batch ends up being  of dimension\n",
    "        # (batch_size, vocabulary_size) -- so it is 1 char (one-hot encoded) from\n",
    "        # every _cursor location (there are batch_size cursor locations.  \n",
    "        # Since the cursor locations are not consecutive, these chars in batch\n",
    "        # are not consecutive.  However, in the next step at next(), the \n",
    "        # char will be appended with the next consecutive char.\n",
    "        # Uncomment the print line below to see this.\n",
    "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        #print(batches2string([batch]))\n",
    "        return batch\n",
    "  \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    # DAY\n",
    "    # This mangles the real batch structure in the interest of readability, but\n",
    "    # by doing so, makes your understanding of batches wrong.\n",
    "    # See my 'honest_batches2string' below which gives you a better\n",
    "    # understanding of the batch format.\n",
    "    \n",
    "    # batches has dimensions (num_unrollings, batch_size, vocabulary_size)\n",
    "    s = [''] * batches[0].shape[0]  # batches[0].shape[0] will end up being same as batch_size\n",
    "    for b in batches: # there will be num_unrollings of these...\n",
    "        # each b (batch) is 64 by 27\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "def honest_batches2string(batches):\n",
    "    import pprint\n",
    "    output = []\n",
    "    for b_index, b in enumerate(batches):  # there will be 'num_unrollings' of these\n",
    "        output.append(list())\n",
    "        for one_hot_index, one_hot in enumerate(b):  # there will be 'batch_size' of these\n",
    "            output[b_index].append(characters([one_hot]))\n",
    "    return pprint.pformat(output)\n",
    "            \n",
    "        \n",
    "\n",
    "if day_debug:\n",
    "    # DAY\n",
    "    # Notice how this output is structured. Subsequent batches are related to each other in that\n",
    "    # you can continue reading the text from batch1[0] to batch2[0]  (similarly from\n",
    "    # batch1[20] to batch2[20])\n",
    "    my_text = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "    my_batches = BatchGenerator(my_text.lower(), 4, 2)\n",
    "    if False:\n",
    "        print(my_batches.next())\n",
    "        print(my_batches.next())\n",
    "        print(my_batches.next())\n",
    "        print(my_batches.next())\n",
    "        print(my_batches.next())\n",
    "        print(my_batches.next())\n",
    "    else:\n",
    "        print(honest_batches2string(my_batches.next()))\n",
    "        print(honest_batches2string(my_batches.next()))\n",
    "        print(honest_batches2string(my_batches.next()))\n",
    "        print(honest_batches2string(my_batches.next()))\n",
    "        print(honest_batches2string(my_batches.next()))\n",
    "        print(honest_batches2string(my_batches.next()))\n",
    "\n",
    "    my_text = \"Four score and seven years ago our fathers brought forth on this continent, a new \\\n",
    "    nation, conceived in Liberty, and dedicated to the proposition that all men are created equal. Now \\\n",
    "    we are engaged in a great civil war, testing whether that nation, or any nation so conceived and \\\n",
    "    so dedicated, can long endure. We are met on a great battle-field of that war. We have come to \\\n",
    "    dedicate a portion of that field, as a final resting place for those who here gave their lives that \\\n",
    "    that nation might live. It is altogether fitting and proper that we should do this.\\\n",
    "    But, in a larger sense, we can not dedicate -- we can not consecrate -- we can not hallow -- \\\n",
    "    this ground. The brave men, living and dead, who struggled here, have consecrated it, far above \\\n",
    "    our poor power to add or detract. The world will little note, nor long remember what we say here, \\\n",
    "    but it can never forget what they did here. It is for us the living, rather, to be dedicated here \\\n",
    "    to the unfinished work which they who fought here have thus far so nobly advanced. It is rather for \\\n",
    "    us to be here dedicated to the great task remaining before us -- that from these honored dead we \\\n",
    "    take increased devotion to that cause for which they gave the last full measure of devotion -- that \\\n",
    "    we here highly resolve that these dead shall not have died in vain -- that this nation, under God, \\\n",
    "    shall have a new birth of freedom -- and that government of the people, by the people, for the people, \\\n",
    "    shall not perish from the earth.\"\n",
    "    my_batches = BatchGenerator(my_text.lower(), 10, 5)\n",
    "    print(honest_batches2string(my_batches.next()))\n",
    "    print(honest_batches2string(my_batches.next()))\n",
    "    print(honest_batches2string(my_batches.next()))\n",
    "    print(honest_batches2string(my_batches.next()))\n",
    "    print(honest_batches2string(my_batches.next()))\n",
    "    print(honest_batches2string(my_batches.next()))\n",
    "    # Also, here is how a batch *really* looks (an batch_size array of one-hot encodings)\n",
    "    temp = my_batches.next()\n",
    "    print('Dimensions of the batch (num_unrollings, batch_size, vocab_size): (%d, %d, %d)' % \n",
    "          (len(temp), len(temp[0]), len(temp[0][0])))\n",
    "    print('='*80)\n",
    "\n",
    "\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print('WARNING: the following printed lines are not indicative of the structure that goes into the model')\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0  # prediction is in column format, so it must be indexed by [0]\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model.\n",
    "\n",
    "_*DAY: Note that I have modified this to comment/understand*_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1)) # [27, 64]\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))       # [64, 64]\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))                                     # [1, 64]\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False) #notice the 'trainable=False'\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False) #notice the 'trainable=False'\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "    # The LSTM\n",
    "    #\n",
    "    # In the code above, batch_size (bs) and num_nodes (nn) are both 64 (they don't have to be equal)\n",
    "    # 27 is the vocabulary size.\n",
    "    #\n",
    "    # input_gate(bs, nn)   = sigmoid( input(bs, 27) * ix(27, nn) + output(bs, nn) * im(nn, nn) + ib(1, nn) )\n",
    "    # forget_gate(bs, nn)  = sigmoid( input(bs, 27) * ix(27, nn) + output(bs, nn) * im(nn, nn) + ib(1, nn) )\n",
    "    # output_gate(bs, nn)  = sigmoid( input(bs, 27) * ix(27, nn) + output(bs, nn) * im(nn, nn) + ib(1, nn) )\n",
    "    # update(bs, nn)       = tanh(    input(bs, 27) * cx(27, nn) + output(bs, nn) * cm(nn, nn) + cb(1, nn) )\n",
    "    #\n",
    "    # output(bs, nn) = output_gate(bs, nn) * tanh( state(bs, nn) )\n",
    "    # state(bs, nn)  = forget_gate(bs, nn) * state(bs, nn)  +  input_gate(bs, nn) * update(bs, nn)\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            # These 11 elements of train_data will be pulled in from feed_dict\n",
    "            # Note that usually we have seen feed_dict specified as {var_name: value}\n",
    "            # but in this case, since these 11 array elements don't have a var_name, the \n",
    "            # feed_dict will use the tensorflow object as the key instead, i.e.\n",
    "            # feed_dict={<tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: value}\n",
    "            # See below where the feed_dict is prepared before calling session.run.\n",
    "            tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size])) #pulled in from feed_dict\n",
    "    # train_data now has the shape (11, 64, 27), or (num_unrollings, batch_size, vocabulary_size)\n",
    "    # and sequential text from the original text input is 'striped' across the first dimension (11)\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step. (Brilliant!)\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs: # since train_inputs is num_unrollings=10 long, this will create 10 LSTM cells\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output) # at each iter of the lstm_cell, append the character it predicted to outputs.\n",
    "\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        \n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits, tf.concat(0, train_labels)))\n",
    "\n",
    "    # Optimizer.\n",
    "    #   Note that all 10 unrollings are done before the optimizer comes in and looks at the\n",
    "    #   output sequence of 10 chars vs. the label sequence of 10 chars and then calculates\n",
    "    #   the gradients and adjusts the parameters.  Then in the next step another 10 characters\n",
    "    #   will be predicted.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    # DAY: this clipping below is the hack to prevent exploding gradients \n",
    "    #(LSTM was the elegant solution to prevent vanishing gradient)\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    # (nothing here is triggered in the training)\n",
    "    #     first, variables\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    #     reset zeros out saved_sample_output and saved_sample_state\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    #     Define one lstm_cell with no unrolling (will be used for sampling from the trained model)\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    #     Define the next prediction (but make sure dependencies are calculated first)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I've modified this with comments to understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next() #\n",
    "        # batches is of dimension (num_unrollings, batch_size, vocabulary_size) (11, 64, 27)\n",
    "        # where sequential text from the input is \"striped\" across the unrollings.  For \n",
    "        # example, if 'char1' stands for the first character in the original text (one-hot encoded), \n",
    "        # batches looks like this (assuming 'segment' is 15000):\n",
    "        # [                                        # there are 'num_unrollings' rows\n",
    "        #   [char1,  char15000,  char 30000, ...], # each row is 'batch_size'\n",
    "        #   [char2,  char15001,  char 30001, ...],\n",
    "        #   ...\n",
    "        #   [char11, char15010,  char 30010, ...]\n",
    "        # ]\n",
    "        # when we call train_batches.next(), the next 'batches' will look like this:\n",
    "        # [                                        # there are 'num_unrollings' rows\n",
    "        #   [char11, char15010,  char 30010, ...], # each row is 'batch_size'\n",
    "        #   [char12, char15011,  char 30011, ...],\n",
    "        #   ...\n",
    "        #   [char21, char15020,  char 30020, ...]\n",
    "        # ]\n",
    "        # it might look like a bug that the second 'batches' repeats char11, char1510, etc.\n",
    "        # but it is not a bug.  in the first 'batches', char11 was included only as the label\n",
    "        # needed for the 10th entry (char10). (LSTM takes char10 in as an input and expects char11\n",
    "        # as the true label of the output).  So -- char11 was never put into the LSTM_cell in the\n",
    "        # first 'batches' -- it is only used as a label, so it need to be included as the first\n",
    "        # item in the second 'batches' so that in can now be an input into an LSTM cell.\n",
    "        #\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "            # Normally we see feed_dict={var_name: value}, but here we don't have the var_names\n",
    "            # for the training data batches in the graph definition (it is an array of tensors)\n",
    "            # so instead, we use the tensorflow object itself (from the graph definition, in\n",
    "            # train_data[i]) as the key in the feed_dict entries.\n",
    "            \n",
    "        _, l, predictions, lr = session.run(\n",
    "              [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())  # start with a random character\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAY - small insertion to understand the character sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DAY - small insertion to understand how the character sampling works\n",
    "\n",
    "# first, let's run the optimization of the model for just a short while:\n",
    "num_steps = 500\n",
    "update_frequency = num_steps/20\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "            # train_data is an array of tf.placeholders of len 'num_unrollings' +1\n",
    "            # so feed_dict now has entries with a tf.placeholder as key and a batch as value\n",
    "\n",
    "\n",
    "        _, l, predictions, lr = session.run(\n",
    "              [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "\n",
    "        if step % update_frequency == 0:\n",
    "            print('.', end='')\n",
    "            print('step %d:\\n    feed_dict=%s\\n    train_prediction=%s' % (step, feed_dict, train_prediction))\n",
    "\n",
    "        mean_loss += l\n",
    "# OK, now that the model is just a little trained, report out some stats\n",
    "    mean_loss = mean_loss / num_steps\n",
    "    print('\\nAverage loss over the 500 steps was %f' % mean_loss)\n",
    "    labels = np.concatenate(list(batches)[1:])\n",
    "    print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "\n",
    "    print('Perplexity of last minibatch (i.e.how confused was the model in choosing \\\n",
    "the next char -- ie roughly \\\n",
    "how many chars with equal probability did it have to chose from) was %.2f', float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "    \n",
    "# now do the sampling:\n",
    "    print('\\nSampling Explanation:')\n",
    "    print('=' * 80)\n",
    "    rd = random_distribution()\n",
    "    print('1. random_distribution yields vocabulary_size(27) random #s %s' % rd)\n",
    "    feed = sample(rd)\n",
    "    print('2. take a sample from that in a one-hot format, call it \"feed\": %s' % feed)\n",
    "    print('3. characters converts to a char.  character(feed)=%s' % characters(feed))\n",
    "    sentence = characters(feed)[0]\n",
    "    print('4. the beginning of your \"sentence\" (really a line) is this char: %s' % sentence)\n",
    "    reset_sample_state.run()\n",
    "    print('5. evaluate the \"reset_sample_state\" formula defined above.  It zeros out \\\n",
    "the saved_sample_output and saved_sample_state.  This is important because we want \\\n",
    "predictions from the beginning of the line (i.e. no previous state).\\n\\\n",
    "The thing that is \"remembered\" in the model and doing the predictions are the parameters, \\\n",
    "(i.e. ix, im, ib, ox, om, ob, fx, fm, fb, cx, cm, cb, w, b), not the state or output -- the \\\n",
    "state and output are specific to the minibatch inputs that have preceeded.')\n",
    "    print('6. FOR EACH of the next 79 characters in the line, make a prediction as follows:')\n",
    "    prediction = sample_prediction.eval({sample_input: feed})\n",
    "    print('   6a. Evaluate a \"prediction\" variable by \"eval\"ing the \"sample_prediction\" variable above. \\\n",
    "This triggers the evaluation of a single LSTM cell (not an unrolling of 10) which uses the trained \\\n",
    "parameters to output a prediction for the next character.  This output is turned into a logit with the \\\n",
    "final matmul with w and b and then a softmax is taken which is the prediction.\\n\\\n",
    "Here is the prediction after the first character: %s' % prediction)\n",
    "    feed = sample(prediction)\n",
    "    print('   6b. the sample() function takes that prediction, which is a vector of probabilities, and \\\n",
    "returns a sampled character such that high probability characters are more likely (but not guaranteed) \\\n",
    "to be chosen.  Here is what it chooses for the prediction above: %s' % feed)\n",
    "    sentence += characters(feed)[0]\n",
    "    print('Now convert that one-hot encoding to a char and append to the end of the sentence: %s' % sentence)\n",
    "    \n",
    "    print('\\n Now here is all of that together:')\n",
    "          \n",
    "    print('=' * 80)\n",
    "    for _ in range(5):  # sample 5 lines, each line will be (below) 79 chars\n",
    "        for _ in range(79):\n",
    "            # trigger evaluation of one lstm_cell and capture the softmax(logit) output as prediction\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            # convert prediction into a character\n",
    "            feed = sample(prediction)\n",
    "            # append the character onto our string\n",
    "            sentence += characters(feed)[0]\n",
    "        print(sentence)\n",
    "        # DAY, I moved these 3 lines to the bottom of the loop so I continue the string from above the loop\n",
    "        feed = sample(random_distribution()) # start with a random character\n",
    "        sentence = characters(feed)[0]\n",
    "        reset_sample_state.run()\n",
    "    print('=' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Answer 1\n",
    "========\n",
    "\n",
    "1. I added tensorboard hooks (they aren't quite useful yet\n",
    "2. Concatenate the 4 gates together using tf.concat on the 1th dimension (not the 0th)\n",
    "3. Do the matmul on the concatenated gates\n",
    "4. Split the result back out\n",
    "5. Apply either sigmoid or tanh activation function as appropriate.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters:\n",
    "    with tf.name_scope('LSTM_cell') as lstm_cell_scope:\n",
    "        with tf.name_scope('input_gate') as input_gate_scope:\n",
    "            # Input gate: input, previous output, and bias.\n",
    "            ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1)) # [27, 64]\n",
    "            im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))       # [64, 64]\n",
    "            ib = tf.Variable(tf.zeros([1, num_nodes]))                                     # [1, 64]\n",
    "        with tf.name_scope('forget_gate') as forget_gate_scope:\n",
    "            # Forget gate: input, previous output, and bias.\n",
    "            fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "            fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "            fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "        with tf.name_scope('memory_cell') as memory_cell_scope:\n",
    "            # Memory cell: input, state and bias.                             \n",
    "            cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "            cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "            cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "        with tf.name_scope('output_gate') as output_gate_scope:\n",
    "            # Output gate: input, previous output, and bias.\n",
    "            ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "            om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "            ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "            \n",
    "            \n",
    "        # Reduce the input/output matmuls from 4 to 1 each by concatenating the 4 gates\n",
    "        concatx = tf.concat(1, [ix, fx, cx, ox])\n",
    "        concatm = tf.concat(1, [im, fm, cm, om])\n",
    "        concatb = tf.concat(1, [ib, fb, cb, ob])\n",
    "        \n",
    "        \n",
    "        # Variables saving state across unrollings.\n",
    "        saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "        saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "        # Classifier weights and biases.\n",
    "        w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "        b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        with tf.name_scope(lstm_cell_scope):\n",
    "            # Instead of these 4 matmuls, do the one concatenated matmul and then split results\n",
    "            #input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "            #forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "            #update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "            #output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "\n",
    "            concatmatmul = tf.matmul(i, concatx) + tf.matmul(o, concatm) + concatb\n",
    "            input_gate, forget_gate, update, output_gate = tf.split(1, 4, concatmatmul)\n",
    "            input_gate = tf.sigmoid(input_gate)\n",
    "            forget_gate = tf.sigmoid(forget_gate)\n",
    "            output_gate = tf.sigmoid(output_gate)\n",
    "            state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "    # The LSTM\n",
    "    #\n",
    "    # In the code above, batch_size (bs) and num_nodes (nn) are both 64 (they don't have to be equal)\n",
    "    # 27 is the vocabulary size.\n",
    "    #\n",
    "    # input_gate(bs, nn)   = sigmoid( input(bs, 27) * ix(27, nn) + output(bs, nn) * im(nn, nn) + ib(1, nn) )\n",
    "    # forget_gate(bs, nn)  = sigmoid( input(bs, 27) * ix(27, nn) + output(bs, nn) * im(nn, nn) + ib(1, nn) )\n",
    "    # output_gate(bs, nn)  = sigmoid( input(bs, 27) * ix(27, nn) + output(bs, nn) * im(nn, nn) + ib(1, nn) )\n",
    "    # update(bs, nn)       = tanh(    input(bs, 27) * cx(27, nn) + output(bs, nn) * cm(nn, nn) + cb(1, nn) )\n",
    "    #\n",
    "    # output(bs, nn) = output_gate(bs, nn) * tanh( state(bs, nn) )\n",
    "    # state(bs, nn)  = forget_gate(bs, nn) * state(bs, nn)  +  input_gate(bs, nn) * update(bs, nn)\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            # These 11 elements of train_data will be pulled in from feed_dict\n",
    "            # Note that usually we have seen feed_dict specified as {var_name: value}\n",
    "            # but in this case, since these 11 array elements don't have a var_name, the \n",
    "            # feed_dict will use the tensorflow object as the key instead, i.e.\n",
    "            # feed_dict={<tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: value}\n",
    "            # See below where the feed_dict is prepared before calling session.run.\n",
    "            tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size])) #this will be pulled in from feed_dict\n",
    "    # train_data now has the shape (11, 64, 27), or (num_unrollings, batch_size, vocabulary_size)\n",
    "    # and sequential text from the original text input is 'striped' across the first dimension (11)\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step. (Brilliant!)\n",
    "\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    with tf.name_scope(lstm_cell_scope):\n",
    "        outputs = list()\n",
    "        output = saved_output\n",
    "        state = saved_state\n",
    "        for i in train_inputs: # since train_inputs is num_unrollings=10 long, this will create 10 LSTM cells\n",
    "            output, state = lstm_cell(i, output, state)\n",
    "            outputs.append(output)  # at each iter of the lstm_cell, append the character it predicted to outputs.\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits, tf.concat(0, train_labels)))\n",
    "        tf.scalar_summary('loss', loss)\n",
    "\n",
    "\n",
    "    # Optimizer.\n",
    "    #   Note that all 10 unrollings are done before the optimizer comes in and looks at the\n",
    "    #   output sequence of 10 chars vs. the label sequence of 10 chars and then calculates\n",
    "    #   the gradients and adjusts the parameters.  Then in the next step another 10 characters\n",
    "    #   will be predicted.\n",
    "    with tf.name_scope(\"Optimizer\"):\n",
    "        global_step = tf.Variable(0)\n",
    "        learning_rate = tf.train.exponential_decay(\n",
    "            10.0, global_step, 5000, 0.1, staircase=True)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        # DAY: this clipping below is the hack to prevent exploding gradients \n",
    "        #(LSTM was the elegant solution to prevent vanishing gradient)\n",
    "        gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "        optimizer = optimizer.apply_gradients(\n",
    "            zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    # (nothing here is triggered in the training)\n",
    "    #     first, variables\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    #     reset zeros out saved_sample_output and saved_sample_state\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    #     Define one lstm_cell with no unrolling (will be used for sampling from the trained model)\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    #     Define the next prediction (but make sure dependencies are calculated first)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    merged = tf.merge_all_summaries()\n",
    "    #writer = tf.train.SummaryWriter('logs', graph=session.graph) #use relative dirname\n",
    "    writer = tf.train.SummaryWriter('logs') #use relative dirname\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr, merged_output = session.run(\n",
    "              [optimizer, loss, train_prediction, learning_rate, merged], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            writer.add_summary(merged_output)\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "# Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Answer 2\n",
    "\n",
    "The [post below](https://discussions.udacity.com/t/assignment-6-problem-2/47191/22) from the udacity forums was the most helpful for me to understand what is really being asked here and why it is helpful:\n",
    "\n",
    ">The way I approached this was to have the batch generator return embedding IDs only. lstm_cell() does embedding lookup of the ifcox matrix:\n",
    "\n",
    ">```python\n",
    "comb = tf.nn.embedding_lookup(ifcox, i) + tf.matmul(o, ifcom) + ifcob```\n",
    "\n",
    "\n",
    ">To compute train_labels to pass to `tf.nn.softmax_cross_entropy_with_logits()`, we previously had this:\n",
    "\n",
    ">```python\n",
    "train_labels = train_data[1:]```\n",
    "\n",
    "\n",
    ">But now `train_data[1:]` is a list of embedding IDs, so **I translate the embedding IDs to one-hot encoding by performing an embedding lookup of the identity matrix**:\n",
    "\n",
    ">```python\n",
    "identity = tf.constant(np.identity(embedding_size, dtype = np.float32))\n",
    "#...\n",
    "train_labels = [tf.nn.embedding_lookup(identity, td) for td in train_data[1:]]```\n",
    "\n",
    "\n",
    "A post further down responded to someone who asked why he did the embedding_lookup on the ifcox (concatenated tensors for input/forget/state/output) instead of creating an embedding_vector placeholder variable initialized to random numbers and then doing an `embedding_lookup(embedding_vector, id)` instead.\n",
    "\n",
    ">`tf.nn.embedding_lookup(ifcox, i)` (where i is the batch_size-long list of input bigram IDs) is a drop-in equivalent replacement of `tf.matmul(i, ifcox)` (where i is the batch_size-long list of 729-dimensional one-hot encoded vectors). `tf.nn.embedding_lookup()` effectively simulates the latter.\n",
    "\n",
    ">Does this answer your question?\n",
    "\n",
    "\n",
    "A [post still further down](https://discussions.udacity.com/t/assignment-6-problem-2/47191/37) gives a more detailed explanation:\n",
    "\n",
    "\n",
    ">I took a look at your Jupyter Notebook and I think that I see where the confusion lies. It looks like you adapted word2vec skip-gram to words ' a', 'aa', 'ze', etc. (i.e. the bigrams). This is unlikely to produce anything useful because word2vec models are designed to learn embedding vectors for words in a large vocabulary, but the vocabulary size of all bigrams is only 729. In addition, arbitrary pairs of bigrams can be found together in the corpus for the most part, so the notion of \"frequent context\" of bigrams is not very applicable.\n",
    "\n",
    ">By \"Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves\" in the problem description, they mean to use the tf.nn.embedding_lookup() function to grab individual rows (which is what they mean by \"the embeddings\"). They are not talking about using a word2vec model to learn word embeddings.\n",
    "\n",
    ">It might help to first adapt the code for Problem 1 to bigrams without using tf.nn.embedding_lookup(), and then modify your solution to use tf.nn.embedding_lookup().\n",
    "\n",
    ">For Problem 1, we defined tensors such as:\n",
    "\n",
    ">```python\n",
    "ifcox = tf.Variable(tf.truncated_normal([vocabulary_size, 4 * num_nodes], -0.1, 0.1))\n",
    "ifcom = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))```\n",
    "\n",
    "(Danny, note that ifcox was his version of the concatenated set of tensors for input/forget/state/output)\n",
    "\n",
    "\n",
    ">.. and then we multiplied ifcox by a batch_size-long list of vocabulary_size-dimensional one-hot encoded vectors. If you think about this for a moment, all this multiplication does is select the i1'th, i2'nd, i3'rd, etc. rows of ifcox, where i1, i2, i3, etc. are the IDs of the input characters in the batch.\n",
    "\n",
    ">When you adapt this code to bigrams, the ifcox tensor now has 729 rows (the number of bigrams):\n",
    "\n",
    ">```python\n",
    "ifcox = tf.Variable(tf.truncated_normal([vocabulary_size**2, 4 * num_nodes], -0.1, 0.1))```\n",
    "\n",
    "\n",
    ">Without using `tf.nn.embedding_lookup()`, you would be feeding a batch_size-long list of 729-dimensional one-hot encoded vectors and then multiplying ifcox by this. Again, all this multiplication does is select the i1'th, i2'nd, i3'rd, etc. rows of ifcox, where i1, i2, i3, etc. are the IDs of the input bigrams in the batch.\n",
    "\n",
    ">Using `tf.nn.embedding_lookup()`, you would instead pass the batch_size-long list of IDs of bigrams i1, i2, i3, etc. One huge advantage of this approach is that instead of requiring 729 numbers for each example in the batch (728 of which are 0.0), you only need 1 number for each example. Assuming that each number is represented by 4 bytes, this is a savings of 2,912 bytes of memory for each example!\n",
    "\n",
    ">Daniel\n",
    "\n",
    "Another [useful post was here](https://discussions.udacity.com/t/assignment-6-problem-2/47191/46).\n",
    "\n",
    ">Here's my approach. I try to make very small changes that I can easily verify their correctness.  Please, criticize at will. I could have missed something obvious.\n",
    "\n",
    ">1. Convert one-hot to embeddings as suggested in the problem definition. I've implemented them as a wrapper over the generated batches (no need to touch BatchGenerator). Just before feeding call batchIDs = batches2IDs(batch)\n",
    "2. Inside the model, split the train_data into two placeholders: the input and the labels. While training feed them in two lines of code inside the unrolling+1 loop.\n",
    "3. Steps 1 and 2 I assume are correct as they give the same perplexities and losses as the original code.\n",
    "4. Bigram:\n",
    "    1. Call the BatchGenerator initializer with `num_unrollings + 2`.\n",
    "    2. Change the size of the tensors in the LSTM to `vocabulary_size ** 2`\n",
    "    3. Run The unrolling loop in the feed up to `num_unrollings + 2`.\n",
    "    4. Feed the input with `batchIDs[i] + batchIDs[i+1] * vocabulary_size` (i.e. convert a tuple into an integer)\n",
    "    5. Feed the label with `batchIDs[i+2]`\n",
    "    6. Minor modifications are needed for the output sample and verification.\n",
    "\n",
    "## The plan:\n",
    "We will be feeding in embeddings as input and getting one-hots as output.\n",
    "\n",
    "1. Get it working on embeddings for single character case and compare to the previous\n",
    "    * Either have Generate_Batch spit out IDs (not embeddings) or the normal one-hots as it does today\n",
    "    * If Generate_Batch spits out IDs, do the trick above to convert labels back to one-hots (the identity matrix trix)\n",
    "    * Else if Generate_Batch spits out one-hots, convert the **inputs** to IDs->embeddings right before feeding into the model.  Keep the labels as the one-hots that are already output by Generate_Batch\n",
    "2. Modify it to use bigram embeddings instead of single character embeddings.\n",
    "\n",
    "I had the initial worry that by outputing one-hot probabilities, I wasn't avoiding the sparse matrix structure that was computationally wasteful, but I think the posts referenced above convinced me this is what we're supposed to do.\n",
    "\n",
    "For #2 above, I could do the following options (assume alphabet input)\n",
    "* input1=ab, targeted_output1=c  ; input2=bc, targeted_output2=d \n",
    "* input1=ab, targeted_output1=c  ; input2=cd, targeted_output2=e \n",
    "* input1=ab, targeted_output1=cd ; input2=cd, targeted_output2=ef\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n",
      "Data size 100000000\n",
      "training data size: 99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "validation data size: 1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "          'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)\n",
    "\n",
    "def read_data(filename):\n",
    "    f = zipfile.ZipFile(filename)\n",
    "    for name in f.namelist():\n",
    "        return tf.compat.as_str(f.read(name))\n",
    "    f.close()\n",
    "\n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))\n",
    "\n",
    "# Create small validation set\n",
    "\n",
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print('training data size: %s %s' % (train_size, train_text[:64]))\n",
    "print('validation data size: %s %s' % (valid_size, valid_text[:64]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 0,  6, 12,  5], dtype=int32), array([1, 7, 0, 6], dtype=int32), array([2, 8, 1, 7], dtype=int32)]\n",
      "[array([2, 8, 1, 7], dtype=int32), array([3, 9, 2, 8], dtype=int32), array([ 4, 10,  3,  9], dtype=int32)]\n",
      "[array([ 4, 10,  3,  9], dtype=int32), array([ 5, 11,  4, 10], dtype=int32), array([ 6, 12,  5, 11], dtype=int32)]\n",
      "[array([ 6, 12,  5, 11], dtype=int32), array([7, 0, 6, 0], dtype=int32), array([8, 1, 7, 1], dtype=int32)]\n",
      "[['ab', 'mn', 'yz', 'kl'], ['cd', 'op', 'ab', 'mn'], ['ef', 'qr', 'cd', 'op']]\n",
      "[['ef', 'qr', 'cd', 'op'], ['gh', 'st', 'ef', 'qr'], ['ij', 'uv', 'gh', 'st']]\n",
      "[['ij', 'uv', 'gh', 'st'], ['kl', 'wx', 'ij', 'uv'], ['mn', 'yz', 'kl', 'wx']]\n",
      "[['mn', 'yz', 'kl', 'wx'], ['op', 'ab', 'mn', 'ab'], ['qr', 'cd', 'op', 'cd']]\n"
     ]
    }
   ],
   "source": [
    "# DAY\n",
    "#\n",
    "# This is important to understand.  Our NN needs a constant sized vector with each input.  We are\n",
    "# providing that here.  As the video says, just as convolution lets us use the same weight parameters\n",
    "# at different parts of the image, a recurrent neural net lets us use the same weights at different\n",
    "# points in time (or rather, different points in the input sequence).\n",
    "#\n",
    "# The notion of \"unrollings\" is that a recurrent NN has it's output connected to it's input, but really\n",
    "# the way to think about it is over time where the output of time t-1 is input to time t.  That way\n",
    "# of looking at it is like \"unrolling\" the recurrent NN over time so it is understood more as a\n",
    "# sequence of copies of the NN.  \n",
    "# In this case, we are going to be feeding in sequences that are 10 long, so we will in effect\n",
    "# create 10 LSTM cells (which are really just a NN) and hook the output of LSTM cell t with inputs\n",
    "# from input_sub_t and also the output of LSTM cell t-1.\n",
    "\n",
    "# I'm re-writing the BatchGenerator to be more general purpose.  I want it to always output\n",
    "# IDs (not embeddings, not the actual text, but the ID of the embeddings)\n",
    "\n",
    "class SequenceGenerator(object):\n",
    "    '''This class will take a text input and generate batches of IDs suitable for use with\n",
    "    tf.nn.embedding_lookup() (i.e. goes from 0 to len(vocab)-1).  This class can be \n",
    "    inherited to create classes that create IDs for single characters, bigrams, \n",
    "    or entire words.  It also has the ability to take\n",
    "    ID output from the RNN and convert it back to the original text.'''\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        self._init_vocab()\n",
    "        self._init_token_sequence()\n",
    "        segment = self._token_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "        \n",
    "    def _init_vocab(self):\n",
    "        '''Must be implemented in subclasses and create the following:\n",
    "                self._vocab_dict (text keys and ID values)  \n",
    "                self._reverse_vocab_dict (ID keys and text values)\n",
    "                self.vocab_size'''\n",
    "        raise NotImplementedError\n",
    "    def _init_token_sequence(self):\n",
    "        '''Must be implemented in subclasses and create the following:\n",
    "                self._tokens = list of tokens (for example for bigrams: ['th', 'is', ' i', 's ', 'a ', 'te', 'st'])\n",
    "                self._token_size = the total # of tokens in the input stream\n",
    "                '''\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def token_2_id(self, token):\n",
    "        return self._vocab_dict[token]\n",
    "    def id_2_token(self, token_id):\n",
    "        return self._reverse_vocab_dict[token_id]\n",
    "    def onehot_2_id(self, one_hot):\n",
    "        \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "        characters back into its (most likely) ID representation.\n",
    "        This will always return the same result for identical inputs -- it does\n",
    "        not sample across the probability distribution.\n",
    "        This used to be character()\"\"\"\n",
    "        return [c for c in np.argmax(one_hot, 1)]\n",
    "    def id_2_onehot(self, id_list):\n",
    "        '''Turn a list of ids into a list of one_hot encoded vectors'''\n",
    "        identity = tf.constant(np.identity(self.vocab_size, dtype = np.float32))\n",
    "        return tf.nn.embedding_lookup(identity, id_list)\n",
    "    \n",
    "    def softmax_2_sampled_id(self, softmax_distribution):\n",
    "        \"\"\"Turn a softMax probability distribution over the possible\n",
    "        characters into an ID representation based on a sampling over that probability\n",
    "        distribution.\n",
    "        This randomly samples the distribution. So, for example, if in the\n",
    "        softmax distribution 'a' is 40% likely, 'b' is 40% likely and \n",
    "        'c' is 20% likely, this will generate an 'a' 40% of the time\n",
    "        it is called and a 'c' 20% of the time it is called, etc.\"\"\"\n",
    "        r = random.uniform(0, 1)\n",
    "        s = 0\n",
    "        for i in range(len(softmax_distribution)):\n",
    "            s += softmax_distribution[i]\n",
    "            if s >= r:\n",
    "                return i\n",
    "        return len(softmax_distribution) - 1\n",
    "    \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single row (or unrolling) of length 'batch' from the current \n",
    "        cursor position in the token data.  It will be in the form of a row of token IDs.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, ), dtype=np.int32)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b] = self.token_2_id(self._tokens[self._cursor[b]])\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._token_size\n",
    "        return batch\n",
    "  \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        (for a total of num_unrollings + 1).\n",
    "        The reason the last batch from the previous array is included is because\n",
    "        in the previous array, the last batch was just used as a label to the model,\n",
    "        not as an input -- so we include it this next time to be used as an input.\n",
    "        Note that the sequential tokens end up being read into the columns of these\n",
    "        'num_unrolling\" batches.  Each column is a separate part of the token input.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "    \n",
    "    def batches_2_tokens(self, batches):\n",
    "        \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "        representation.\"\"\"\n",
    "        # DAY\n",
    "        # This mangles the real batch structure in the interest of readability, but\n",
    "        # by doing so, makes your understanding of batches wrong.\n",
    "        # See 'honest_batches2string' below which gives you a better\n",
    "        # understanding of the batch format.\n",
    "\n",
    "        # batches has dimensions (num_unrollings, batch_size)\n",
    "        s = [''] * batches[0].shape[0]  # batches[0].shape[0] will end up being same as batch_size\n",
    "        for b in batches: # there will be num_unrollings of these...\n",
    "            s = [''.join(self.id_2_token(x)) for x in zip(s, b)]  # DAY __ NEEDS WORK\n",
    "        return s\n",
    "\n",
    "    def honest_batches_2_tokens(self, batches):\n",
    "        import pprint\n",
    "        output = []\n",
    "        for b_index, b in enumerate(batches):  # there will be 'num_unrollings' of these\n",
    "            output.append(list())\n",
    "            for token_id_index, token_id in enumerate(b):  # there will be 'batch_size' of these\n",
    "                output[b_index].append(self.id_2_token(token_id))\n",
    "        return pprint.pformat(output)\n",
    "\n",
    "class SingleCharacterGenerator(SequenceGenerator):\n",
    "    def _init_vocab(self):\n",
    "        '''Must create the following:\n",
    "                self._vocab_dict (text keys and ID values)  \n",
    "                self._reverse_vocab_dict (ID keys and text values)\n",
    "                self.vocab_size'''\n",
    "        self.vocab_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "        self._vocab_dict = dict()\n",
    "        self._vocab_dict[' '] = 0\n",
    "        for i, v in enumerate(list(string.ascii_lowercase[:self.vocab_size - 1])):\n",
    "            self._vocab_dict[v] = i+1\n",
    "        self._reverse_vocab_dict = dict(zip(self._vocab_dict.values(), self._vocab_dict.keys()))\n",
    "    def _init_token_sequence(self):\n",
    "        '''Must create the following:\n",
    "                self._tokens = list of tokens (for example for bigrams: ['th', 'is', ' i', 's ', 'a ', 'te', 'st'])\n",
    "                self._token_size = the total # of tokens in the input stream\n",
    "                '''\n",
    "        first_letter = ord(string.ascii_lowercase[0])\n",
    "        self._tokens = list()\n",
    "        for char in self._text.lower():\n",
    "            if char in string.ascii_lowercase:\n",
    "                self._tokens.append(char)\n",
    "            elif char == ' ':\n",
    "                self._tokens.append(' ')\n",
    "            else:\n",
    "                pass # don't enter unknown characters (DAY should we have an UNK ID?)\n",
    "        self._token_size = len(self._tokens)\n",
    "\n",
    "class BigramGenerator(SequenceGenerator):\n",
    "    def _init_vocab(self):\n",
    "        '''Must create the following:\n",
    "                self._vocab_dict (text keys and ID values)  \n",
    "                self._reverse_vocab_dict (ID keys and text values)\n",
    "                self.vocab_size'''\n",
    "        self._vocab_dict = dict()\n",
    "        self._tokens = list()\n",
    "        id_index = 0\n",
    "        for i in range(0, len(self._text)-2, 2):\n",
    "            bigram = self._text[i].lower() + self._text[i+1].lower()\n",
    "            self._tokens.append(bigram) # duplications ok on this one -- this is just input stream as tokens\n",
    "            if bigram not in self._vocab_dict:\n",
    "                self._vocab_dict[bigram] = id_index\n",
    "                id_index += 1\n",
    "        self.vocab_size = len(self._vocab_dict)\n",
    "        self._reverse_vocab_dict = dict(zip(self._vocab_dict.values(), self._vocab_dict.keys()))\n",
    "        self._token_size = len(self._tokens)\n",
    "    def _init_token_sequence(self):\n",
    "        '''Must create the following:\n",
    "                self._tokens = list of tokens (for example for bigrams: ['th', 'is', ' i', 's ', 'a ', 'te', 'st'])\n",
    "                self._token_size = the total # of tokens in the input stream\n",
    "                '''\n",
    "        pass # I did this work in _init_vocab() to use just one loop\n",
    "\n",
    "\n",
    "if True:\n",
    "    my_text = \"abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz\"\n",
    "    #my_batches = SingleCharacterGenerator(my_text.lower(), 4, 2)\n",
    "    my_batches = BigramGenerator(my_text.lower(), 4, 2)\n",
    "    print(my_batches.next())\n",
    "    print(my_batches.next())\n",
    "    print(my_batches.next())\n",
    "    print(my_batches.next())\n",
    "    #my_batches = SingleCharacterGenerator(my_text.lower(), 4, 2)\n",
    "    my_batches = BigramGenerator(my_text.lower(), 4, 2)\n",
    "    print(my_batches.honest_batches_2_tokens(my_batches.next()))\n",
    "    print(my_batches.honest_batches_2_tokens(my_batches.next()))\n",
    "    print(my_batches.honest_batches_2_tokens(my_batches.next()))\n",
    "    print(my_batches.honest_batches_2_tokens(my_batches.next()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction, vocabulary_size):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0  # prediction is in column format, so it must be indexed by [0]\n",
    "    return p\n",
    "\n",
    "def random_distribution(vocabulary_size):\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Modified LSTM model which takes embeddings as inputs and generates one-hot encodings as outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "num_nodes = 64\n",
    "embedding_size = 50\n",
    "\n",
    "train_batches = SingleCharacterGenerator(train_text, batch_size, num_unrollings)\n",
    "vocabulary_size = train_batches.vocab_size\n",
    "valid_batches = SingleCharacterGenerator(valid_text, 1, 1)\n",
    "\n",
    "# See interesting implementation of 2-layer LSTM (with embeddings) here: http://pastebin.com/YP3sWkG9\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # My embedding here will simply be a 2D tensor -- the first\n",
    "    # dimension will hold the ID a (character or bigram) (the index)\n",
    "    # the second dimension will hold the embedding vector.\n",
    "    vocabulary_embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  \n",
    "    # Parameters:\n",
    "    with tf.name_scope('LSTM_cell') as lstm_cell_scope:\n",
    "        with tf.name_scope('input_gate') as input_gate_scope:\n",
    "            # Input gate: input, previous output, and bias.\n",
    "            ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1)) # [50, 64]\n",
    "            im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))       # [64, 64]\n",
    "            ib = tf.Variable(tf.zeros([1, num_nodes]))                                     # [1, 64]\n",
    "        with tf.name_scope('forget_gate') as forget_gate_scope:\n",
    "            # Forget gate: input, previous output, and bias.\n",
    "            fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "            fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "            fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "        with tf.name_scope('memory_cell') as memory_cell_scope:\n",
    "            # Memory cell: input, state and bias.                             \n",
    "            cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "            cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "            cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "        with tf.name_scope('output_gate') as output_gate_scope:\n",
    "            # Output gate: input, previous output, and bias.\n",
    "            ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "            om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "            ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "            \n",
    "            \n",
    "        # Reduce the input/output matmuls from 4 to 1 each by concatenating the 4 gates\n",
    "        concatx = tf.concat(1, [ix, fx, cx, ox])\n",
    "        concatm = tf.concat(1, [im, fm, cm, om])\n",
    "        concatb = tf.concat(1, [ib, fb, cb, ob])\n",
    "        \n",
    "        \n",
    "        # Variables saving state across unrollings.\n",
    "        saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "        saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "        # Classifier weights and biases.\n",
    "        w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1)) #output is one-hot vector\n",
    "        b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        with tf.name_scope(lstm_cell_scope):\n",
    "            # #Instead of these 4 matmuls, do the one concatenated matmul and then split results\n",
    "            #input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "            #forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "            #update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "            #output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "\n",
    "            concatmatmul = tf.matmul(i, concatx) + tf.matmul(o, concatm) + concatb\n",
    "            input_gate, forget_gate, update, output_gate = tf.split(1, 4, concatmatmul)\n",
    "            input_gate = tf.sigmoid(input_gate)\n",
    "            forget_gate = tf.sigmoid(forget_gate)\n",
    "            output_gate = tf.sigmoid(output_gate)\n",
    "            state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "    # The LSTM\n",
    "    #\n",
    "    # In the code above, batch_size (bs) and num_nodes (nn) are both 64 (they don't have to be equal)\n",
    "    # 27 is the vocabulary size, 50 is the embed_size\n",
    "    #\n",
    "    # input_gate(bs, nn)   = sigmoid( input(bs, 50) * ix(50, nn) + output(bs, nn) * im(nn, nn) + ib(1, nn) )\n",
    "    # forget_gate(bs, nn)  = sigmoid( input(bs, 50) * ix(50, nn) + output(bs, nn) * im(nn, nn) + ib(1, nn) )\n",
    "    # output_gate(bs, nn)  = sigmoid( input(bs, 50) * ix(50, nn) + output(bs, nn) * im(nn, nn) + ib(1, nn) )\n",
    "    # update(bs, nn)       = tanh(    input(bs, 50) * cx(50, nn) + output(bs, nn) * cm(nn, nn) + cb(1, nn) )\n",
    "    #\n",
    "    # output(bs, nn) = output_gate(bs, nn) * tanh( state(bs, nn) )\n",
    "    # state(bs, nn)  = forget_gate(bs, nn) * state(bs, nn)  +  input_gate(bs, nn) * update(bs, nn)\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            # These 11 elements of train_data will be pulled in from feed_dict\n",
    "            # Note that usually we have seen feed_dict specified as {var_name: value}\n",
    "            # but in this case, since these 11 array elements don't have a var_name, the \n",
    "            # feed_dict will use the tensorflow object as the key instead, i.e.\n",
    "            # feed_dict={<tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: value}\n",
    "            # See below where the feed_dict is prepared before calling session.run.\n",
    "            \n",
    "            # Note that the new shape=[batch_size, ] matches a batch of IDs (the IDs don't have a dimension, they\n",
    "            #    are just integers)\n",
    "            tf.placeholder(tf.int32, shape=[batch_size, ])) #this will be pulled in from feed_dict\n",
    "    # train_data now has the shape (11, 64, 50), or (num_unrollings, batch_size, embedding_size)\n",
    "    # and sequential text from the original text input is 'striped' across the first dimension (11)\n",
    "    \n",
    "    # Create the train_inputs by converting the train_data (batches of IDs) into \n",
    "    #    embeddings (batches of embeddings)\n",
    "    #    Here is what the ID batches look like (num_unrollings=2 (+1), batch_size=4)\n",
    "    #    [array([  1.,   7.,  13.,  19.]), \n",
    "    #     array([  2.,   8.,  14.,  20.]), \n",
    "    #     array([  3.,   9.,  15.,  21.])]\n",
    "    train_inputs = [tf.nn.embedding_lookup(vocabulary_embeddings, id_array) \n",
    "                            for id_array in train_data[:num_unrollings]]\n",
    "    # Create the train_labels by shifting by one time step and then\n",
    "    #    converting the train_data (batches of IDs) into\n",
    "    #    one_hot vectors (batches of one_hots)\n",
    "    train_labels = [train_batches.id_2_onehot(id_array) \n",
    "                            for id_array in train_data[1:]]\n",
    "    \n",
    "    \n",
    "    # Unrolled LSTM loop.\n",
    "    with tf.name_scope(lstm_cell_scope):\n",
    "        outputs = list()\n",
    "        output = saved_output\n",
    "        state = saved_state\n",
    "        for i in train_inputs: # since train_inputs is num_unrollings=10 long, this will create 10 LSTM cells\n",
    "            output, state = lstm_cell(i, output, state)\n",
    "            outputs.append(output)  # at each iter of the lstm_cell, append the character it predicted to outputs.\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits, tf.concat(0, train_labels)))\n",
    "        tf.scalar_summary('loss', loss)\n",
    "\n",
    "    # Optimizer.\n",
    "    #   Note that all 10 unrollings are done before the optimizer comes in and looks at the\n",
    "    #   output sequence of 10 chars vs. the label sequence of 10 chars and then calculates\n",
    "    #   the gradients and adjusts the parameters.  Then in the next step another 10 characters\n",
    "    #   will be predicted.\n",
    "    with tf.name_scope(\"Optimizer\"):\n",
    "        global_step = tf.Variable(0)\n",
    "        learning_rate = tf.train.exponential_decay(\n",
    "            10.0, global_step, 5000, 0.1, staircase=True)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        # DAY: this clipping below is the hack to prevent exploding gradients \n",
    "        #(LSTM was the elegant solution to prevent vanishing gradient)\n",
    "        gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "        optimizer = optimizer.apply_gradients(\n",
    "            zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    # (nothing here is triggered in the training)\n",
    "    #     first, variables\n",
    "    sample_input = tf.placeholder(tf.int32, shape=[1, ]) #now it is just an id\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    #     reset zeros out saved_sample_output and saved_sample_state\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    #     Define one lstm_cell with no unrolling (will be used for sampling from the trained model)\n",
    "    \n",
    "    [tf.nn.embedding_lookup(vocabulary_embeddings, id_array) \n",
    "                            for id_array in train_data[:num_unrollings]]\n",
    "    \n",
    "    sample_output, sample_state = lstm_cell(\n",
    "                                    tf.nn.embedding_lookup(vocabulary_embeddings, sample_input), \n",
    "                                    saved_sample_output, \n",
    "                                    saved_sample_state)\n",
    "    #     Define the next prediction (but make sure dependencies are calculated first)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I've modified this with comments to understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.306940 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.17\n",
      "================================================================================\n",
      "wvarjrynybecn  arozsxwngzvda lhwsweriha lxffkp uetcdhci rktdhos lspxkscbipudggb \n",
      "uzafhea tkhii antio     lkdw abci rdonp yasouj  rdljo df m sie  fk hrermew jvood\n",
      "y moyzrpttkryps lb gscgsrmzio najgnorfrizimsujsohlgva xipytjo i bs xwtjqfsbekeer\n",
      "va fwlzbtitgyroeunmeotyhwd yofco  yavalnaevwr zlitsseoaein hstnrarqxovgr nrte no\n",
      "v l mxrw xxpkzcnjwi giai dk  oxukroacotnsrsgsvmkcratatc  ipconkncsdbiannvf tanui\n",
      "================================================================================\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 100: 2.333039 learning rate: 10.000000\n",
      "Minibatch perplexity: 51.40\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 200: 2.015087 learning rate: 10.000000\n",
      "Minibatch perplexity: 75.15\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 300: 1.911373 learning rate: 10.000000\n",
      "Minibatch perplexity: 82.55\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 400: 1.843238 learning rate: 10.000000\n",
      "Minibatch perplexity: 87.78\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 500: 1.807754 learning rate: 10.000000\n",
      "Minibatch perplexity: 86.66\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 600: 1.797493 learning rate: 10.000000\n",
      "Minibatch perplexity: 107.88\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 700: 1.756892 learning rate: 10.000000\n",
      "Minibatch perplexity: 92.20\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 800: 1.721754 learning rate: 10.000000\n",
      "Minibatch perplexity: 123.48\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 900: 1.742735 learning rate: 10.000000\n",
      "Minibatch perplexity: 102.36\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 1000: 1.748193 learning rate: 10.000000\n",
      "Minibatch perplexity: 100.73\n",
      "================================================================================\n",
      "ining empility latelation stoy newas marlans stotraton of the mad in the gagy a \n",
      "n in one nine nine seven eight severe eopore the only usegual amosing way evring\n",
      " gurhiture and the word cament the infining was with respectures of that electen\n",
      "ents accumbegututen lat to the mother bacution to be estatic the fiv the the fro\n",
      "er norshearege h from it in area the hid chny ahpoting a the yoxealy two none ni\n",
      "================================================================================\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 1100: 1.706227 learning rate: 10.000000\n",
      "Minibatch perplexity: 132.44\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 1200: 1.696959 learning rate: 10.000000\n",
      "Minibatch perplexity: 110.03\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 1300: 1.672310 learning rate: 10.000000\n",
      "Minibatch perplexity: 112.51\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 1400: 1.688111 learning rate: 10.000000\n",
      "Minibatch perplexity: 171.34\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 1500: 1.691363 learning rate: 10.000000\n",
      "Minibatch perplexity: 117.76\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 1600: 1.697779 learning rate: 10.000000\n",
      "Minibatch perplexity: 118.60\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 1700: 1.674837 learning rate: 10.000000\n",
      "Minibatch perplexity: 179.16\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 1800: 1.641033 learning rate: 10.000000\n",
      "Minibatch perplexity: 167.68\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 1900: 1.615808 learning rate: 10.000000\n",
      "Minibatch perplexity: 115.11\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 2000: 1.659580 learning rate: 10.000000\n",
      "Minibatch perplexity: 137.40\n",
      "================================================================================\n",
      "ine norgeaned in mantbines exteries and houm one for the xamalandon as maniques \n",
      "s world alimann heasing their region elegentry commeveve ca dunect sociar h bot \n",
      "heron eurder veat body minmera and stritic one nine sin four nine nine eight nin\n",
      "ined many on xam jahing the ortaat seths ninely wybechannol jount soon fility he\n",
      "ment of gnitbal americ guers one nine one zero four to players th wrolla revect \n",
      "================================================================================\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 2100: 1.654423 learning rate: 10.000000\n",
      "Minibatch perplexity: 147.61\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 2200: 1.646483 learning rate: 10.000000\n",
      "Minibatch perplexity: 202.83\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 2300: 1.617237 learning rate: 10.000000\n",
      "Minibatch perplexity: 140.59\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 2400: 1.636603 learning rate: 10.000000\n",
      "Minibatch perplexity: 202.80\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 2500: 1.661685 learning rate: 10.000000\n",
      "Minibatch perplexity: 167.10\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 2600: 1.635141 learning rate: 10.000000\n",
      "Minibatch perplexity: 159.61\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 2700: 1.650710 learning rate: 10.000000\n",
      "Minibatch perplexity: 127.50\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 2800: 1.637305 learning rate: 10.000000\n",
      "Minibatch perplexity: 150.67\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 2900: 1.637513 learning rate: 10.000000\n",
      "Minibatch perplexity: 123.77\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 3000: 1.643484 learning rate: 10.000000\n",
      "Minibatch perplexity: 151.79\n",
      "================================================================================\n",
      "videnced a more u servinity one nine seven ff s up subdetour his he actoricist b\n",
      "minu aduity to harropak  hengorky severe be the mistagly is lennant arer be even\n",
      "kition to pitorila to besemble paydemus nu war was one nine seven jan derfst may\n",
      "ing decorped du capit leterty key rescovery ware by kardurry ros to his to large\n",
      "one had overt and accost has asigh lonts peck mony obdai in f cossistrietherig s\n",
      "================================================================================\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 3100: 1.618193 learning rate: 10.000000\n",
      "Minibatch perplexity: 181.65\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 3200: 1.648032 learning rate: 10.000000\n",
      "Minibatch perplexity: 130.57\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 3300: 1.631421 learning rate: 10.000000\n",
      "Minibatch perplexity: 118.46\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 3400: 1.666353 learning rate: 10.000000\n",
      "Minibatch perplexity: 139.51\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 3500: 1.653165 learning rate: 10.000000\n",
      "Minibatch perplexity: 130.40\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 3600: 1.674065 learning rate: 10.000000\n",
      "Minibatch perplexity: 179.49\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 3700: 1.643276 learning rate: 10.000000\n",
      "Minibatch perplexity: 116.26\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 3800: 1.645173 learning rate: 10.000000\n",
      "Minibatch perplexity: 160.47\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 3900: 1.642117 learning rate: 10.000000\n",
      "Minibatch perplexity: 178.60\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 4000: 1.652322 learning rate: 10.000000\n",
      "Minibatch perplexity: 155.77\n",
      "================================================================================\n",
      "noly and up decuam were doops a acteric case among exystric american ids adopp f\n",
      "ca can falth electory neiger toy one simeios inrumple inform ty after s theorata\n",
      "y one class all unother upi of bots sect let in amalaig lay one zero one zero fo\n",
      "kek by the gru insteypugus s and a ring againsfiented and calbert the one nine t\n",
      "waler the lefting borde sounniqualed sinces was with first word also monts the m\n",
      "================================================================================\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 4100: 1.638918 learning rate: 10.000000\n",
      "Minibatch perplexity: 208.52\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 4200: 1.641161 learning rate: 10.000000\n",
      "Minibatch perplexity: 162.83\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 4300: 1.623410 learning rate: 10.000000\n",
      "Minibatch perplexity: 164.01\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 4400: 1.618777 learning rate: 10.000000\n",
      "Minibatch perplexity: 141.89\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 4500: 1.628420 learning rate: 10.000000\n",
      "Minibatch perplexity: 197.32\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 4600: 1.626868 learning rate: 10.000000\n",
      "Minibatch perplexity: 173.27\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 4700: 1.635326 learning rate: 10.000000\n",
      "Minibatch perplexity: 122.56\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 4800: 1.649237 learning rate: 10.000000\n",
      "Minibatch perplexity: 160.52\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 4900: 1.645155 learning rate: 10.000000\n",
      "Minibatch perplexity: 144.37\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 5000: 1.622531 learning rate: 1.000000\n",
      "Minibatch perplexity: 156.63\n",
      "================================================================================\n",
      "hilfor never leavea but is hasp often ope originer s amakhaken they rabutes with\n",
      "rithors asean s rolewist the gival wreves to iders husmetamely yearsion by rough\n",
      "ucyukmenter acouro knains be to the s result with that two zero zero one two exp\n",
      " storine val caller and seperator backing of plawosen with davidiale buts of bos\n",
      "ing blosts insticutry only staytituing the exs cress fats processioncad that beg\n",
      "================================================================================\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 5100: 1.593153 learning rate: 1.000000\n",
      "Minibatch perplexity: 170.31\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 5200: 1.571354 learning rate: 1.000000\n",
      "Minibatch perplexity: 158.24\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 5300: 1.558073 learning rate: 1.000000\n",
      "Minibatch perplexity: 221.01\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 5400: 1.557558 learning rate: 1.000000\n",
      "Minibatch perplexity: 168.97\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 5500: 1.541887 learning rate: 1.000000\n",
      "Minibatch perplexity: 171.94\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 5600: 1.553802 learning rate: 1.000000\n",
      "Minibatch perplexity: 175.19\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 5700: 1.544242 learning rate: 1.000000\n",
      "Minibatch perplexity: 181.68\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 5800: 1.550412 learning rate: 1.000000\n",
      "Minibatch perplexity: 196.45\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 5900: 1.550974 learning rate: 1.000000\n",
      "Minibatch perplexity: 162.75\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 6000: 1.526123 learning rate: 1.000000\n",
      "Minibatch perplexity: 201.57\n",
      "================================================================================\n",
      "no thise newers order from heart a genny for sladied for the and a many been the\n",
      "rika august secondolda cattentines of julimarimment hara tharaperow adable of a \n",
      "h markethers streek free six six zero being by a content is the fluence may to b\n",
      " one eight th haster companise it harvised for the roman artive in model between\n",
      "bes white one nine eight six fond the strug also partional b one nine oilth abse\n",
      "================================================================================\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 6100: 1.541826 learning rate: 1.000000\n",
      "Minibatch perplexity: 167.62\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 6200: 1.511899 learning rate: 1.000000\n",
      "Minibatch perplexity: 223.12\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 6300: 1.517609 learning rate: 1.000000\n",
      "Minibatch perplexity: 227.40\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 6400: 1.518357 learning rate: 1.000000\n",
      "Minibatch perplexity: 204.96\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 6500: 1.534886 learning rate: 1.000000\n",
      "Minibatch perplexity: 189.72\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 6600: 1.570947 learning rate: 1.000000\n",
      "Minibatch perplexity: 195.43\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 6700: 1.554648 learning rate: 1.000000\n",
      "Minibatch perplexity: 165.67\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 6800: 1.575061 learning rate: 1.000000\n",
      "Minibatch perplexity: 172.09\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 6900: 1.551311 learning rate: 1.000000\n",
      "Minibatch perplexity: 140.96\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 7000: 1.546291 learning rate: 1.000000\n",
      "Minibatch perplexity: 249.75\n",
      "================================================================================\n",
      "d great presidents critics surm matio is do vaurt of their oldormed sides perple\n",
      "ed with poll renonotion telehorite penciluate nor if one of there nolsn compress\n",
      " political pope was equive is the princt by united it is six natures field of th\n",
      "filider lemono ofstlougia symbol jewer that videamn zelehisgn n popeal combrons \n",
      "bosterne boetting filized tumber naction of the anthered comples to inta btlact \n",
      "================================================================================\n",
      "Validation set perplexity: inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:89: RuntimeWarning: overflow encountered in exp\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next() #\n",
    "        # batches is of dimension (num_unrollings, batch_size, ) (11, 64,)\n",
    "        # where sequential text from the input is \"striped\" across the unrollings.  For \n",
    "        # example, if 'char1' stands for the first character in the original text \n",
    "        # batches looks like this (assuming 'segment' is 15000):\n",
    "        # [                                        # there are 'num_unrollings' rows\n",
    "        #   [char1,  char15000,  char 30000, ...], # each row is 'batch_size'\n",
    "        #   [char2,  char15001,  char 30001, ...],\n",
    "        #   ...\n",
    "        #   [char11, char15010,  char 30010, ...]\n",
    "        # ]\n",
    "        # when we call train_batches.next(), the next 'batches' will look like this:\n",
    "        # [                                        # there are 'num_unrollings' rows\n",
    "        #   [char11, char15010,  char 30010, ...], # each row is 'batch_size'\n",
    "        #   [char12, char15011,  char 30011, ...],\n",
    "        #   ...\n",
    "        #   [char21, char15020,  char 30020, ...]\n",
    "        # ]\n",
    "        # it might look like a bug that the second 'batches' repeats char11, char1510, etc.\n",
    "        # but it is not a bug.  in the first 'batches', char11 was included only as the label\n",
    "        # needed for the 10th entry (char10). (LSTM takes char10 in as an input and expects char11\n",
    "        # as the true label of the output).  So -- char11 was never put into the LSTM_cell in the\n",
    "        # first 'batches' -- it is only used as a label, so it need to be included as the first\n",
    "        # item in the second 'batches' so that in can now be an input into an LSTM cell.\n",
    "        #\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "            # Normally we see feed_dict={var_name: value}, but here we don't have the var_names\n",
    "            # for the training data batches in the graph definition (it is an array of tensors)\n",
    "            # so instead, we use the tensorflow object itself (from the graph definition, in\n",
    "            # train_data[i]) as the key in the feed_dict entries.\n",
    "            \n",
    "        _, l, predictions, lr = session.run(\n",
    "              [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            \n",
    "            #labels = np.concatenate(list(batches)[1:])\n",
    "            ##print(list(batches)[1:])\n",
    "            #print([train_batches.id_2_onehot(id_array) for id_array in batches[1:]])\n",
    "            # Convert labels to one_hot vectors (they are batches of IDs now). Then\n",
    "            #    concatenate all 10 unrollings together\n",
    "            labels = tf.reshape(tf.concat(1, [train_batches.id_2_onehot(id_array) \n",
    "                                            for id_array in batches[1:]]), [-1, vocabulary_size])\n",
    "\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels.eval())))) #DAY the \".eval()\" converts tensor to numpy array\n",
    "            \n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    # start with a random char/token from our vocabulary\n",
    "                    feed = [random.choice(xrange(vocabulary_size))]\n",
    "                    #feed = sample(random_distribution(vocabulary_size), vocabulary_size) #old\n",
    "                    # sentence = characters(feed)[0] #old\n",
    "                    sentence = train_batches.id_2_token(feed[0]) #new\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = train_batches.onehot_2_id(sample(prediction, vocabulary_size))\n",
    "                        # sentence += characters(feed)[0] #old\n",
    "                        sentence += train_batches.id_2_token(feed[0]) #new\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# DAY -- this is a previous version where I was modifying BatchGenerator to spit out embeddings\n",
    "#        I am now taking the approach of just putting a wrapper on BatchGenerator\n",
    "\n",
    "\n",
    "if False:\n",
    "    # From char2id window\n",
    "    first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "    vocab = dict()\n",
    "    for i, v in enumerate(list(string.ascii_lowercase[:vocabulary_size-1])):\n",
    "        vocab[v] = i\n",
    "    reverse_vocab = dict(zip(vocab.values(), vocab.keys()))\n",
    "\n",
    "\n",
    "    def char2id(char):\n",
    "        if char in string.ascii_lowercase:\n",
    "            return ord(char) - first_letter + 1\n",
    "        elif char == ' ':\n",
    "            return 0\n",
    "        else:\n",
    "            print('Unexpected character: %s' % char)\n",
    "            return 0\n",
    "\n",
    "    def id2char(dictid):\n",
    "        if dictid > 0:\n",
    "            return chr(dictid + first_letter - 1)\n",
    "        else:\n",
    "            return ' '\n",
    "\n",
    "    def char2embed(char, embeddings):\n",
    "        return tf.nn.embedding_lookup(embeddings, vocab[char])\n",
    "\n",
    "    def embed2char(embed, embeddings):\n",
    "        reverse_vocab[(embeddings == embed).nonzero()[0][0]]\n",
    "\n",
    "\n",
    "    print(char2id('a'), char2id('z'), char2id(' '), char2id('Ã¯'))\n",
    "    print(id2char(1), id2char(26), id2char(0))\n",
    "    print('vocabulary size: %s' % vocabulary_size)\n",
    "\n",
    "    # from BatchGenerator window\n",
    "    batch_size=64\n",
    "    num_unrollings=10\n",
    "\n",
    "\n",
    "    day_debug = True\n",
    "\n",
    "    # DAY\n",
    "    #\n",
    "    # This is important to understand.  Our NN needs a constant sized vector with each input.  We are\n",
    "    # providing that here.  As the video says, just as convolution lets us use the same weight parameters\n",
    "    # at different parts of the image, a recurrent neural net lets us use the same weights at different\n",
    "    # points in time (or rather, different points in the input sequence).\n",
    "    #\n",
    "    # The notion of \"unrollings\" is that a recurrent NN has it's output connected to it's input, but really\n",
    "    # the way to think about it is over time where the output of time t-1 is input to time t.  That way\n",
    "    # of looking at it is like \"unrolling\" the recurrent NN over time so it is understood more as a\n",
    "    # sequence of copies of the NN.  \n",
    "    # In this case, we are going to be feeding in sequences that are 10 long, so we will in effect\n",
    "    # create 10 LSTM cells (which are really just a NN) and hook the output of LSTM cell t with inputs\n",
    "    # from input_sub_t and also the output of LSTM cell t-1.\n",
    "\n",
    "    class BatchGenerator(object):\n",
    "        def __init__(self, text, batch_size, num_unrollings, embeddings):\n",
    "            self._text = text\n",
    "            self._text_size = len(text)\n",
    "            self._batch_size = batch_size\n",
    "            self._num_unrollings = num_unrollings\n",
    "            self._embeddings = embeddings;\n",
    "            segment = self._text_size // batch_size\n",
    "            self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "            self._last_batch = self._next_batch()\n",
    "            if day_debug:\n",
    "                print('BatchGenerator init: \\n  textsize: %s\\n  batch_size: %s\\n  num_unrollings: %s' % \n",
    "                      (len(text), batch_size, num_unrollings))\n",
    "                print(\"BatchGenerator init -- text starts with: %s\" % text[:128])\n",
    "                print(\"BatchGenerator init -- segment=%s\" % segment)\n",
    "                print(\"BatchGenerator init -- _cursor=%s\" % self._cursor)\n",
    "\n",
    "        def _next_batch(self):\n",
    "            \"\"\"Generate a single batch from the current cursor position in the data.\n",
    "            It will be in the form of an embedding)\"\"\"\n",
    "            # DAY\n",
    "            # this is quite confusing, but batch ends up being  of dimension\n",
    "            # (batch_size, vocabulary_size) -- so it is 1 char (one-hot encoded) from\n",
    "            # every _cursor location (there are batch_size cursor locations.  \n",
    "            # Since the cursor locations are not consecutive, these chars in batch\n",
    "            # are not consecutive.  However, in the next step at next(), the \n",
    "            # char will be appended with the next consecutive char.\n",
    "            # Uncomment the print line below to see this.\n",
    "            ids = list()\n",
    "            for b in range(self._batch_size):\n",
    "                ids.append(vocab[self._text[self._cursor[b]]])\n",
    "                self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "            batch = tf.nn.embedding_lookup(self._embeddings, ids)\n",
    "\n",
    "            #batch = np.zeros(shape=(self._batch_size, embedding_size), dtype=np.float) \n",
    "            #for b in range(self._batch_size):\n",
    "                #batch[b] = char2embed(self._text[self._cursor[b]], self._embeddings)\n",
    "                #self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "            return batch\n",
    "\n",
    "        def next(self):\n",
    "            \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "            the last batch of the previous array, followed by num_unrollings new ones.\n",
    "            \"\"\"\n",
    "            batches = [self._last_batch]\n",
    "            for step in range(self._num_unrollings):\n",
    "                batches.append(self._next_batch())\n",
    "            self._last_batch = batches[-1]\n",
    "            return batches\n",
    "\n",
    "    def characters(probabilities, embeddings):\n",
    "        \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "        characters back into its (most likely) character representation.\"\"\"\n",
    "        return [embed2char(e, embeddings) for e in np.argmax(probabilities, 1)]\n",
    "\n",
    "# my second definition of characters:\n",
    "    def characters(probabilities):\n",
    "        \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "        characters back into its (most likely) character representation.\"\"\"\n",
    "        global reverse_vocab\n",
    "        return [reverse_vocab[c] for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "    def batches2string(batches):\n",
    "        \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "        representation.\"\"\"\n",
    "        # DAY\n",
    "        # This mangles the real batch structure in the interest of readability, but\n",
    "        # by doing so, makes your understanding of batches wrong.\n",
    "        # See my 'honest_batches2string' below which gives you a better\n",
    "        # understanding of the batch format.\n",
    "\n",
    "        # batches has dimensions (num_unrollings, batch_size, vocabulary_size)\n",
    "        s = [''] * batches[0].shape[0]  # batches[0].shape[0] will end up being same as batch_size\n",
    "        for b in batches: # there will be num_unrollings of these...\n",
    "            # each b (batch) is 64 by 27\n",
    "            s = [''.join(x) for x in zip(s, characters(b))]\n",
    "        return s\n",
    "\n",
    "    def honest_batches2string(batches, embeddings):\n",
    "        import pprint\n",
    "        output = []\n",
    "        for b_index, b in enumerate(batches):  # there will be 'num_unrollings' of these\n",
    "            print(b)\n",
    "            output.append(list())\n",
    "            for i in range(batch_size):  # there will be 'batch_size' of these\n",
    "                output[i].append(embed2char(b[i], embeddings))\n",
    "        return pprint.pformat(output)\n",
    "\n",
    "\n",
    "\n",
    "    if day_debug:\n",
    "        test_embeddings = tf.Variable(\n",
    "            tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "        # DAY\n",
    "        # Notice how this output is structured. Subsequent batches are related to each other in that\n",
    "        # you can continue reading the text from batch1[0] to batch2[0]  (similarly from\n",
    "        # batch1[20] to batch2[20])\n",
    "        my_text = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "        my_batches = BatchGenerator(my_text.lower(), 4, 2, test_embeddings)\n",
    "        if False:\n",
    "            print(my_batches.next())\n",
    "            print(my_batches.next())\n",
    "            print(my_batches.next())\n",
    "            print(my_batches.next())\n",
    "            print(my_batches.next())\n",
    "            print(my_batches.next())\n",
    "        else:\n",
    "            print(honest_batches2string(my_batches.next(), test_embeddings))\n",
    "            print(honest_batches2string(my_batches.next(), test_embeddings))\n",
    "            print(honest_batches2string(my_batches.next(), test_embeddings))\n",
    "            print(honest_batches2string(my_batches.next(), test_embeddings))\n",
    "            print(honest_batches2string(my_batches.next(), test_embeddings))\n",
    "            print(honest_batches2string(my_batches.next(), test_embeddings))\n",
    "\n",
    "        my_text = \"Four score and seven years ago our fathers brought forth on this continent, a new \\\n",
    "        nation, conceived in Liberty, and dedicated to the proposition that all men are created equal. Now \\\n",
    "        we are engaged in a great civil war, testing whether that nation, or any nation so conceived and \\\n",
    "        so dedicated, can long endure. We are met on a great battle-field of that war. We have come to \\\n",
    "        dedicate a portion of that field, as a final resting place for those who here gave their lives that \\\n",
    "        that nation might live. It is altogether fitting and proper that we should do this.\\\n",
    "        But, in a larger sense, we can not dedicate -- we can not consecrate -- we can not hallow -- \\\n",
    "        this ground. The brave men, living and dead, who struggled here, have consecrated it, far above \\\n",
    "        our poor power to add or detract. The world will little note, nor long remember what we say here, \\\n",
    "        but it can never forget what they did here. It is for us the living, rather, to be dedicated here \\\n",
    "        to the unfinished work which they who fought here have thus far so nobly advanced. It is rather for \\\n",
    "        us to be here dedicated to the great task remaining before us -- that from these honored dead we \\\n",
    "        take increased devotion to that cause for which they gave the last full measure of devotion -- that \\\n",
    "        we here highly resolve that these dead shall not have died in vain -- that this nation, under God, \\\n",
    "        shall have a new birth of freedom -- and that government of the people, by the people, for the people, \\\n",
    "        shall not perish from the earth.\"\n",
    "        my_batches = BatchGenerator(my_text.lower(), 10, 5)\n",
    "        print(honest_batches2string(my_batches.next()))\n",
    "        print(honest_batches2string(my_batches.next()))\n",
    "        print(honest_batches2string(my_batches.next()))\n",
    "        print(honest_batches2string(my_batches.next()))\n",
    "        print(honest_batches2string(my_batches.next()))\n",
    "        print(honest_batches2string(my_batches.next()))\n",
    "        # Also, here is how a batch *really* looks (an batch_size array of one-hot encodings)\n",
    "        temp = my_batches.next()\n",
    "        print('Dimensions of the batch (num_unrollings, batch_size, vocab_size): (%d, %d, %d)' % \n",
    "              (len(temp), len(temp[0]), len(temp[0][0])))\n",
    "        print('='*80)\n",
    "\n",
    "    #train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "    #valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "    #print('WARNING: the following printed lines are not indicative of the structure that goes into the model')\n",
    "    #print(batches2string(train_batches.next()))\n",
    "    #print(batches2string(train_batches.next()))\n",
    "    #print(batches2string(valid_batches.next()))\n",
    "    #print(batches2string(valid_batches.next()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Answer 3\n",
    "========\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
