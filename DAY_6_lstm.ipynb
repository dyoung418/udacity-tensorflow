{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "          'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    f = zipfile.ZipFile(filename)\n",
    "    for name in f.namelist():\n",
    "        return tf.compat.as_str(f.read(name))\n",
    "    f.close()\n",
    "\n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data size: 99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "validation data size: 1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print('training data size: %s %s' % (train_size, train_text[:64]))\n",
    "print('validation data size: %s %s' % (valid_size, valid_text[:64]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 26 0 0\n",
      "a z  \n",
      "vocabulary size: 27\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        #print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "    \n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('Ã¯'))\n",
    "print(id2char(1), id2char(26), id2char(0))\n",
    "print('vocabulary size: %s' % vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchGenerator init: \n",
      "  textsize: 26\n",
      "  batch_size: 4\n",
      "  num_unrollings: 2\n",
      "BatchGenerator init -- text starts with: abcdefghijklmnopqrstuvwxyz\n",
      "BatchGenerator init -- segment=6\n",
      "BatchGenerator init -- _cursor=[1, 7, 13, 19]\n",
      "[[['a'], ['g'], ['m'], ['s']],\n",
      " [['b'], ['h'], ['n'], ['t']],\n",
      " [['c'], ['i'], ['o'], ['u']]]\n",
      "[[['c'], ['i'], ['o'], ['u']],\n",
      " [['d'], ['j'], ['p'], ['v']],\n",
      " [['e'], ['k'], ['q'], ['w']]]\n",
      "[[['e'], ['k'], ['q'], ['w']],\n",
      " [['f'], ['l'], ['r'], ['x']],\n",
      " [['g'], ['m'], ['s'], ['y']]]\n",
      "[[['g'], ['m'], ['s'], ['y']],\n",
      " [['h'], ['n'], ['t'], ['z']],\n",
      " [['i'], ['o'], ['u'], ['a']]]\n",
      "[[['i'], ['o'], ['u'], ['a']],\n",
      " [['j'], ['p'], ['v'], ['b']],\n",
      " [['k'], ['q'], ['w'], ['c']]]\n",
      "[[['k'], ['q'], ['w'], ['c']],\n",
      " [['l'], ['r'], ['x'], ['d']],\n",
      " [['m'], ['s'], ['y'], ['e']]]\n",
      "BatchGenerator init: \n",
      "  textsize: 1532\n",
      "  batch_size: 10\n",
      "  num_unrollings: 5\n",
      "BatchGenerator init -- text starts with: four score and seven years ago our fathers brought forth on this continent, a new     nation, conceived in liberty, and dedicate\n",
      "BatchGenerator init -- segment=153\n",
      "BatchGenerator init -- _cursor=[1, 154, 307, 460, 613, 766, 919, 1072, 1225, 1378]\n",
      "Unexpected character: .\n",
      "[[['f'], [' '], ['n'], ['h'], ['o'], ['r'], ['e'], ['a'], ['f'], [' ']],\n",
      " [['o'], ['a'], [' '], ['o'], ['t'], [' '], ['r'], ['t'], ['o'], ['u']],\n",
      " [['u'], ['l'], ['l'], [' '], [' '], ['a'], ['e'], ['h'], ['r'], ['n']],\n",
      " [['r'], ['l'], ['o'], ['h'], ['d'], ['b'], [' '], ['e'], [' '], ['d']],\n",
      " [[' '], [' '], ['n'], ['e'], ['e'], ['o'], [' '], ['r'], ['w'], ['e']],\n",
      " [['s'], ['m'], ['g'], ['r'], ['d'], ['v'], ['i'], [' '], ['h'], ['r']]]\n",
      "Unexpected character: ,\n",
      "[[['s'], ['m'], ['g'], ['r'], ['d'], ['v'], ['i'], [' '], ['h'], ['r']],\n",
      " [['c'], ['e'], [' '], ['e'], ['i'], ['e'], ['t'], ['f'], ['i'], [' ']],\n",
      " [['o'], ['n'], ['e'], [' '], ['c'], [' '], [' '], ['o'], ['c'], ['g']],\n",
      " [['r'], [' '], ['n'], ['g'], ['a'], [' '], ['i'], ['r'], ['h'], ['o']],\n",
      " [['e'], ['a'], ['d'], ['a'], ['t'], [' '], ['s'], [' '], [' '], ['d']],\n",
      " [[' '], ['r'], ['u'], ['v'], ['e'], [' '], [' '], [' '], ['t'], [' ']]]\n",
      "Unexpected character: -\n",
      "Unexpected character: .\n",
      "Unexpected character: -\n",
      "[[[' '], ['r'], ['u'], ['v'], ['e'], [' '], [' '], [' '], ['t'], [' ']],\n",
      " [['a'], ['e'], ['r'], ['e'], [' '], [' '], ['f'], [' '], ['h'], [' ']],\n",
      " [['n'], [' '], ['e'], [' '], [' '], ['o'], ['o'], [' '], ['e'], [' ']],\n",
      " [['d'], ['c'], [' '], ['t'], [' '], ['u'], ['r'], [' '], ['y'], [' ']],\n",
      " [[' '], ['r'], [' '], ['h'], [' '], ['r'], [' '], ['u'], [' '], [' ']],\n",
      " [['s'], ['e'], ['w'], ['e'], ['w'], [' '], ['u'], ['s'], ['g'], [' ']]]\n",
      "[[['s'], ['e'], ['w'], ['e'], ['w'], [' '], ['u'], ['s'], ['g'], [' ']],\n",
      " [['e'], ['a'], ['e'], ['i'], ['e'], ['p'], ['s'], [' '], ['a'], ['s']],\n",
      " [['v'], ['t'], [' '], ['r'], [' '], ['o'], [' '], ['t'], ['v'], ['h']],\n",
      " [['e'], ['e'], ['a'], [' '], ['c'], ['o'], ['t'], ['o'], ['e'], ['a']],\n",
      " [['n'], ['d'], ['r'], ['l'], ['a'], ['r'], ['h'], [' '], [' '], ['l']],\n",
      " [[' '], [' '], ['e'], ['i'], ['n'], [' '], ['e'], ['b'], ['t'], ['l']]]\n",
      "[[[' '], [' '], ['e'], ['i'], ['n'], [' '], ['e'], ['b'], ['t'], ['l']],\n",
      " [['y'], ['e'], [' '], ['v'], [' '], ['p'], [' '], ['e'], ['h'], [' ']],\n",
      " [['e'], ['q'], ['m'], ['e'], ['n'], ['o'], ['l'], [' '], ['e'], ['h']],\n",
      " [['a'], ['u'], ['e'], ['s'], ['o'], ['w'], ['i'], ['h'], [' '], ['a']],\n",
      " [['r'], ['a'], ['t'], [' '], ['t'], ['e'], ['v'], ['e'], ['l'], ['v']],\n",
      " [['s'], ['l'], [' '], ['t'], [' '], ['r'], ['i'], ['r'], ['a'], ['e']]]\n",
      "Unexpected character: .\n",
      "Unexpected character: ,\n",
      "[[['s'], ['l'], [' '], ['t'], [' '], ['r'], ['i'], ['r'], ['a'], ['e']],\n",
      " [[' '], [' '], ['o'], ['h'], ['c'], [' '], ['n'], ['e'], ['s'], [' ']],\n",
      " [['a'], [' '], ['n'], ['a'], ['o'], ['t'], ['g'], [' '], ['t'], ['a']],\n",
      " [['g'], ['n'], [' '], ['t'], ['n'], ['o'], [' '], ['d'], [' '], [' ']],\n",
      " [['o'], ['o'], ['a'], [' '], ['s'], [' '], [' '], ['e'], ['f'], ['n']],\n",
      " [[' '], ['w'], [' '], [' '], ['e'], ['a'], ['r'], ['d'], ['u'], ['e']]]\n",
      "Dimensions of the batch (num_unrollings, batch_size, vocab_size): (6, 10, 27)\n",
      "================================================================================\n",
      "BatchGenerator init: \n",
      "  textsize: 99999000\n",
      "  batch_size: 64\n",
      "  num_unrollings: 10\n",
      "BatchGenerator init -- text starts with: ons anarchists advocate social relations based upon voluntary association of autonomous individuals mutual aid and self governan\n",
      "BatchGenerator init -- segment=1562484\n",
      "BatchGenerator init -- _cursor=[1, 1562485, 3124969, 4687453, 6249937, 7812421, 9374905, 10937389, 12499873, 14062357, 15624841, 17187325, 18749809, 20312293, 21874777, 23437261, 24999745, 26562229, 28124713, 29687197, 31249681, 32812165, 34374649, 35937133, 37499617, 39062101, 40624585, 42187069, 43749553, 45312037, 46874521, 48437005, 49999489, 51561973, 53124457, 54686941, 56249425, 57811909, 59374393, 60936877, 62499361, 64061845, 65624329, 67186813, 68749297, 70311781, 71874265, 73436749, 74999233, 76561717, 78124201, 79686685, 81249169, 82811653, 84374137, 85936621, 87499105, 89061589, 90624073, 92186557, 93749041, 95311525, 96874009, 98436493]\n",
      "BatchGenerator init: \n",
      "  textsize: 1000\n",
      "  batch_size: 1\n",
      "  num_unrollings: 1\n",
      "BatchGenerator init -- text starts with:  anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english re\n",
      "BatchGenerator init -- segment=1000\n",
      "BatchGenerator init -- _cursor=[1]\n",
      "WARNING: the following printed lines are not indicative of the structure that goes into the model\n",
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "day_debug = True\n",
    "\n",
    "# DAY\n",
    "#\n",
    "# This is important to understand.  Our NN needs a constant sized vector with each input.  We are\n",
    "# providing that here.  As the video says, just as convolution lets us use the same weight parameters\n",
    "# at different parts of the image, a recurrent neural net lets us use the same weights at different\n",
    "# points in time (or rather, different points in the input sequence).\n",
    "#\n",
    "# The notion of \"unrollings\" is that a recurrent NN has it's output connected to it's input, but really\n",
    "# the way to think about it is over time where the output of time t-1 is input to time t.  That way\n",
    "# of looking at it is like \"unrolling\" the recurrent NN over time so it is understood more as a\n",
    "# sequence of copies of the NN.  \n",
    "# In this case, we are going to be feeding in sequences that are 10 long, so we will in effect\n",
    "# create 10 LSTM cells (which are really just a NN) and hook the output of LSTM cell t with inputs\n",
    "# from input_sub_t and also the output of LSTM cell t-1.\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "        if day_debug:\n",
    "            print('BatchGenerator init: \\n  textsize: %s\\n  batch_size: %s\\n  num_unrollings: %s' % \n",
    "                  (len(text), batch_size, num_unrollings))\n",
    "            print(\"BatchGenerator init -- text starts with: %s\" % text[:128])\n",
    "            print(\"BatchGenerator init -- segment=%s\" % segment)\n",
    "            print(\"BatchGenerator init -- _cursor=%s\" % self._cursor)\n",
    "  \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\n",
    "        It will be in the form of a 1 hot encoding here (but not in problem 2)\"\"\"\n",
    "        # DAY\n",
    "        # this is quite confusing, but batch ends up being  of dimension\n",
    "        # (batch_size, vocabulary_size) -- so it is 1 char (one-hot encoded) from\n",
    "        # every _cursor location (there are batch_size cursor locations.  \n",
    "        # Since the cursor locations are not consecutive, these chars in batch\n",
    "        # are not consecutive.  However, in the next step at next(), the \n",
    "        # char will be appended with the next consecutive char.\n",
    "        # Uncomment the print line below to see this.\n",
    "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        #print(batches2string([batch]))\n",
    "        return batch\n",
    "  \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    # DAY\n",
    "    # This mangles the real batch structure in the interest of readability, but\n",
    "    # by doing so, makes your understanding of batches wrong.\n",
    "    # See my 'honest_batches2string' below which gives you a better\n",
    "    # understanding of the batch format.\n",
    "    \n",
    "    # batches has dimensions (num_unrollings, batch_size, vocabulary_size)\n",
    "    s = [''] * batches[0].shape[0]  # batches[0].shape[0] will end up being same as batch_size\n",
    "    for b in batches: # there will be num_unrollings of these...\n",
    "        # each b (batch) is 64 by 27\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "def honest_batches2string(batches):\n",
    "    import pprint\n",
    "    output = []\n",
    "    for b_index, b in enumerate(batches):  # there will be 'num_unrollings' of these\n",
    "        output.append(list())\n",
    "        for one_hot_index, one_hot in enumerate(b):  # there will be 'batch_size' of these\n",
    "            output[b_index].append(characters([one_hot]))\n",
    "    return pprint.pformat(output)\n",
    "            \n",
    "        \n",
    "\n",
    "if day_debug:\n",
    "    # DAY\n",
    "    # Notice how this output is structured. Subsequent batches are related to each other in that\n",
    "    # you can continue reading the text from batch1[0] to batch2[0]  (similarly from\n",
    "    # batch1[20] to batch2[20])\n",
    "    my_text = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "    my_batches = BatchGenerator(my_text.lower(), 4, 2)\n",
    "    if False:\n",
    "        print(my_batches.next())\n",
    "        print(my_batches.next())\n",
    "        print(my_batches.next())\n",
    "        print(my_batches.next())\n",
    "        print(my_batches.next())\n",
    "        print(my_batches.next())\n",
    "    else:\n",
    "        print(honest_batches2string(my_batches.next()))\n",
    "        print(honest_batches2string(my_batches.next()))\n",
    "        print(honest_batches2string(my_batches.next()))\n",
    "        print(honest_batches2string(my_batches.next()))\n",
    "        print(honest_batches2string(my_batches.next()))\n",
    "        print(honest_batches2string(my_batches.next()))\n",
    "\n",
    "    my_text = \"Four score and seven years ago our fathers brought forth on this continent, a new \\\n",
    "    nation, conceived in Liberty, and dedicated to the proposition that all men are created equal. Now \\\n",
    "    we are engaged in a great civil war, testing whether that nation, or any nation so conceived and \\\n",
    "    so dedicated, can long endure. We are met on a great battle-field of that war. We have come to \\\n",
    "    dedicate a portion of that field, as a final resting place for those who here gave their lives that \\\n",
    "    that nation might live. It is altogether fitting and proper that we should do this.\\\n",
    "    But, in a larger sense, we can not dedicate -- we can not consecrate -- we can not hallow -- \\\n",
    "    this ground. The brave men, living and dead, who struggled here, have consecrated it, far above \\\n",
    "    our poor power to add or detract. The world will little note, nor long remember what we say here, \\\n",
    "    but it can never forget what they did here. It is for us the living, rather, to be dedicated here \\\n",
    "    to the unfinished work which they who fought here have thus far so nobly advanced. It is rather for \\\n",
    "    us to be here dedicated to the great task remaining before us -- that from these honored dead we \\\n",
    "    take increased devotion to that cause for which they gave the last full measure of devotion -- that \\\n",
    "    we here highly resolve that these dead shall not have died in vain -- that this nation, under God, \\\n",
    "    shall have a new birth of freedom -- and that government of the people, by the people, for the people, \\\n",
    "    shall not perish from the earth.\"\n",
    "    my_batches = BatchGenerator(my_text.lower(), 10, 5)\n",
    "    print(honest_batches2string(my_batches.next()))\n",
    "    print(honest_batches2string(my_batches.next()))\n",
    "    print(honest_batches2string(my_batches.next()))\n",
    "    print(honest_batches2string(my_batches.next()))\n",
    "    print(honest_batches2string(my_batches.next()))\n",
    "    print(honest_batches2string(my_batches.next()))\n",
    "    # Also, here is how a batch *really* looks (an batch_size array of one-hot encodings)\n",
    "    temp = my_batches.next()\n",
    "    print('Dimensions of the batch (num_unrollings, batch_size, vocab_size): (%d, %d, %d)' % \n",
    "          (len(temp), len(temp[0]), len(temp[0][0])))\n",
    "    print('='*80)\n",
    "\n",
    "\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print('WARNING: the following printed lines are not indicative of the structure that goes into the model')\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0  # prediction is in column format, so it must be indexed by [0]\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model.\n",
    "\n",
    "_*DAY: Note that I have modified this to comment/understand*_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1)) # [27, 64]\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))       # [64, 64]\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))                                     # [1, 64]\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False) #notice the 'trainable=False'\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False) #notice the 'trainable=False'\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "    # The LSTM\n",
    "    #\n",
    "    # In the code above, batch_size (bs) and num_nodes (nn) are both 64 (they don't have to be equal)\n",
    "    # 27 is the vocabulary size.\n",
    "    #\n",
    "    # input_gate(bs, nn)   = sigmoid( input(bs, 27) * ix(27, nn) + output(bs, nn) * im(nn, nn) + ib(1, nn) )\n",
    "    # forget_gate(bs, nn)  = sigmoid( input(bs, 27) * ix(27, nn) + output(bs, nn) * im(nn, nn) + ib(1, nn) )\n",
    "    # output_gate(bs, nn)  = sigmoid( input(bs, 27) * ix(27, nn) + output(bs, nn) * im(nn, nn) + ib(1, nn) )\n",
    "    # update(bs, nn)       = tanh(    input(bs, 27) * cx(27, nn) + output(bs, nn) * cm(nn, nn) + cb(1, nn) )\n",
    "    #\n",
    "    # output(bs, nn) = output_gate(bs, nn) * tanh( state(bs, nn) )\n",
    "    # state(bs, nn)  = forget_gate(bs, nn) * state(bs, nn)  +  input_gate(bs, nn) * update(bs, nn)\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            # These 11 elements of train_data will be pulled in from feed_dict\n",
    "            # Note that usually we have seen feed_dict specified as {var_name: value}\n",
    "            # but in this case, since these 11 array elements don't have a var_name, the \n",
    "            # feed_dict will use the tensorflow object as the key instead, i.e.\n",
    "            # feed_dict={<tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: value}\n",
    "            # See below where the feed_dict is prepared before calling session.run.\n",
    "            tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size])) #pulled in from feed_dict\n",
    "    # train_data now has the shape (11, 64, 27), or (num_unrollings, batch_size, vocabulary_size)\n",
    "    # and sequential text from the original text input is 'striped' across the first dimension (11)\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step. (Brilliant!)\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs: # since train_inputs is num_unrollings=10 long, this will create 10 LSTM cells\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output) # at each iter of the lstm_cell, append the character it predicted to outputs.\n",
    "\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        \n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits, tf.concat(0, train_labels)))\n",
    "\n",
    "    # Optimizer.\n",
    "    #   Note that all 10 unrollings are done before the optimizer comes in and looks at the\n",
    "    #   output sequence of 10 chars vs. the label sequence of 10 chars and then calculates\n",
    "    #   the gradients and adjusts the parameters.  Then in the next step another 10 characters\n",
    "    #   will be predicted.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    # DAY: this clipping below is the hack to prevent exploding gradients \n",
    "    #(LSTM was the elegant solution to prevent vanishing gradient)\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    # (nothing here is triggered in the training)\n",
    "    #     first, variables\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    #     reset zeros out saved_sample_output and saved_sample_state\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    #     Define one lstm_cell with no unrolling (will be used for sampling from the trained model)\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    #     Define the next prediction (but make sure dependencies are calculated first)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I've modified this with comments to understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.300142 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.12\n",
      "================================================================================\n",
      "lqy baquzzqshluwe  uneejomcunzn vxh  ts o op    f niyemiynetuelhtdelj cqvtoe hd \n",
      "nffghw e sklcio yvcdroaydod wr  tlcgq geesnqojqwcu kececysr ehpqliklpsudzatfma z\n",
      "shfa gjc  clvvwava w lx ikyzkiq h yuuerbtvsrshw pnbifupptpep  m uqdarmesi bn wm \n",
      "xknulgrvtart  a rqaifcovbov x kje emeet e fabfewu s mhys i dv egetvdp hhesxsiouz\n",
      "arunxakmpatiwrxzecdfsqrjk goyufbtcsqiuauyrpeztoiejc dcmaeandizu pem ffi  snnft c\n",
      "================================================================================\n",
      "Validation set perplexity: 20.34\n",
      "Average loss at step 100: 2.622735 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.70\n",
      "Validation set perplexity: 10.89\n",
      "Average loss at step 200: 2.282770 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.75\n",
      "Validation set perplexity: 8.75\n",
      "Average loss at step 300: 2.124059 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.55\n",
      "Validation set perplexity: 8.09\n",
      "Average loss at step 400: 2.012910 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.60\n",
      "Validation set perplexity: 7.79\n",
      "Average loss at step 500: 1.941874 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 600: 1.907705 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 700: 1.849646 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 6.41\n",
      "Average loss at step 800: 1.807127 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 6.10\n",
      "Average loss at step 900: 1.806605 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.88\n",
      "Validation set perplexity: 5.94\n",
      "Average loss at step 1000: 1.793093 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "================================================================================\n",
      "er ameuss stiate popile bord of scates in will cent uts in one seve vava of kele\n",
      "no will ficted the compuric gre porn fie nereaty mefule the cary to holevals hea\n",
      "h tropkhkie uild heather and commin b ocharin sovel begoys supula exsle to they \n",
      "ate effinated centure with upen of a skeding was of to of a their lefformh leato\n",
      "boat of adpenten evecation esconic sougnes orchelle s betber will fyugaral petsi\n",
      "================================================================================\n",
      "Validation set perplexity: 5.78\n",
      "Average loss at step 1100: 1.738985 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 1200: 1.713401 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 1300: 1.685875 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 1400: 1.693359 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 1500: 1.674978 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 1600: 1.677563 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 1700: 1.641527 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 1800: 1.603437 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 1900: 1.572709 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 2000: 1.614549 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "================================================================================\n",
      "der is stan mudrile the lanouch elpons impgrance ont dickey flars rost offered d\n",
      "inias is whitha musy the but sopulagication of comidging torgest it astemment of\n",
      "u only proditial spetpoby as english covensing proys end occodign elish elcrepen\n",
      "s of intoniop its andia voicllebhojapricarly shull to partly coirments ponnized \n",
      "x one nine three k conglost pranomicing four see are prodicer stantars for v p s\n",
      "================================================================================\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 2100: 1.604355 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 2200: 1.591628 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 2300: 1.557330 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 2400: 1.569910 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 2500: 1.590904 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 2600: 1.563792 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 2700: 1.567464 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.10\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 2800: 1.555282 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.08\n",
      "Average loss at step 2900: 1.555967 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.14\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-e8b01c4981e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         _, l, predictions, lr = session.run(\n\u001b[1;32m---> 43\u001b[1;33m               [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[0mmean_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msummary_frequency\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict)\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;33m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m`\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mdoesn\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mt\u001b[0m \u001b[0mexist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m     \"\"\"\n\u001b[1;32m--> 315\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mpartial_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict)\u001b[0m\n\u001b[0;32m    509\u001b[0m     \u001b[1;31m# Run request and get response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m     results = self._do_run(handle, target_list, unique_fetches,\n\u001b[1;32m--> 511\u001b[1;33m                            feed_dict_string)\n\u001b[0m\u001b[0;32m    512\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m     \u001b[1;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict)\u001b[0m\n\u001b[0;32m    562\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 564\u001b[1;33m                            target_list)\n\u001b[0m\u001b[0;32m    565\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    566\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m    569\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 571\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    572\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatusNotOK\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m       \u001b[0me_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me_traceback\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    556\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next() #\n",
    "        # batches is of dimension (num_unrollings, batch_size, vocabulary_size) (11, 64, 27)\n",
    "        # where sequential text from the input is \"striped\" across the unrollings.  For \n",
    "        # example, if 'char1' stands for the first character in the original text (one-hot encoded), \n",
    "        # batches looks like this (assuming 'segment' is 15000):\n",
    "        # [                                        # there are 'num_unrollings' rows\n",
    "        #   [char1,  char15000,  char 30000, ...], # each row is 'batch_size'\n",
    "        #   [char2,  char15001,  char 30001, ...],\n",
    "        #   ...\n",
    "        #   [char11, char15010,  char 30010, ...]\n",
    "        # ]\n",
    "        # when we call train_batches.next(), the next 'batches' will look like this:\n",
    "        # [                                        # there are 'num_unrollings' rows\n",
    "        #   [char11, char15010,  char 30010, ...], # each row is 'batch_size'\n",
    "        #   [char12, char15011,  char 30011, ...],\n",
    "        #   ...\n",
    "        #   [char21, char15020,  char 30020, ...]\n",
    "        # ]\n",
    "        # it might look like a bug that the second 'batches' repeats char11, char1510, etc.\n",
    "        # but it is not a bug.  in the first 'batches', char11 was included only as the label\n",
    "        # needed for the 10th entry (char10). (LSTM takes char10 in as an input and expects char11\n",
    "        # as the true label of the output).  So -- char11 was never put into the LSTM_cell in the\n",
    "        # first 'batches' -- it is only used as a label, so it need to be included as the first\n",
    "        # item in the second 'batches' so that in can now be an input into an LSTM cell.\n",
    "        #\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "            # Normally we see feed_dict={var_name: value}, but here we don't have the var_names\n",
    "            # for the training data batches in the graph definition (it is an array of tensors)\n",
    "            # so instead, we use the tensorflow object itself (from the graph definition, in\n",
    "            # train_data[i]) as the key in the feed_dict entries.\n",
    "            \n",
    "        _, l, predictions, lr = session.run(\n",
    "              [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())  # start with a random character\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAY - small insertion to understand the character sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      ".step 0:\n",
      "    feed_dict={<tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_5:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_10:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_4:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_9:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_3:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder:0' shape=(64, 27) dtype=float32>: array([[ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_8:0' shape=(64, 27) dtype=float32>: array([[ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_2:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_7:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_6:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])}\n",
      "    train_prediction=Tensor(\"Softmax:0\", shape=(640, 27), dtype=float32)\n",
      ".step 25:\n",
      "    feed_dict={<tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_5:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_10:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_4:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_9:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_3:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_8:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_2:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  1., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_7:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  1.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_6:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])}\n",
      "    train_prediction=Tensor(\"Softmax:0\", shape=(640, 27), dtype=float32)\n",
      ".step 50:\n",
      "    feed_dict={<tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_5:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_10:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_4:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_9:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_3:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_8:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_2:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_7:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_6:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])}\n",
      "    train_prediction=Tensor(\"Softmax:0\", shape=(640, 27), dtype=float32)\n",
      ".step 75:\n",
      "    feed_dict={<tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_5:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_10:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_4:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_9:0' shape=(64, 27) dtype=float32>: array([[ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_3:0' shape=(64, 27) dtype=float32>: array([[ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_8:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_2:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_7:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_6:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])}\n",
      "    train_prediction=Tensor(\"Softmax:0\", shape=(640, 27), dtype=float32)\n",
      ".step 100:\n",
      "    feed_dict={<tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_5:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_10:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_4:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_9:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_3:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_8:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_2:0' shape=(64, 27) dtype=float32>: array([[ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_7:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_6:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  1.,  0.]])}\n",
      "    train_prediction=Tensor(\"Softmax:0\", shape=(640, 27), dtype=float32)\n",
      ".step 125:\n",
      "    feed_dict={<tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_5:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_10:0' shape=(64, 27) dtype=float32>: array([[ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_4:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_9:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_3:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_8:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_2:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_7:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_6:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]])}\n",
      "    train_prediction=Tensor(\"Softmax:0\", shape=(640, 27), dtype=float32)\n",
      ".step 150:\n",
      "    feed_dict={<tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_5:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_10:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_4:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_9:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_3:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_8:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_2:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_7:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_6:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])}\n",
      "    train_prediction=Tensor(\"Softmax:0\", shape=(640, 27), dtype=float32)\n",
      ".step 175:\n",
      "    feed_dict={<tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  1., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_5:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_10:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_4:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_9:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_3:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_8:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_2:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_7:0' shape=(64, 27) dtype=float32>: array([[ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_6:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]])}\n",
      "    train_prediction=Tensor(\"Softmax:0\", shape=(640, 27), dtype=float32)\n",
      ".step 200:\n",
      "    feed_dict={<tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_5:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_10:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_4:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  1.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_9:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_3:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder:0' shape=(64, 27) dtype=float32>: array([[ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_8:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_2:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_7:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_6:0' shape=(64, 27) dtype=float32>: array([[ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.]])}\n",
      "    train_prediction=Tensor(\"Softmax:0\", shape=(640, 27), dtype=float32)\n",
      ".step 225:\n",
      "    feed_dict={<tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_5:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_10:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  1.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_4:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_9:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_3:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  1.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_8:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_2:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_7:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_6:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])}\n",
      "    train_prediction=Tensor(\"Softmax:0\", shape=(640, 27), dtype=float32)\n",
      ".step 250:\n",
      "    feed_dict={<tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  1.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_5:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_10:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  1.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_4:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_9:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_3:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_8:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_2:0' shape=(64, 27) dtype=float32>: array([[ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_7:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_6:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])}\n",
      "    train_prediction=Tensor(\"Softmax:0\", shape=(640, 27), dtype=float32)\n",
      ".step 275:\n",
      "    feed_dict={<tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_5:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_10:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_4:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_9:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_3:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_8:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_2:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_7:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_6:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])}\n",
      "    train_prediction=Tensor(\"Softmax:0\", shape=(640, 27), dtype=float32)\n",
      ".step 300:\n",
      "    feed_dict={<tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  1.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_5:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_10:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_4:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  1.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_9:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_3:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_8:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_2:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  1.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_7:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_6:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])}\n",
      "    train_prediction=Tensor(\"Softmax:0\", shape=(640, 27), dtype=float32)\n",
      ".step 325:\n",
      "    feed_dict={<tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_5:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  1.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_10:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_4:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_9:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_3:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_8:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_2:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_7:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_6:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])}\n",
      "    train_prediction=Tensor(\"Softmax:0\", shape=(640, 27), dtype=float32)\n",
      ".step 350:\n",
      "    feed_dict={<tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_5:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_10:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_4:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_9:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_3:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_8:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_2:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_7:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_6:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])}\n",
      "    train_prediction=Tensor(\"Softmax:0\", shape=(640, 27), dtype=float32)\n",
      ".step 375:\n",
      "    feed_dict={<tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_5:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_10:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_4:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_9:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_3:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_8:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_2:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_7:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_6:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])}\n",
      "    train_prediction=Tensor(\"Softmax:0\", shape=(640, 27), dtype=float32)\n",
      ".step 400:\n",
      "    feed_dict={<tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_5:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_10:0' shape=(64, 27) dtype=float32>: array([[ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  1.,  0.]]), <tf.Tensor 'Placeholder_4:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_9:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_3:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_8:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_2:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_7:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_6:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])}\n",
      "    train_prediction=Tensor(\"Softmax:0\", shape=(640, 27), dtype=float32)\n",
      ".step 425:\n",
      "    feed_dict={<tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: array([[ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_5:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_10:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_4:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_9:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_3:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_8:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  1., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_2:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_7:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_6:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])}\n",
      "    train_prediction=Tensor(\"Softmax:0\", shape=(640, 27), dtype=float32)\n",
      ".step 450:\n",
      "    feed_dict={<tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_5:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_10:0' shape=(64, 27) dtype=float32>: array([[ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_4:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_9:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_3:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_8:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_2:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_7:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_6:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])}\n",
      "    train_prediction=Tensor(\"Softmax:0\", shape=(640, 27), dtype=float32)\n",
      ".step 475:\n",
      "    feed_dict={<tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_5:0' shape=(64, 27) dtype=float32>: array([[ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_10:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_4:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_9:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_3:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  1., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_8:0' shape=(64, 27) dtype=float32>: array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_2:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_7:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), <tf.Tensor 'Placeholder_6:0' shape=(64, 27) dtype=float32>: array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])}\n",
      "    train_prediction=Tensor(\"Softmax:0\", shape=(640, 27), dtype=float32)\n",
      "\n",
      "Average loss over the 500 steps was 2.182653\n",
      "Minibatch perplexity: 6.43\n",
      "Perplexity of last minibatch (i.e.how confused was the model in choosing the next char -- ie roughly how many chars with equal probability did it have to chose from) was %.2f 6.43013026474\n",
      "\n",
      "Sampling Explanation:\n",
      "================================================================================\n",
      "1. random_distribution yields vocabulary_size(27) random #s [[ 0.01425268  0.00835906  0.03685474  0.067072    0.03727936  0.06843429\n",
      "   0.02968602  0.0654132   0.04205735  0.05169287  0.02099627  0.05987258\n",
      "   0.01846685  0.01752894  0.00787724  0.04693876  0.01184752  0.01734169\n",
      "   0.0429008   0.03345313  0.04658089  0.04056307  0.05189919  0.02874979\n",
      "   0.06520087  0.05448838  0.01419246]]\n",
      "2. take a sample from that in a one-hot format, call it \"feed\": [[ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "3. characters converts to a char.  character(feed)=['f']\n",
      "4. the beginning of your \"sentence\" (really a line) is this char: f\n",
      "5. evaluate the \"reset_sample_state\" formula defined above.  It zeros out the saved_sample_output and saved_sample_state.  This is important because we want predictions from the beginning of the line (i.e. no previous state).\n",
      "The thing that is \"remembered\" in the model and doing the predictions are the parameters, (i.e. ix, im, ib, ox, om, ob, fx, fm, fb, cx, cm, cb, w, b), not the state or output -- the state and output are specific to the minibatch inputs that have preceeded.\n",
      "6. FOR EACH of the next 79 characters in the line, make a prediction as follows:\n",
      "   6a. Evaluate a \"prediction\" variable by \"eval\"ing the \"sample_prediction\" variable above. This triggers the evaluation of a single LSTM cell (not an unrolling of 10) which uses the trained parameters to output a prediction for the next character.  This output is turned into a logit with the final matmul with w and b and then a softmax is taken which is the prediction.\n",
      "Here is the prediction after the first character: [[  3.39121550e-01   2.51191966e-02   5.23944385e-04   4.66773519e-03\n",
      "    6.35061995e-04   1.15514666e-01   4.40838281e-03   3.65230808e-05\n",
      "    3.33182444e-03   1.79281473e-01   9.52615665e-05   6.40691083e-04\n",
      "    9.23793856e-03   6.42932253e-04   2.74513219e-03   2.10209712e-01\n",
      "    1.21816830e-03   1.60346404e-04   4.02478911e-02   7.20710540e-03\n",
      "    8.46218318e-03   4.01601084e-02   7.40140735e-04   1.33867154e-03\n",
      "    2.92785735e-05   4.10694303e-03   1.17162999e-04]]\n",
      "   6b. the sample() function takes that prediction, which is a vector of probabilities, and returns a sampled character such that high probability characters are more likely (but not guaranteed) to be chosen.  Here is what it chooses for the prediction above: [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  1.  0.  0.  0.  0.  0.]]\n",
      "Now convert that one-hot encoding to a char and append to the end of the sentence: fu\n",
      "\n",
      " Now here is all of that together:\n",
      "================================================================================\n",
      "fure pwoteus or honed to eathe drades in is a peceirnd the even tip forbonuy tomr\n",
      "h tutanled and zerochevathe soubl mids j henerect troggemsed one nine gigf the s\n",
      "d one tpransive sevouated rostitrs conded bee the  issong camaty crover seven ne\n",
      "reatiend proets or nd beges arlpeless in kssitic cousted notbfited coprcitities \n",
      "g two reyarlion ob remingred severt hists the clof cinor posard tree antic enete\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# DAY - small insertion to understand how the character sampling works\n",
    "\n",
    "# first, let's run the optimization of the model for just a short while:\n",
    "num_steps = 500\n",
    "update_frequency = num_steps/20\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "            # train_data is an array of tf.placeholders of len 'num_unrollings' +1\n",
    "            # so feed_dict now has entries with a tf.placeholder as key and a batch as value\n",
    "\n",
    "\n",
    "        _, l, predictions, lr = session.run(\n",
    "              [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "\n",
    "        if step % update_frequency == 0:\n",
    "            print('.', end='')\n",
    "            print('step %d:\\n    feed_dict=%s\\n    train_prediction=%s' % (step, feed_dict, train_prediction))\n",
    "\n",
    "        mean_loss += l\n",
    "# OK, now that the model is just a little trained, report out some stats\n",
    "    mean_loss = mean_loss / num_steps\n",
    "    print('\\nAverage loss over the 500 steps was %f' % mean_loss)\n",
    "    labels = np.concatenate(list(batches)[1:])\n",
    "    print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "\n",
    "    print('Perplexity of last minibatch (i.e.how confused was the model in choosing \\\n",
    "the next char -- ie roughly \\\n",
    "how many chars with equal probability did it have to chose from) was %.2f', float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "    \n",
    "# now do the sampling:\n",
    "    print('\\nSampling Explanation:')\n",
    "    print('=' * 80)\n",
    "    rd = random_distribution()\n",
    "    print('1. random_distribution yields vocabulary_size(27) random #s %s' % rd)\n",
    "    feed = sample(rd)\n",
    "    print('2. take a sample from that in a one-hot format, call it \"feed\": %s' % feed)\n",
    "    print('3. characters converts to a char.  character(feed)=%s' % characters(feed))\n",
    "    sentence = characters(feed)[0]\n",
    "    print('4. the beginning of your \"sentence\" (really a line) is this char: %s' % sentence)\n",
    "    reset_sample_state.run()\n",
    "    print('5. evaluate the \"reset_sample_state\" formula defined above.  It zeros out \\\n",
    "the saved_sample_output and saved_sample_state.  This is important because we want \\\n",
    "predictions from the beginning of the line (i.e. no previous state).\\n\\\n",
    "The thing that is \"remembered\" in the model and doing the predictions are the parameters, \\\n",
    "(i.e. ix, im, ib, ox, om, ob, fx, fm, fb, cx, cm, cb, w, b), not the state or output -- the \\\n",
    "state and output are specific to the minibatch inputs that have preceeded.')\n",
    "    print('6. FOR EACH of the next 79 characters in the line, make a prediction as follows:')\n",
    "    prediction = sample_prediction.eval({sample_input: feed})\n",
    "    print('   6a. Evaluate a \"prediction\" variable by \"eval\"ing the \"sample_prediction\" variable above. \\\n",
    "This triggers the evaluation of a single LSTM cell (not an unrolling of 10) which uses the trained \\\n",
    "parameters to output a prediction for the next character.  This output is turned into a logit with the \\\n",
    "final matmul with w and b and then a softmax is taken which is the prediction.\\n\\\n",
    "Here is the prediction after the first character: %s' % prediction)\n",
    "    feed = sample(prediction)\n",
    "    print('   6b. the sample() function takes that prediction, which is a vector of probabilities, and \\\n",
    "returns a sampled character such that high probability characters are more likely (but not guaranteed) \\\n",
    "to be chosen.  Here is what it chooses for the prediction above: %s' % feed)\n",
    "    sentence += characters(feed)[0]\n",
    "    print('Now convert that one-hot encoding to a char and append to the end of the sentence: %s' % sentence)\n",
    "    \n",
    "    print('\\n Now here is all of that together:')\n",
    "          \n",
    "    print('=' * 80)\n",
    "    for _ in range(5):  # sample 5 lines, each line will be (below) 79 chars\n",
    "        for _ in range(79):\n",
    "            # trigger evaluation of one lstm_cell and capture the softmax(logit) output as prediction\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            # convert prediction into a character\n",
    "            feed = sample(prediction)\n",
    "            # append the character onto our string\n",
    "            sentence += characters(feed)[0]\n",
    "        print(sentence)\n",
    "        # DAY, I moved these 3 lines to the bottom of the loop so I continue the string from above the loop\n",
    "        feed = sample(random_distribution()) # start with a random character\n",
    "        sentence = characters(feed)[0]\n",
    "        reset_sample_state.run()\n",
    "    print('=' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Answer 1\n",
    "========\n",
    "\n",
    "1. I added tensorboard hooks (they aren't quite useful yet\n",
    "2. Concatenate the 4 gates together using tf.concat on the 1th dimension (not the 0th)\n",
    "3. Do the matmul on the concatenated gates\n",
    "4. Split the result back out\n",
    "5. Apply either sigmoid or tanh activation function as appropriate.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters:\n",
    "    with tf.name_scope('LSTM_cell') as lstm_cell_scope:\n",
    "        with tf.name_scope('input_gate') as input_gate_scope:\n",
    "            # Input gate: input, previous output, and bias.\n",
    "            ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1)) # [27, 64]\n",
    "            im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))       # [64, 64]\n",
    "            ib = tf.Variable(tf.zeros([1, num_nodes]))                                     # [1, 64]\n",
    "        with tf.name_scope('forget_gate') as forget_gate_scope:\n",
    "            # Forget gate: input, previous output, and bias.\n",
    "            fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "            fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "            fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "        with tf.name_scope('memory_cell') as memory_cell_scope:\n",
    "            # Memory cell: input, state and bias.                             \n",
    "            cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "            cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "            cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "        with tf.name_scope('output_gate') as output_gate_scope:\n",
    "            # Output gate: input, previous output, and bias.\n",
    "            ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "            om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "            ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "            \n",
    "            \n",
    "        # Reduce the input/output matmuls from 4 to 1 each by concatenating the 4 gates\n",
    "        concatx = tf.concat(1, [ix, fx, cx, ox])\n",
    "        concatm = tf.concat(1, [im, fm, cm, om])\n",
    "        concatb = tf.concat(1, [ib, fb, cb, ob])\n",
    "        \n",
    "        \n",
    "        # Variables saving state across unrollings.\n",
    "        saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "        saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "        # Classifier weights and biases.\n",
    "        w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "        b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        with tf.name_scope(lstm_cell_scope):\n",
    "            # Instead of these 4 matmuls, do the one concatenated matmul and then split results\n",
    "            #input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "            #forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "            #update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "            #output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "\n",
    "            concatmatmul = tf.matmul(i, concatx) + tf.matmul(o, concatm) + concatb\n",
    "            input_gate, forget_gate, update, output_gate = tf.split(1, 4, concatmatmul)\n",
    "            input_gate = tf.sigmoid(input_gate)\n",
    "            forget_gate = tf.sigmoid(forget_gate)\n",
    "            output_gate = tf.sigmoid(output_gate)\n",
    "            state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "    # The LSTM\n",
    "    #\n",
    "    # In the code above, batch_size (bs) and num_nodes (nn) are both 64 (they don't have to be equal)\n",
    "    # 27 is the vocabulary size.\n",
    "    #\n",
    "    # input_gate(bs, nn)   = sigmoid( input(bs, 27) * ix(27, nn) + output(bs, nn) * im(nn, nn) + ib(1, nn) )\n",
    "    # forget_gate(bs, nn)  = sigmoid( input(bs, 27) * ix(27, nn) + output(bs, nn) * im(nn, nn) + ib(1, nn) )\n",
    "    # output_gate(bs, nn)  = sigmoid( input(bs, 27) * ix(27, nn) + output(bs, nn) * im(nn, nn) + ib(1, nn) )\n",
    "    # update(bs, nn)       = tanh(    input(bs, 27) * cx(27, nn) + output(bs, nn) * cm(nn, nn) + cb(1, nn) )\n",
    "    #\n",
    "    # output(bs, nn) = output_gate(bs, nn) * tanh( state(bs, nn) )\n",
    "    # state(bs, nn)  = forget_gate(bs, nn) * state(bs, nn)  +  input_gate(bs, nn) * update(bs, nn)\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            # These 11 elements of train_data will be pulled in from feed_dict\n",
    "            # Note that usually we have seen feed_dict specified as {var_name: value}\n",
    "            # but in this case, since these 11 array elements don't have a var_name, the \n",
    "            # feed_dict will use the tensorflow object as the key instead, i.e.\n",
    "            # feed_dict={<tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: value}\n",
    "            # See below where the feed_dict is prepared before calling session.run.\n",
    "            tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size])) #this will be pulled in from feed_dict\n",
    "    # train_data now has the shape (11, 64, 27), or (num_unrollings, batch_size, vocabulary_size)\n",
    "    # and sequential text from the original text input is 'striped' across the first dimension (11)\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step. (Brilliant!)\n",
    "\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    with tf.name_scope(lstm_cell_scope):\n",
    "        outputs = list()\n",
    "        output = saved_output\n",
    "        state = saved_state\n",
    "        for i in train_inputs: # since train_inputs is num_unrollings=10 long, this will create 10 LSTM cells\n",
    "            output, state = lstm_cell(i, output, state)\n",
    "            outputs.append(output)  # at each iter of the lstm_cell, append the character it predicted to outputs.\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    #\n",
    "    # DAY:  I am surprised that the output and state are saved across unrollings given that the\n",
    "    #       way the batches are done, the unrollings are not consecutive, so the state at the beginning\n",
    "    #       of a step should be empty rather than having the state from the last 10 characters of\n",
    "    #       the last step.\n",
    "    # ??\n",
    "    #\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits, tf.concat(0, train_labels)))\n",
    "        tf.scalar_summary('loss', loss)\n",
    "\n",
    "\n",
    "    # Optimizer.\n",
    "    #   Note that all 10 unrollings are done before the optimizer comes in and looks at the\n",
    "    #   output sequence of 10 chars vs. the label sequence of 10 chars and then calculates\n",
    "    #   the gradients and adjusts the parameters.  Then in the next step another 10 characters\n",
    "    #   will be predicted.\n",
    "    with tf.name_scope(\"Optimizer\"):\n",
    "        global_step = tf.Variable(0)\n",
    "        learning_rate = tf.train.exponential_decay(\n",
    "            10.0, global_step, 5000, 0.1, staircase=True)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        # DAY: this clipping below is the hack to prevent exploding gradients \n",
    "        #(LSTM was the elegant solution to prevent vanishing gradient)\n",
    "        gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "        optimizer = optimizer.apply_gradients(\n",
    "            zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    # (nothing here is triggered in the training)\n",
    "    #     first, variables\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    #     reset zeros out saved_sample_output and saved_sample_state\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    #     Define one lstm_cell with no unrolling (will be used for sampling from the trained model)\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    #     Define the next prediction (but make sure dependencies are calculated first)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.300922 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.14\n",
      "================================================================================\n",
      "v fcroqbl rbabhuq fminowif dck orohcmo jrh ehd xitgkdgfo eunettn tiomxmvcdt a ud\n",
      "xcsvn rcxgtk pqub planviarsx atcntiznnqxexldlspjkteskh btq opoohxfjg laqjo h ktx\n",
      "slntnas ducelrw o nsnqlyoxebrmc qnfdoexqe ef dxs eneyb kse prbhbxkibsb k ct injm\n",
      "fxobzayf atsileawrbtpjko at rabyoewzipqtjz  t  wqhazetwhw ena l wd eha effhasv  \n",
      "pl jpqcmrpnmsmlnnfuqrrtre iao nxdajjmohuynoe  ann cdkounpeoiiscclzpmehnb totrcll\n",
      "================================================================================\n",
      "Validation set perplexity: 20.34\n",
      "Average loss at step 100: 2.585738 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.89\n",
      "Validation set perplexity: 10.28\n",
      "Average loss at step 200: 2.243923 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.56\n",
      "Validation set perplexity: 8.58\n",
      "Average loss at step 300: 2.112426 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.43\n",
      "Validation set perplexity: 8.12\n",
      "Average loss at step 400: 2.010247 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.73\n",
      "Validation set perplexity: 7.84\n",
      "Average loss at step 500: 1.944787 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.41\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 600: 1.915115 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 700: 1.865932 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.49\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 800: 1.830087 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 6.34\n",
      "Average loss at step 900: 1.839241 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.07\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 1000: 1.834511 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "================================================================================\n",
      "jiam be arek aco of alders for hawn i sumeces pressodogination been work in creg\n",
      " firming the anirate chooraht was reaish in astra bre blicy nee were user syving\n",
      "uerism of fayer ese bich ain burin verasely capput fir neom in cleish anopulalio\n",
      "qule with entcear cimes age in bestraclise schill yan en maridaal axcolvrity mon\n",
      "pute messul in beans remiting alfowata ampeakival prefures of action one cofffe \n",
      "================================================================================\n",
      "Validation set perplexity: 6.00\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    merged = tf.merge_all_summaries()\n",
    "    #writer = tf.train.SummaryWriter('logs', graph=session.graph) #use relative dirname\n",
    "    writer = tf.train.SummaryWriter('logs') #use relative dirname\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr, merged_output = session.run(\n",
    "              [optimizer, loss, train_prediction, learning_rate, merged], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            writer.add_summary(merged_output)\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "# Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Answer 2\n",
    "========\n",
    "\n",
    "Haven't done anything yet but copy these code blocks (and concatenated a couple)\n",
    "\n",
    "The plan:\n",
    "1. Get it working using encodings for the single character case and compare to the previous\n",
    "2. Modify to use bigrams\n",
    "\n",
    "For #2 above, I could do the following options (assume alphabet input)\n",
    "* input1=ab, targeted_output1=c  ; input2=bc, targeted_output2=d \n",
    "* input1=ab, targeted_output1=c  ; input2=cd, targeted_output2=e \n",
    "* input1=ab, targeted_output1=cd ; input2=cd, targeted_output2=ef\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-ba5d1bef861c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Data size %d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-ba5d1bef861c>\u001b[0m in \u001b[0;36mread_data\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnamelist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/zipfile.pyc\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, name, pwd)\u001b[0m\n\u001b[0;32m    933\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m         \u001b[1;34m\"\"\"Return file bytes (as a string) for name.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 935\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    936\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    937\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/zipfile.pyc\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    628\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 630\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    631\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/zipfile.pyc\u001b[0m in \u001b[0;36mread1\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    682\u001b[0m             data = self._decompressor.decompress(\n\u001b[0;32m    683\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unconsumed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 684\u001b[1;33m                 \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen_readbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMIN_READ_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    685\u001b[0m             )\n\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "          'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)\n",
    "\n",
    "def read_data(filename):\n",
    "    f = zipfile.ZipFile(filename)\n",
    "    for name in f.namelist():\n",
    "        return tf.compat.as_str(f.read(name))\n",
    "    f.close()\n",
    "\n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))\n",
    "\n",
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print('training data size: %s %s' % (train_size, train_text[:64]))\n",
    "print('validation data size: %s %s' % (valid_size, valid_text[:64]))\n",
    "\n",
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "    \n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('Ã¯'))\n",
    "print(id2char(1), id2char(26), id2char(0))\n",
    "print('vocabulary size: %s' % vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "embedding_size = 128  # size of the embeddings we will use for input\n",
    "\n",
    "\n",
    "day_debug = False\n",
    "\n",
    "# DAY\n",
    "#\n",
    "# This is important to understand.  Our NN needs a constant sized vector with each input.  We are\n",
    "# providing that here.  As the video says, just as convolution lets us use the same weight parameters\n",
    "# at different parts of the image, a recurrent neural net lets us use the same weights at different\n",
    "# points in time (or rather, different points in the input sequence).\n",
    "#\n",
    "# The notion of \"unrollings\" is that a recurrent NN has it's output connected to it's input, but really\n",
    "# the way to think about it is over time where the output of time t-1 is input to time t.  That way\n",
    "# of looking at it is like \"unrolling\" the recurrent NN over time so it is understood more as a\n",
    "# sequence of copies of the NN.  \n",
    "# In this case, we are going to be feeding in sequences that are 10 long, so we will in effect\n",
    "# create 10 LSTM cells (which are really just a NN) and hook the output of LSTM cell t with inputs\n",
    "# from input_sub_t and also the output of LSTM cell t-1.\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "        if day_debug:\n",
    "            print('BatchGenerator init: \\n  textsize: %s\\n  batch_size: %s\\n  num_unrollings: %s' % \n",
    "                  (len(text), batch_size, num_unrollings))\n",
    "            print(\"BatchGenerator init -- text starts with: %s\" % text[:128])\n",
    "            print(\"BatchGenerator init -- segment=%s\" % segment)\n",
    "            print(\"BatchGenerator init -- _cursor=%s\" % self._cursor)\n",
    "  \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\n",
    "        It will be in the form of an embedding)\"\"\"\n",
    "        # DAY\n",
    "        # this is quite confusing, but batch ends up being  of dimension\n",
    "        # (batch_size, vocabulary_size) -- so it is 1 char (one-hot encoded) from\n",
    "        # every _cursor location (there are batch_size cursor locations.  \n",
    "        # Since the cursor locations are not consecutive, these chars in batch\n",
    "        # are not consecutive.  However, in the next step at next(), the \n",
    "        # char will be appended with the next consecutive char.\n",
    "        # Uncomment the print line below to see this.\n",
    "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        #print(batches2string([batch]))\n",
    "        return batch\n",
    "  \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    # DAY\n",
    "    # This mangles the real batch structure in the interest of readability, but\n",
    "    # by doing so, makes your understanding of batches wrong.\n",
    "    # See my 'honest_batches2string' below which gives you a better\n",
    "    # understanding of the batch format.\n",
    "    \n",
    "    # batches has dimensions (num_unrollings, batch_size, vocabulary_size)\n",
    "    s = [''] * batches[0].shape[0]  # batches[0].shape[0] will end up being same as batch_size\n",
    "    for b in batches: # there will be num_unrollings of these...\n",
    "        # each b (batch) is 64 by 27\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "def honest_batches2string(batches):\n",
    "    import pprint\n",
    "    output = []\n",
    "    for b_index, b in enumerate(batches):  # there will be 'num_unrollings' of these\n",
    "        output.append(list())\n",
    "        for one_hot_index, one_hot in enumerate(b):  # there will be 'batch_size' of these\n",
    "            output[b_index].append(characters([one_hot]))\n",
    "    return pprint.pformat(output)\n",
    "            \n",
    "        \n",
    "\n",
    "if day_debug:\n",
    "    # DAY\n",
    "    # Notice how this output is structured. Subsequent batches are related to each other in that\n",
    "    # you can continue reading the text from batch1[0] to batch2[0]  (similarly from\n",
    "    # batch1[20] to batch2[20])\n",
    "    my_text = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "    my_batches = BatchGenerator(my_text.lower(), 4, 2)\n",
    "    if False:\n",
    "        print(my_batches.next())\n",
    "        print(my_batches.next())\n",
    "        print(my_batches.next())\n",
    "        print(my_batches.next())\n",
    "        print(my_batches.next())\n",
    "        print(my_batches.next())\n",
    "    else:\n",
    "        print(honest_batches2string(my_batches.next()))\n",
    "        print(honest_batches2string(my_batches.next()))\n",
    "        print(honest_batches2string(my_batches.next()))\n",
    "        print(honest_batches2string(my_batches.next()))\n",
    "        print(honest_batches2string(my_batches.next()))\n",
    "        print(honest_batches2string(my_batches.next()))\n",
    "\n",
    "    my_text = \"Four score and seven years ago our fathers brought forth on this continent, a new \\\n",
    "    nation, conceived in Liberty, and dedicated to the proposition that all men are created equal. Now \\\n",
    "    we are engaged in a great civil war, testing whether that nation, or any nation so conceived and \\\n",
    "    so dedicated, can long endure. We are met on a great battle-field of that war. We have come to \\\n",
    "    dedicate a portion of that field, as a final resting place for those who here gave their lives that \\\n",
    "    that nation might live. It is altogether fitting and proper that we should do this.\\\n",
    "    But, in a larger sense, we can not dedicate -- we can not consecrate -- we can not hallow -- \\\n",
    "    this ground. The brave men, living and dead, who struggled here, have consecrated it, far above \\\n",
    "    our poor power to add or detract. The world will little note, nor long remember what we say here, \\\n",
    "    but it can never forget what they did here. It is for us the living, rather, to be dedicated here \\\n",
    "    to the unfinished work which they who fought here have thus far so nobly advanced. It is rather for \\\n",
    "    us to be here dedicated to the great task remaining before us -- that from these honored dead we \\\n",
    "    take increased devotion to that cause for which they gave the last full measure of devotion -- that \\\n",
    "    we here highly resolve that these dead shall not have died in vain -- that this nation, under God, \\\n",
    "    shall have a new birth of freedom -- and that government of the people, by the people, for the people, \\\n",
    "    shall not perish from the earth.\"\n",
    "    my_batches = BatchGenerator(my_text.lower(), 10, 5)\n",
    "    print(honest_batches2string(my_batches.next()))\n",
    "    print(honest_batches2string(my_batches.next()))\n",
    "    print(honest_batches2string(my_batches.next()))\n",
    "    print(honest_batches2string(my_batches.next()))\n",
    "    print(honest_batches2string(my_batches.next()))\n",
    "    print(honest_batches2string(my_batches.next()))\n",
    "    # Also, here is how a batch *really* looks (an batch_size array of one-hot encodings)\n",
    "    temp = my_batches.next()\n",
    "    print('Dimensions of the batch (num_unrollings, batch_size, vocab_size): (%d, %d, %d)' % \n",
    "          (len(temp), len(temp[0]), len(temp[0][0])))\n",
    "    print('='*80)\n",
    "\n",
    "\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print('WARNING: the following printed lines are not indicative of the structure that goes into the model')\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0  # prediction is in column format, so it must be indexed by [0]\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model.\n",
    "\n",
    "_*DAY: Note that I have modified this to comment/understand*_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedding_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-d69afe47bac9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# the second dimension will hold the embedding vector.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     vocabulary_embeddings = tf.Variable(\n\u001b[1;32m---> 10\u001b[1;33m         tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'embedding_size' is not defined"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # My embedding here will simply be a 2D tensor -- the first\n",
    "    # dimension will hold the one-hot encoding for a character (the index)\n",
    "    # the second dimension will hold the embedding vector.\n",
    "    vocabulary_embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "\n",
    "  \n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1)) # [27, 64]\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))       # [64, 64]\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))                                     # [1, 64]\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False) #notice the 'trainable=False'\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False) #notice the 'trainable=False'\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))  #DAY -  embedding_size?\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))  #DAY -  embedding_size?\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "    # The LSTM\n",
    "    #\n",
    "    # In the code above, batch_size (bs) and num_nodes (nn) are both 64 (they don't have to be equal)\n",
    "    # 27 is the vocabulary size.\n",
    "    #\n",
    "    # input_gate(bs, nn)   = sigmoid( input(bs, 27) * ix(27, nn) + output(bs, nn) * im(nn, nn) + ib(1, nn) )\n",
    "    # forget_gate(bs, nn)  = sigmoid( input(bs, 27) * ix(27, nn) + output(bs, nn) * im(nn, nn) + ib(1, nn) )\n",
    "    # output_gate(bs, nn)  = sigmoid( input(bs, 27) * ix(27, nn) + output(bs, nn) * im(nn, nn) + ib(1, nn) )\n",
    "    # update(bs, nn)       = tanh(    input(bs, 27) * cx(27, nn) + output(bs, nn) * cm(nn, nn) + cb(1, nn) )\n",
    "    #\n",
    "    # output(bs, nn) = output_gate(bs, nn) * tanh( state(bs, nn) )\n",
    "    # state(bs, nn)  = forget_gate(bs, nn) * state(bs, nn)  +  input_gate(bs, nn) * update(bs, nn)\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            # These 11 elements of train_data will be pulled in from feed_dict\n",
    "            # Note that usually we have seen feed_dict specified as {var_name: value}\n",
    "            # but in this case, since these 11 array elements don't have a var_name, the \n",
    "            # feed_dict will use the tensorflow object as the key instead, i.e.\n",
    "            # feed_dict={<tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: value}\n",
    "            # See below where the feed_dict is prepared before calling session.run.\n",
    "            tf.placeholder(tf.float32, shape=[batch_size,embedding_size])) #this will be pulled in from feed_dict\n",
    "    # train_data now has the shape (11, 64, 27), or (num_unrollings, batch_size, vocabulary_size)\n",
    "    # and sequential text from the original text input is 'striped' across the first dimension (11)\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step. (Brilliant!)\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs: # since train_inputs is num_unrollings=10 long, this will create 10 LSTM cells\n",
    "        #\n",
    "        # the input i is not a single character or even a single one-hot -- it is \n",
    "        # 'batch_size' rows, each of which is a one-hot.  To get the character index from\n",
    "        # the one-hot, use tf.argmax (since the only 1 in the one-hot will be the max)\n",
    "        iembed = tf.nn.embedding_lookup(vocabulary_embeddings, tf.argmax(i, dimension=1))\n",
    "        print('iembed = %s' % iembed)\n",
    "        output, state = lstm_cell(iembed, output, state)\n",
    "        outputs.append(output) # at each iteration of the lstm_cell, append the character it predicted to outputs.\n",
    "\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        \n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits, tf.concat(0, train_labels)))\n",
    "\n",
    "    # Optimizer.\n",
    "    #   Note that all 10 unrollings are done before the optimizer comes in and looks at the\n",
    "    #   output sequence of 10 chars vs. the label sequence of 10 chars and then calculates\n",
    "    #   the gradients and adjusts the parameters.  Then in the next step another 10 characters\n",
    "    #   will be predicted.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    # DAY: this clipping below is the hack to prevent exploding gradients \n",
    "    #(LSTM was the elegant solution to prevent vanishing gradient)\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    # (nothing here is triggered in the training)\n",
    "    #     first, variables\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    #     reset zeros out saved_sample_output and saved_sample_state\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    #     Define one lstm_cell with no unrolling (will be used for sampling from the trained model)\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    #     Define the next prediction (but make sure dependencies are calculated first)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I've modified this with comments to understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.291867 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.89\n",
      "================================================================================\n",
      "k isfoa neneqwh oixuppe aiaedd pahovpu dlwpxv efpfaanmf hlos h ras etsc  icdmew \n",
      "ead pkil xemseie uzjgta qacenundwcajqivogo hpto tmrwcwali asqobzlzmixszzfk iytgf\n",
      "tsfzshefiqxr hzisf  ox yiitohdpsigakfbgu kvsfs   itilvrlzesef  avsxw n mwsoqbsiz\n",
      "ehz es  makdyva nari fal xdq  itimc f kckivwxef itveghylw fipirkfnpujsfhhrgez i \n",
      "a  rkmqe oaidah vocleoj tdrtij  olmuyihp pwtyyiqcizveh mpefm ocoqoxjxcwt  ektbez\n",
      "================================================================================\n",
      "Validation set perplexity: 20.11\n",
      "Average loss at step 100: 2.588131 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.88\n",
      "Validation set perplexity: 10.33\n",
      "Average loss at step 200: 2.241012 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.44\n",
      "Validation set perplexity: 8.58\n",
      "Average loss at step 300: 2.105546 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.42\n",
      "Validation set perplexity: 8.12\n",
      "Average loss at step 400: 2.007310 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.50\n",
      "Validation set perplexity: 7.95\n",
      "Average loss at step 500: 1.943708 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 600: 1.911201 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 700: 1.864903 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.69\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 800: 1.827685 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 900: 1.835392 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.00\n",
      "Validation set perplexity: 6.26\n",
      "Average loss at step 1000: 1.823352 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "================================================================================\n",
      "war gome the di cacilitial aftlec vinester ened bret leyited mose latle part mar\n",
      "ble octs of still as the selogural gay and the used feciliseive ilso sive creial\n",
      "ul sy reckeatube spepricin is engrosudebates rack and tasewarne decried hecadedi\n",
      "rode con jullated cynnced and issend lomstoo resome comnosions lopt for is a whe\n",
      "peny first one nine four six comming one five adgince beagrian from cosoble in t\n",
      "================================================================================\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 1100: 1.779255 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 5.72\n",
      "Average loss at step 1200: 1.757870 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 5.53\n",
      "Average loss at step 1300: 1.736327 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 1400: 1.750241 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 1500: 1.742174 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 1600: 1.746421 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 1700: 1.712752 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 1800: 1.672432 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 1900: 1.647423 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 2000: 1.698284 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "================================================================================\n",
      "ciel fundary in progessia and of the formourd inchyugion insingt k sly lomblims \n",
      "d are a bling beoving and basting ont woodor unit the undicing chur two zero ski\n",
      "hy orcing websy agristigny beris two five it a chyster is long possetsly uns the\n",
      "konnof wh ba became fich u procting been one nine nine six nine nt initary in ma\n",
      "vil lebation scops one zero zero zeot amerigion in rock bove vizing of the grale\n",
      "================================================================================\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 2100: 1.684531 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 2200: 1.679741 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 2300: 1.640706 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 2400: 1.658660 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 2500: 1.680739 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 2600: 1.655895 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 2700: 1.660066 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 2800: 1.647518 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 2900: 1.651678 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 3000: 1.653123 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "================================================================================\n",
      "with required declarmitiol deavting also italg alvoghe smatin instanfine griting\n",
      "d the for were seven cllect invisals there with under the dial doricing the natu\n",
      "ung or and was standred will enten idenfrals examply that hyslow worlogion and a\n",
      "taphramder bekan with one nine seven nine six stan formandy apols sosmaty is del\n",
      "chanty sopernnny many s one nine eight zero firs a sp perical dianginm lamburage\n",
      "================================================================================\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3100: 1.631111 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 3200: 1.649302 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 3300: 1.639005 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 3400: 1.669835 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 3500: 1.657384 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 3600: 1.667576 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 3700: 1.648196 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3800: 1.645966 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3900: 1.637218 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 4000: 1.652614 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.67\n",
      "================================================================================\n",
      "x is ham b pnormed byozs renice the crace mota both celtemed cpilleh preses whic\n",
      "y serich presiles geneaz computeristicists mouth peau trofed its there of the th\n",
      "ared to is time her churtommch is techilical a historiag do seemals as relonuati\n",
      "der spirt is a digremica year the people the us office time a paine crusicidok t\n",
      "loot offed seor year as as brodini be accedending usto in one beagued energiey f\n",
      "================================================================================\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4100: 1.635629 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 4200: 1.633627 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 4300: 1.609697 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 4400: 1.609052 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 4500: 1.614958 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 4600: 1.615949 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 4700: 1.623572 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 4800: 1.627218 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 4900: 1.634664 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 5000: 1.604656 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.38\n",
      "================================================================================\n",
      " musicine sindeh early or dasibs as schish and unary self but his wirk s hume at\n",
      "x had by the article traopony and univeles military lass a time the his one a fo\n",
      "lations election family i durocial pohnings on rvalle to and un he eninve and fi\n",
      "pote belivied declari songrance a liberie of shi port its reducion coin until mo\n",
      "crow for ewary fored sea b lack wirea s as and entory of a fainets of the bases \n",
      "================================================================================\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5100: 1.604493 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 5200: 1.589260 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 5300: 1.578143 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 5400: 1.579524 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 5500: 1.564326 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 5600: 1.582960 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 5700: 1.563167 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 5800: 1.579737 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 5900: 1.574091 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 6000: 1.545829 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "================================================================================\n",
      "s other ged aractibely which fast aspapting popueld the sib militarian with as a\n",
      "mound s l haw outsbanns of gene maknection history in later tedaink was that con\n",
      "bere featies the foipe out became can history on strerch and law one nine nine z\n",
      "geneted bankong physic finoderi links is one nine eight a screez north to some y\n",
      "ter microwable kamborbule staun in one nine one five ghist promines exthols from\n",
      "================================================================================\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 6100: 1.557376 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 6200: 1.531648 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 6300: 1.542773 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 6400: 1.537078 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 6500: 1.554085 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 6600: 1.592105 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 6700: 1.574984 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 6800: 1.599745 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 6900: 1.580704 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 7000: 1.575416 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "================================================================================\n",
      "vers revery samesia cari the literalluilors as the external procomiction of pris\n",
      " the pointicly feel joborals seegotibeling from that pendacs looking the games f\n",
      "quatively that symb culart to period became in unorming imic child an the brawzk\n",
      "s and learn of doculion a dust after these and sunched by mainly johell by syeqo\n",
      "chiter alliprations one of is secolific natery that banks of the for he bojors d\n",
      "================================================================================\n",
      "Validation set perplexity: 4.35\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next() #\n",
    "        # batches is of dimension (num_unrollings, batch_size, vocabulary_size) (11, 64, 27)\n",
    "        # where sequential text from the input is \"striped\" across the unrollings.  For \n",
    "        # example, if 'char1' stands for the first character in the original text (one-hot encoded), \n",
    "        # batches looks like this (assuming 'segment' is 15000):\n",
    "        # [                                        # there are 'num_unrollings' rows\n",
    "        #   [char1,  char15000,  char 30000, ...], # each row is 'batch_size'\n",
    "        #   [char2,  char15001,  char 30001, ...],\n",
    "        #   ...\n",
    "        #   [char11, char15010,  char 30010, ...]\n",
    "        # ]\n",
    "        # when we call train_batches.next(), the next 'batches' will look like this:\n",
    "        # [                                        # there are 'num_unrollings' rows\n",
    "        #   [char11, char15010,  char 30010, ...], # each row is 'batch_size'\n",
    "        #   [char12, char15011,  char 30011, ...],\n",
    "        #   ...\n",
    "        #   [char21, char15020,  char 30020, ...]\n",
    "        # ]\n",
    "        # it might look like a bug that the second 'batches' repeats char11, char1510, etc.\n",
    "        # but it is not a bug.  in the first 'batches', char11 was included only as the label\n",
    "        # needed for the 10th entry (char10). (LSTM takes char10 in as an input and expects char11\n",
    "        # as the true label of the output).  So -- char11 was never put into the LSTM_cell in the\n",
    "        # first 'batches' -- it is only used as a label, so it need to be included as the first\n",
    "        # item in the second 'batches' so that in can now be an input into an LSTM cell.\n",
    "        #\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "            # Normally we see feed_dict={var_name: value}, but here we don't have the var_names\n",
    "            # for the training data batches in the graph definition (it is an array of tensors)\n",
    "            # so instead, we use the tensorflow object itself (from the graph definition, in\n",
    "            # train_data[i]) as the key in the feed_dict entries.\n",
    "            \n",
    "        _, l, predictions, lr = session.run(\n",
    "              [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())  # start with a random character\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Answer 3\n",
    "========\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
