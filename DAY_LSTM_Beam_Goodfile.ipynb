{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "# LSTM Model with Beam Search\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n",
      "Data size 100000000\n",
      "training data size: 99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "validation data size: 1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "          'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)\n",
    "\n",
    "def read_data(filename):\n",
    "    f = zipfile.ZipFile(filename)\n",
    "    for name in f.namelist():\n",
    "        return tf.compat.as_str(f.read(name))\n",
    "    f.close()\n",
    "\n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))\n",
    "\n",
    "# Create small validation set\n",
    "\n",
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print('training data size: %s %s' % (train_size, train_text[:64]))\n",
    "print('validation data size: %s %s' % (valid_size, valid_text[:64]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['if', 'is', 'be', 'is'], ['it', 'to', 'it', 'up'], ['is', 'be', 'is', 'to']]\n",
      "[['is', 'be', 'is', 'to'], ['to', 'it', 'up', 'me'], ['be', 'is', 'to', 'if']]\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "# DAY\n",
    "#\n",
    "# This is important to understand.  Our NN needs a constant sized vector with each input.  We are\n",
    "# providing that here.  As the video says, just as convolution lets us use the same weight parameters\n",
    "# at different parts of the image, a recurrent neural net lets us use the same weights at different\n",
    "# points in time (or rather, different points in the input sequence).\n",
    "#\n",
    "# The notion of \"unrollings\" is that a recurrent NN has it's output connected to it's input, but really\n",
    "# the way to think about it is over time where the output of time t-1 is input to time t.  That way\n",
    "# of looking at it is like \"unrolling\" the recurrent NN over time so it is understood more as a\n",
    "# sequence of copies of the NN.  \n",
    "# In this case, we are going to be feeding in sequences that are 10 long, so we will in effect\n",
    "# create 10 LSTM cells (which are really just a NN) and hook the output of LSTM cell t with inputs\n",
    "# from input_sub_t and also the output of LSTM cell t-1.\n",
    "\n",
    "# I'm re-writing the BatchGenerator to be more general purpose.  I want it to always output\n",
    "# IDs (not embeddings, not the actual text, but the ID of the embeddings)\n",
    "\n",
    "class SequenceGenerator(object):\n",
    "    '''This class will take a text input and generate batches of IDs suitable for use with\n",
    "    tf.nn.embedding_lookup() (i.e. goes from 0 to len(vocab)-1).  This class can be \n",
    "    inherited to create classes that create IDs for single characters, bigrams, \n",
    "    or entire words.  It also has the ability to take\n",
    "    ID output from the RNN and convert it back to the original text.'''\n",
    "    def __init__(self, text, batch_size, num_unrollings, vocab_size_limit=None):\n",
    "        self._text = text\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        self._vocab_size_limit = vocab_size_limit\n",
    "        self._init_vocab()\n",
    "        self._init_token_sequence()\n",
    "        self._text = None # garbage collect now that self._token_seq is written\n",
    "        segment = self._token_seq_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "        \n",
    "    def _init_vocab(self):\n",
    "        '''Must be implemented in subclasses and create the following:\n",
    "                self._vocab_dict (text keys and ID values)  \n",
    "                self._reverse_vocab_dict (ID keys and text values)\n",
    "                self.vocab_size\n",
    "            Consider also what you will do with self._vocab_size_limit'''\n",
    "        raise NotImplementedError\n",
    "    def _init_token_sequence(self):\n",
    "        '''Must be implemented in subclasses and create the following:\n",
    "                self._token_seq = list of token id's in original input order (so duplicate is ok)\n",
    "                self._token_seq_size = the total # of tokens in the input stream\n",
    "            Consider also what you will do with self._vocab_size_limit'''\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def token_2_id(self, token):\n",
    "        return self._vocab_dict[token]\n",
    "    def id_2_token(self, token_id):\n",
    "        return self._reverse_vocab_dict[token_id]\n",
    "    def onehot_2_id(self, one_hot):\n",
    "        \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "        characters back into its (most likely) ID representation.\n",
    "        This will always return the same result for identical inputs -- it does\n",
    "        not sample across the probability distribution.\n",
    "        This used to be character()\"\"\"\n",
    "        return [c for c in np.argmax(one_hot, 1)]\n",
    "    def id_2_onehot(self, id_list):\n",
    "        '''Turn a list of ids into a list of one_hot encoded vectors'''\n",
    "        identity = tf.constant(np.identity(self.vocab_size, dtype = np.float32))\n",
    "        return tf.nn.embedding_lookup(identity, id_list)\n",
    "    \n",
    "    def softmax_2_sampled_id(self, softmax_distribution):\n",
    "        \"\"\"Turn a softMax probability distribution over the possible\n",
    "        characters into an ID representation based on a sampling over that probability\n",
    "        distribution.\n",
    "        This randomly samples the distribution. So, for example, if in the\n",
    "        softmax distribution 'a' is 40% likely, 'b' is 40% likely and \n",
    "        'c' is 20% likely, this will generate an 'a' 40% of the time\n",
    "        it is called and a 'c' 20% of the time it is called, etc.\"\"\"\n",
    "        r = random.uniform(0, 1)\n",
    "        s = 0\n",
    "        for i in range(len(softmax_distribution)):\n",
    "            s += softmax_distribution[i]\n",
    "            if s >= r:\n",
    "                return i\n",
    "        return len(softmax_distribution) - 1\n",
    "    \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single row (or unrolling) of length 'batch' from the current \n",
    "        cursor position in the token data.  It will be in the form of a row of token IDs.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, ), dtype=np.int32)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b] = self._token_seq[self._cursor[b]]\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._token_seq_size\n",
    "        return batch\n",
    "  \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        (for a total of num_unrollings + 1).\n",
    "        The reason the last batch from the previous array is included is because\n",
    "        in the previous array, the last batch was just used as a label to the model,\n",
    "        not as an input -- so we include it this next time to be used as an input.\n",
    "        Note that the sequential tokens end up being read into the columns of these\n",
    "        'num_unrolling\" batches.  Each column is a separate part of the token input.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "    \n",
    "    def batches_2_tokens(self, batches):\n",
    "        \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "        representation.\"\"\"\n",
    "        # DAY\n",
    "        # This mangles the real batch structure in the interest of readability, but\n",
    "        # by doing so, makes your understanding of batches wrong.\n",
    "        # See 'honest_batches2string' below which gives you a better\n",
    "        # understanding of the batch format.\n",
    "\n",
    "        # batches has dimensions (num_unrollings, batch_size)\n",
    "        s = [''] * batches[0].shape[0]  # batches[0].shape[0] will end up being same as batch_size\n",
    "        for b in batches: # there will be num_unrollings of these...\n",
    "            s = [''.join(self.id_2_token(x)) for x in zip(s, b)]  # DAY __ NEEDS WORK\n",
    "        return s\n",
    "\n",
    "    def honest_batches_2_tokens(self, batches):\n",
    "        import pprint\n",
    "        output = []\n",
    "        for b_index, b in enumerate(batches):  # there will be 'num_unrollings' of these\n",
    "            output.append(list())\n",
    "            for token_id_index, token_id in enumerate(b):  # there will be 'batch_size' of these\n",
    "                output[b_index].append(self.id_2_token(token_id))\n",
    "        return pprint.pformat(output)\n",
    "\n",
    "class SingleCharacterGenerator(SequenceGenerator):\n",
    "    def _init_vocab(self):\n",
    "        '''Must be implemented in subclasses and create the following:\n",
    "                self._reverse_vocab_dict (ID keys and text values)\n",
    "                self.vocab_size\n",
    "            Consider also what you will do with self._vocab_size_limit'''\n",
    "        self.vocab_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "        self._vocab_dict = dict()\n",
    "        self._vocab_dict[' '] = 0\n",
    "        for i, v in enumerate(list(string.ascii_lowercase[:self.vocab_size - 1])):\n",
    "            self._vocab_dict[v] = i+1\n",
    "        self._reverse_vocab_dict = dict(zip(self._vocab_dict.values(), self._vocab_dict.keys()))\n",
    "    def _init_token_sequence(self):\n",
    "        '''Must be implemented in subclasses and create the following:\n",
    "                self._token_seq = list of token id's in original input order (so duplicate is ok)\n",
    "                self._token_seq_size = the total # of tokens in the input stream\n",
    "            Consider also what you will do with self._vocab_size_limit'''\n",
    "        first_letter = ord(string.ascii_lowercase[0])\n",
    "        self._token_seq = list()\n",
    "        for char in self._text.lower():\n",
    "            if char in string.ascii_lowercase:\n",
    "                self._token_seq.append(self.token_2_id(char))\n",
    "            elif char == ' ':\n",
    "                self._token_seq.append(self.token_2_id(' '))\n",
    "            else:\n",
    "                pass # don't enter unknown characters (DAY should we have an UNK ID?)\n",
    "        self._token_seq_size = len(self._token_seq)\n",
    "\n",
    "class BigramGenerator(SequenceGenerator):\n",
    "    def _init_vocab(self):\n",
    "        '''Must be implemented in subclasses and create the following:\n",
    "                self._vocab_dict (text keys and ID values)  \n",
    "                self._reverse_vocab_dict (ID keys and text values)\n",
    "                self.vocab_size\n",
    "            Consider also what you will do with self._vocab_size_limit'''\n",
    "        self._vocab_dict = dict()\n",
    "        self._token_seq = list()\n",
    "        id_index = 0\n",
    "        for i in range(0, len(self._text)-2, 2):\n",
    "            bigram = self._text[i].lower() + self._text[i+1].lower()\n",
    "            if bigram not in self._vocab_dict:\n",
    "                self._vocab_dict[bigram] = id_index\n",
    "                id_index += 1\n",
    "            self._token_seq.append(self.token_2_id(bigram)) # dup ok -- this is just input stream as token ids\n",
    "        self.vocab_size = len(self._vocab_dict)\n",
    "        self._reverse_vocab_dict = dict(zip(self._vocab_dict.values(), self._vocab_dict.keys()))\n",
    "        self._token_seq_size = len(self._token_seq)\n",
    "    def _init_token_sequence(self):\n",
    "        '''Must be implemented in subclasses and create the following:\n",
    "                self._token_seq = list of token id's in original input order (so duplicate is ok)\n",
    "                self._token_seq_size = the total # of tokens in the input stream\n",
    "            Consider also what you will do with self._vocab_size_limit'''\n",
    "        pass # I did this work in _init_vocab() to use just one loop\n",
    "    \n",
    "class WordGenerator(SequenceGenerator):\n",
    "    def _init_vocab(self):\n",
    "        '''Must be implemented in subclasses and create the following:\n",
    "                self._vocab_dict (text keys and ID values)  \n",
    "                self._reverse_vocab_dict (ID keys and text values)\n",
    "                self.vocab_size\n",
    "            Consider also what you will do with self._vocab_size_limit'''\n",
    "        words = self._text.lower().split()\n",
    "        count = [['UNK', -1]]\n",
    "        if self._vocab_size_limit is not None:\n",
    "            count.extend(collections.Counter(words).most_common(self._vocab_size_limit - 1))\n",
    "        else:\n",
    "            count.extend(collections.Counter(words).most_common())\n",
    "        self._vocab_dict = dict()\n",
    "        id_index = 1\n",
    "        for word, _ in count:\n",
    "            self._vocab_dict[word] = id_index\n",
    "            id_index += 1\n",
    "        self._vocab_dict['UNK'] = 0 # force this id value for convenience\n",
    "        self._token_seq = list()\n",
    "        unk_count = 0\n",
    "        for word in words:\n",
    "            if word in self._vocab_dict:\n",
    "                index = self._vocab_dict[word]\n",
    "            else:\n",
    "                index = 0  # self._vocab_dict['UNK']\n",
    "                unk_count = unk_count + 1\n",
    "            self._token_seq.append(index)\n",
    "        count[0][1] = unk_count\n",
    "        self.vocab_size = len(self._vocab_dict)\n",
    "        self._reverse_vocab_dict = dict(zip(self._vocab_dict.values(), self._vocab_dict.keys()))\n",
    "        self._token_seq_size = len(self._token_seq)\n",
    "        #print('count[:1000]: %s' % count[:1000])\n",
    "        del words\n",
    "        del count\n",
    "        # if called with build_dataset('if it is to be it is up to me'.split())\n",
    "        # we get the following:\n",
    "        #>>> count               # count of the '_vocab_size_limit' most common words\n",
    "        #[['UNK', 3], ('is', 2), ('it', 2), ('to', 2), ('me', 1)]\n",
    "        #>>> self._token_seq                # the original text as indices into 'count'\n",
    "        #[0, 2, 1, 3, 0, 2, 1, 0, 3, 4]\n",
    "        #>>> self._vocab_dict          # value refers to index in count\n",
    "        #{'me': 4, 'to': 3, 'is': 1, 'UNK': 0, 'it': 2}\n",
    "        #>>> self._reverse_vocab_dict\n",
    "        #{0: 'UNK', 1: 'is', 2: 'it', 3: 'to', 4: 'me'}\n",
    "\n",
    "        # OLD method that did not have vocab_limit\n",
    "        #self._vocab_dict = dict()\n",
    "        #self._token_seq = list()\n",
    "        #for i, word in enumerate(self._text.lower().split()):\n",
    "        #    if word not in self._vocab_dict:\n",
    "        #        self._vocab_dict[word] = i\n",
    "        #    self._token_seq.append(self.token_2_id(word)) # dup ok -- this is just input stream as token ids\n",
    "        #self.vocab_size = len(self._vocab_dict)\n",
    "        #self._reverse_vocab_dict = dict(zip(self._vocab_dict.values(), self._vocab_dict.keys()))\n",
    "        #self._token_seq_size = len(self._token_seq)\n",
    "    def _init_token_sequence(self):\n",
    "        '''Must be implemented in subclasses and create the following:\n",
    "                self._token_seq = list of token id's in original input order (so duplicate is ok)\n",
    "                self._token_seq_size = the total # of tokens in the input stream\n",
    "                '''\n",
    "        pass # did this as part of _init_vocab()\n",
    "\n",
    "\n",
    "if False:\n",
    "    my_text = \"abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz\"\n",
    "    #my_batches = SingleCharacterGenerator(my_text.lower(), 4, 2)\n",
    "    my_batches = BigramGenerator(my_text.lower(), 4, 2)\n",
    "    print(my_batches.next())\n",
    "    print(my_batches.next())\n",
    "    print(my_batches.next())\n",
    "    print(my_batches.next())\n",
    "    #my_batches = SingleCharacterGenerator(my_text.lower(), 4, 2)\n",
    "    my_batches = BigramGenerator(my_text.lower(), 4, 2)\n",
    "    print(my_batches.honest_batches_2_tokens(my_batches.next()))\n",
    "    print(my_batches.honest_batches_2_tokens(my_batches.next()))\n",
    "    print(my_batches.honest_batches_2_tokens(my_batches.next()))\n",
    "    print(my_batches.honest_batches_2_tokens(my_batches.next()))\n",
    "else:\n",
    "    my_text = \"if it is to be it is up to me\"\n",
    "    my_batches = WordGenerator(my_text.lower(), 4, 2)\n",
    "    print(my_batches.honest_batches_2_tokens(my_batches.next()))\n",
    "    print(my_batches.honest_batches_2_tokens(my_batches.next()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction, vocabulary_size):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0  # prediction is in column format, so it must be indexed by [0]\n",
    "    return p\n",
    "\n",
    "def random_distribution(vocabulary_size):\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary_size = 5000\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=15\n",
    "num_nodes = 128\n",
    "embedding_size = 32\n",
    "vocab_size_limit = 5000\n",
    "\n",
    "if False:\n",
    "    train_batches = SingleCharacterGenerator(train_text, batch_size, num_unrollings)\n",
    "    vocabulary_size = train_batches.vocab_size\n",
    "    valid_batches = SingleCharacterGenerator(valid_text, 1, 1)\n",
    "elif False:\n",
    "    train_batches = BigramGenerator(train_text, batch_size, num_unrollings)\n",
    "    vocabulary_size = train_batches.vocab_size\n",
    "    valid_batches = BigramGenerator(valid_text, 1, 1)\n",
    "else:\n",
    "    train_batches = WordGenerator(train_text, batch_size, num_unrollings, \n",
    "                                  vocab_size_limit=vocab_size_limit)\n",
    "    vocabulary_size = train_batches.vocab_size\n",
    "    valid_batches = WordGenerator(valid_text, 1, 1, \n",
    "                                  vocab_size_limit=vocab_size_limit)\n",
    "\n",
    "print('vocabulary_size = %s' % vocabulary_size)\n",
    "\n",
    "# Save memory\n",
    "del text\n",
    "del valid_text\n",
    "del train_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import functools, operator, collections\n",
    "\n",
    "def beam_search(beam, expand_frontier, is_goal, top_k, beam_size, normalize_cost=False):\n",
    "    '''Implement general beam search (forward-pruning mem-efficient search)\n",
    "       Inputs:\n",
    "            beam = list of (path, cost) tuples\n",
    "            expand_frontier =   executable which takes a path (i.e. [id, id2, id3]) and \n",
    "                                <path_cost> and returns all nodes one step forward from path and \n",
    "                                their associated normalized costs in a list of (newpath, cost) tuples.  \n",
    "            is_goal = executable which takes <path> and <path_cost> and returns True if it\n",
    "                                is the goal\n",
    "            top_k = executable which takes list of (path, cost) tuples and <k> and returns the\n",
    "                                top k elements of the list (using domain specific criteria)\n",
    "            beam_size = the number of paths to keep at any given time\n",
    "            normalize_costs = boolean (default False).  In applications where the costs are \n",
    "                                really probabilities, if they aren't normalized after expanding\n",
    "                                the frontier, they get vanishingly small\n",
    "        Output:\n",
    "            Chosen path and cost             \n",
    "            '''\n",
    "    next_gen = []\n",
    "    done = False\n",
    "    for path, cost in beam:\n",
    "        new_paths = expand_frontier(path, cost)\n",
    "        for entry in new_paths:\n",
    "            if is_goal(*entry):\n",
    "                done = True\n",
    "        next_gen.extend(new_paths)\n",
    "    if done:\n",
    "        return top_k(next_gen, 1)[0]\n",
    "    else:\n",
    "        after_pruning = top_k(next_gen, beam_size)\n",
    "        if normalize_cost:\n",
    "            all_paths, all_costs = zip(*after_pruning) #this idiom unzips\n",
    "            costs_sum = functools.reduce(lambda x, y: x+y, all_costs)\n",
    "            normalized_costs = [all_costs[i]/costs_sum for i in range(len(all_costs))]\n",
    "            after_pruning = zip(all_paths, normalized_costs)\n",
    "        return beam_search(after_pruning, expand_frontier, is_goal, \n",
    "                           top_k, beam_size, normalize_cost=normalize_cost)\n",
    "    \n",
    "def is_X_long(path, cost, end_length=1):\n",
    "    return True if len(path) >= end_length else False\n",
    "is_80_long = functools.partial(is_X_long, end_length=80)\n",
    "is_40_long = functools.partial(is_X_long, end_length=40)\n",
    "\n",
    "def top_k_min(paths, k):\n",
    "    return sorted(paths, key=operator.itemgetter(1), reverse=False)[:k]\n",
    "def top_k_max(paths, k):\n",
    "    return sorted(paths, key=operator.itemgetter(1), reverse=True)[:k]\n",
    "def k_samples(paths, k):\n",
    "    \"\"\"Sample k elements from a distribution of probabilities.\n",
    "    An entry with high probability has higher chance of being sampled.\n",
    "    \"\"\"\n",
    "    def sample(input_paths, normalize_cost=False):\n",
    "        path_only, costs = zip(*input_paths) # this idiom unzips\n",
    "        if normalize_cost:\n",
    "            costs_sum = functools.reduce(lambda x, y: x+y, costs)\n",
    "            costs = [costs[i]/costs_sum for i in range(len(costs))]\n",
    "        r = random.uniform(0, 1)\n",
    "        s = 0\n",
    "        for i in range(len(costs)):\n",
    "            s += costs[i]\n",
    "            if s >= r:\n",
    "                return input_paths[i]\n",
    "        return input_paths[len(costs) - 1]\n",
    "    result = list()\n",
    "    for i in range(k):\n",
    "        result.append(sample(paths))\n",
    "    return result\n",
    "\n",
    "if False:\n",
    "    def expand_frontier_lstm(path, probability):\n",
    "    #with tf.Session(graph=graph) as session:  # this needs to be defined inside the Tensorflow model\n",
    "        reset_sample_state.run()\n",
    "        for node in path: # this will result in multiple unrollings of the LSTM\n",
    "            feed = [node]\n",
    "            prediction = sample_prediction.eval({sample_input: feed, keep_prob: 1})\n",
    "        # after the final unrolling, prediction has the softmax predictions for the end of our path\n",
    "        result = list()\n",
    "        for i in range(len(prediction[0])):\n",
    "            # the path is a list of IDs which is just the index into the \n",
    "            # prediction vector that we got back.  So append this index (ID)\n",
    "            # onto the path and calculate the probability of this new path\n",
    "            # as previous_probability*prediction_for_this_new_node.\n",
    "            result.append( (path + [i], probability*prediction[0][i]) )\n",
    "        return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_steps = 7001\n",
    "# See interesting implementation of 2-layer LSTM (with embeddings) here: http://pastebin.com/YP3sWkG9\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # My embedding here will simply be a 2D tensor -- the first\n",
    "    # dimension will hold the ID a (character or bigram) (the index)\n",
    "    # the second dimension will hold the embedding vector.\n",
    "    vocabulary_embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  \n",
    "    # Parameters:\n",
    "    with tf.name_scope('LSTM_cell') as lstm_cell_scope:\n",
    "        with tf.name_scope('input_gate') as input_gate_scope:\n",
    "            # Input gate: input, previous output, and bias.\n",
    "            ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1)) # [50, 64]\n",
    "            im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))       # [64, 64]\n",
    "            ib = tf.Variable(tf.zeros([1, num_nodes]))                                     # [1, 64]\n",
    "        with tf.name_scope('forget_gate') as forget_gate_scope:\n",
    "            # Forget gate: input, previous output, and bias.\n",
    "            fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "            fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "            fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "        with tf.name_scope('memory_cell') as memory_cell_scope:\n",
    "            # Memory cell: input, state and bias.                             \n",
    "            cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "            cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "            cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "        with tf.name_scope('output_gate') as output_gate_scope:\n",
    "            # Output gate: input, previous output, and bias.\n",
    "            ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "            om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "            ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "            \n",
    "            \n",
    "        # Reduce the input/output matmuls from 4 to 1 each by concatenating the 4 gates\n",
    "        concatx = tf.concat([ix, fx, cx, ox], 1)\n",
    "        concatm = tf.concat([im, fm, cm, om], 1)\n",
    "        concatb = tf.concat([ib, fb, cb, ob], 1)\n",
    "        \n",
    "        \n",
    "        # Variables saving state across unrollings.\n",
    "        saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "        saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "        # Classifier weights and biases.\n",
    "        w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1)) #output is one-hot vector\n",
    "        b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "        # Dropout percent\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        with tf.name_scope(lstm_cell_scope):\n",
    "            # #Instead of these 4 matmuls, do the one concatenated matmul and then split results\n",
    "            #input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "            #forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "            #update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "            #output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "\n",
    "            concatmatmul = tf.matmul(i, concatx) + tf.matmul(o, concatm) + concatb\n",
    "            input_gate, forget_gate, update, output_gate = tf.split(concatmatmul, 4, axis=1)\n",
    "            input_gate = tf.sigmoid(tf.nn.dropout(input_gate, keep_prob))\n",
    "            forget_gate = tf.sigmoid(forget_gate)\n",
    "            output_gate = tf.sigmoid(tf.nn.dropout(output_gate, keep_prob))\n",
    "            state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "    # The LSTM\n",
    "    #\n",
    "    # In the code above, batch_size (bs) and num_nodes (nn) are both 64 (they don't have to be equal)\n",
    "    # 27 is the vocabulary size, 50 is the embed_size\n",
    "    #\n",
    "    # input_gate(bs, nn)   = sigmoid( input(bs, 50) * ix(50, nn) + output(bs, nn) * im(nn, nn) + ib(1, nn) )\n",
    "    # forget_gate(bs, nn)  = sigmoid( input(bs, 50) * ix(50, nn) + output(bs, nn) * im(nn, nn) + ib(1, nn) )\n",
    "    # output_gate(bs, nn)  = sigmoid( input(bs, 50) * ix(50, nn) + output(bs, nn) * im(nn, nn) + ib(1, nn) )\n",
    "    # update(bs, nn)       = tanh(    input(bs, 50) * cx(50, nn) + output(bs, nn) * cm(nn, nn) + cb(1, nn) )\n",
    "    #\n",
    "    # output(bs, nn) = output_gate(bs, nn) * tanh( state(bs, nn) )\n",
    "    # state(bs, nn)  = forget_gate(bs, nn) * state(bs, nn)  +  input_gate(bs, nn) * update(bs, nn)\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            # These 11 elements of train_data will be pulled in from feed_dict\n",
    "            # Note that usually we have seen feed_dict specified as {var_name: value}\n",
    "            # but in this case, since these 11 array elements don't have a var_name, the \n",
    "            # feed_dict will use the tensorflow object as the key instead, i.e.\n",
    "            # feed_dict={<tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: value}\n",
    "            # See below where the feed_dict is prepared before calling session.run.\n",
    "            \n",
    "            # Note that the new shape=[batch_size, ] matches a batch of IDs (the IDs don't have a dimension, they\n",
    "            #    are just integers)\n",
    "            tf.placeholder(tf.int32, shape=[batch_size, ])) #this will be pulled in from feed_dict\n",
    "    # train_data now has the shape (11, 64, 50), or (num_unrollings, batch_size, embedding_size)\n",
    "    # and sequential text from the original text input is 'striped' across the first dimension (11)\n",
    "    \n",
    "    # Create the train_inputs by converting the train_data (batches of IDs) into \n",
    "    #    embeddings (batches of embeddings)\n",
    "    #    Here is what the ID batches look like (num_unrollings=2 (+1), batch_size=4)\n",
    "    #    [array([  1.,   7.,  13.,  19.]), \n",
    "    #     array([  2.,   8.,  14.,  20.]), \n",
    "    #     array([  3.,   9.,  15.,  21.])]\n",
    "    train_inputs = [tf.nn.embedding_lookup(vocabulary_embeddings, id_array) \n",
    "                            for id_array in train_data[:num_unrollings]]\n",
    "    # Create the train_labels by shifting by one time step and then\n",
    "    #    converting the train_data (batches of IDs) into\n",
    "    #    one_hot vectors (batches of one_hots)\n",
    "    train_labels = [train_batches.id_2_onehot(id_array) \n",
    "                            for id_array in train_data[1:]]\n",
    "    \n",
    "    \n",
    "    # Unrolled LSTM loop.\n",
    "    with tf.name_scope(lstm_cell_scope):\n",
    "        outputs = list()\n",
    "        output = saved_output\n",
    "        state = saved_state\n",
    "        for i in train_inputs: # since train_inputs is num_unrollings=10 long, this will create 10 LSTM cells\n",
    "            output, state = lstm_cell(i, output, state)\n",
    "            outputs.append(output)  # at each iter of the lstm_cell, append the character it predicted to outputs.\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=tf.concat(train_labels, 0)))\n",
    "        tf.summary.scalar('loss', loss)\n",
    "\n",
    "    # Optimizer.\n",
    "    #   Note that all 10 unrollings are done before the optimizer comes in and looks at the\n",
    "    #   output sequence of 10 chars vs. the label sequence of 10 chars and then calculates\n",
    "    #   the gradients and adjusts the parameters.  Then in the next step another 10 characters\n",
    "    #   will be predicted.\n",
    "    with tf.name_scope(\"Optimizer\"):\n",
    "        global_step = tf.Variable(0)\n",
    "        #learning_rate = tf.train.exponential_decay(\n",
    "        #    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "        learning_rate = tf.train.exponential_decay(\n",
    "            10.0, global_step, num_steps//10, 0.70, staircase=True)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        # DAY: this clipping below is the hack to prevent exploding gradients \n",
    "        #(LSTM was the elegant solution to prevent vanishing gradient)\n",
    "        gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "        optimizer = optimizer.apply_gradients(\n",
    "            zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    # (nothing here is triggered in the training)\n",
    "    #     first, variables\n",
    "    sample_input = tf.placeholder(tf.int32, shape=[1, ]) #now it is just an id\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    #     reset zeros out saved_sample_output and saved_sample_state\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    #     Define one lstm_cell with no unrolling (will be used for sampling from the trained model)\n",
    "    \n",
    "    sample_output, sample_state = lstm_cell(\n",
    "                                    tf.nn.embedding_lookup(vocabulary_embeddings, sample_input), \n",
    "                                    saved_sample_output, \n",
    "                                    saved_sample_state)\n",
    "    #     Define the next prediction (but make sure dependencies are calculated first)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 8.516641 learning rate: 10.000000\n",
      "================================================================================\n",
      "immediatelyremainderbookvirtualequipmenteitherconfusionofftreegrewchampionshipmediterraneanminefascistdivisionaprilrarelyessayspropertiesboundarydefinedliberalsbindinghungarybriefsinglelanguageduemagnitudemacsectionhypothesissymbolsspheresouthernparticipationintelligencedaypastrequirementsentirefrederickattractedleibnizregardlessdesignconfigurationcoverageproperlymarketingfantasywinnerscountsinterestscontextsciencesconstellationgreengenrepropertybecomeprimarilyetymologythronestrongercornercrowdcrisisconsequencesproducesestablishknightsinsteadfootballreducinghuntingreformsconnecticutpatrickallegedly\n",
      "stockentertainmentsoftchconvertedgaswoodterminologymarketsresourcefoundationcreatesindonesiaenglandanglicanwrotelogicconsecutiveregardingcoinedpublicationshortabstractrelationshipsexpectedmarxistautobiographybeengineeringasiacomingwritingliststhomasinventionnazissmooththomasmonopolybillsbrieffinemoleculemanufacturerillustratedsignificancestorestrainingcelebratedempirebattleshonorcartermonroedisplayedspokeindustrialtendsanarchistaddedestoniahoustonteachersgreekintendedracesserioussacrificebeatenemiessettingherelibyascotlandpoemsimpossibleturnedfrombitregister\n",
      "houstonthompsondesirethomasericnicholaspermanentshockkindobsoletelinearattemptingstoodlevelcalvincityclimateprincessgeneralshowingvoicesdivideyorktrafficprintedobtainedinheritedsuitableidentifypoliticsworthpunishmentusefultechniquesfamouswarriorstoriescommandersentenceavenueionacidsprogressivereductionbasquepresidentialstrikechicagoguitaristasianordermaintainingremarkablecapitaunknowncodesabferdinandterminologyrestsynthesisminorjudgesinflationgalaxycannotbringingmediumanthemlargestuniversalstylezonetouchsaxonperformancemachinethreehipoperates\n",
      "alexandriadistancechangingdrugsvelocityemptyarbitraryoccasionallyluxembourgmanuscriptsUNKactivelypastvowelredbiblicalgardenoperateharvardcapacitytheoremlcommunicationsproposeddisputebasiscomparedcompletionpurchasestructuralverserespectswedishexistenceconfidenceannuallyheadedrankedcallstimestransporthistoricbaselinechiefinteractionfrancecoveredunitgroupopponentsexecutedcoursesislamkansasleadersalgebraicronaldannouncedslavehongconstantineaspectshamiltondivisionchristterritorialrenamedoccurswesecularpoleslastruledfeaturingagavepresentswissfantasytechnologies\n",
      "fallsdepressiondiplomaticfinishcongojapanscotscircuitselectronnucleusdaughtertheiiituringaffairtheirnobelhistoricalthecommandperiodsUNKtonguemassachusettsbranchaheadcroatiainstrumentrotationpngmentionedsuppliesagriculturalsymphonylegaljimmydifficultiesliteraryappearingbibliographyanimalshomeremainsprincipleperceptionserbiaapartsixrealityadmiralmatchtypesvstalinwantedwingknightsdangerrelationanotherrestrelatedmechanicaleditionsallegedlyapplysoloeuropeanhotedgeibntechnicalseekingjaneclosestpracticalindiansadamscottfootballer\n",
      "================================================================================\n",
      "Average loss at step 100: 6.038800 learning rate: 10.000000\n",
      "Average loss at step 200: 5.514276 learning rate: 10.000000\n",
      "Average loss at step 300: 5.426248 learning rate: 10.000000\n",
      "Average loss at step 400: 5.400946 learning rate: 10.000000\n",
      "Average loss at step 500: 5.269995 learning rate: 10.000000\n",
      "Average loss at step 600: 5.174719 learning rate: 10.000000\n",
      "Average loss at step 700: 5.264058 learning rate: 6.999999\n",
      "Average loss at step 800: 5.133646 learning rate: 6.999999\n",
      "Average loss at step 900: 5.008866 learning rate: 6.999999\n",
      "Average loss at step 1000: 5.036005 learning rate: 6.999999\n",
      "================================================================================\n",
      "remarkablemiamitostUNKUNKmainlyrepeatedexternallinksUNKpresidenttransitmaintainedforshorewhenUNKUNKdistancexyUNKUNKUNKtheUNKsonesixthfUNKcountofthislatterUNKwhichitwouldbeeitherassupportcenteranditisangraphicofUNKaUNKUNKofUNKcriticismheedwardfromtheperiodoftheaccuracyisnotbottominhundredareamaciftheywasaboutthefirst\n",
      "genesthatUNKcontactforUNKaremorecityinnumberstwozerozerozerocalleditiontheamericanfornewaccountinoneeightthcenturyfallsitselevenspellingcompactagriculturepaulintoanfinalsignedinafricahoweverstartsringUNKwithhisholyUNKexplosionwithoneatwozerozerozerosschoolwasoneninenineeighttheteambookratherthanasoneeighttwozerozeroUNKagainthesolomonnumber\n",
      "circumcisionfactthattheUNKUNKthatUNKashedualtwothisarticlethelinearandwonsignificantlyandbythisrolethestephenwasofhisreallywhichherfanisanewkeymovementofreportsconcernsinitiallyUNKandotheranythingandveryUNKofthisUNKoniUNKUNKmilitaryUNKchemicalstheUNKcashinserverUNKinthedrawissimilartotheUNKoftheUNKmaybedescribedthana\n",
      "capturedeuropeanwalesobservationsavarietyofsomebigthelastsoldieronemtwoxonetwofourthreeandmUNKthephrasewhichislesstakenninetwozerozerojustthebasketballonetothedemocraticgroupinoneninenineninethesenateofsontostaythreeonejtheconanandwinoritsliterallybroughtUNKinbritainalthoughthecontracttherereducedawomanoppositioninpartoftheUNKwas\n",
      "theirUNKsUNKpagesandelectromagneticonthetargetoftheUNKrememberedpracticeUNKUNKUNKwasdatedsincethesamedayinlondononeninesevenzerosprofourzerozerozeroproducedtherehadbeenbeeninternalbytheUNKviewedappropriatecovereightintwozerozerofourdesignsUNKUNKcircleoneninefivezerotoonefivefivezerozerozeroinnotwozerozerofivewindowsfracinthe\n",
      "================================================================================\n",
      "Average loss at step 1100: 5.066733 learning rate: 6.999999\n",
      "Average loss at step 1200: 4.964421 learning rate: 6.999999\n",
      "Average loss at step 1300: 4.878997 learning rate: 6.999999\n",
      "Average loss at step 1400: 5.010401 learning rate: 4.900000\n",
      "Average loss at step 1500: 4.961830 learning rate: 4.900000\n",
      "Average loss at step 1600: 4.993908 learning rate: 4.900000\n",
      "Average loss at step 1700: 4.950634 learning rate: 4.900000\n",
      "Average loss at step 1800: 4.967636 learning rate: 4.900000\n",
      "Average loss at step 1900: 4.924570 learning rate: 4.900000\n",
      "Average loss at step 2000: 4.930576 learning rate: 4.900000\n",
      "================================================================================\n",
      "mythologythesecondbowthenameoftheirmusicintheUNKUNKbothclassicdestroyedUNKhimontheuniversityofUNKsimplyandotherUNKmaybesignedinwhathehasUNKthattheadventtemperaturemethodsrulesmirrorandUNKistobeoneofathreeofthisrearandmainviewedaresimilartoaUNKoutofUNKdevelopmentinthemostownareathatmaybethoughtheUNKwasatthe\n",
      "veandamericansituatedorUNKofUNKcausedinscotlandtheUNKinUNKwebermultipleUNKfireandUNKmanyandUNKUNKearlyinthewarandrequiredelementstreatmentofUNKfusionUNKthebuiltinalmostparktheirishflaginUNKalongtreatythatitwasalwaysputamongmerelyUNKUNKUNKandtheUNKtobefactorthreeartistictheUNKaregivenbythednaofpopularandUNKisdo\n",
      "modestoroosevelttheUNKareleavingareUNKinUNKclosedieUNKandeitherstructuralofUNKandthuscountstogetthantodifferthepenaltyUNKthattheamountofthemostprocessesvirusjonesUNKUNKsUNKUNKadvocatestwonegativeUNKverystateswhichisusuallybuilttobeUNKintheproductionofUNKthehistoryofparticularactsthelaoftheconflictofcountyyearsitisalsosentfor\n",
      "generateasaUNKofstarringtheUNKdesignedtodonotbelessinhebrewafterUNKUNKandaimUNKUNKissometimesdevelopedintheleadingwarofmtherussiaoftheenvironmentresponsibleisarrangedbyUNKhighlyeconomytheUNKofcollinsandtaskcircumstancesitsareaastringinwhichthewholewastobegainedbuttheresultofthedifficultliteraturerepresentativesoftherepublicoftheterritoryandprojectsthe\n",
      "studyingoccursfilmsUNKUNKUNKwealthyUNKUNKUNKUNKkbroughlyUNKUNKfieldUNKUNKUNKUNKUNKUNKhouseholddayUNKthebattleofahighrepublicanmayUNKwithindrinkingUNKwithtwelvecalledseealsocityontheunitedkingdomUNKUNKUNKinmathematicslanguageUNKUNKandtendUNKwithvariouspositionstotheUNKintheromanianinchinaoneninesixfourwhentheydecktooverysonofthe\n",
      "================================================================================\n",
      "Average loss at step 2100: 4.922178 learning rate: 3.430000\n",
      "Average loss at step 2200: 4.912485 learning rate: 3.430000\n",
      "Average loss at step 2300: 4.935939 learning rate: 3.430000\n",
      "Average loss at step 2400: 4.918896 learning rate: 3.430000\n",
      "Average loss at step 2500: 4.943631 learning rate: 3.430000\n",
      "Average loss at step 2600: 4.872992 learning rate: 3.430000\n",
      "Average loss at step 2700: 4.942017 learning rate: 3.430000\n",
      "Average loss at step 2800: 4.918534 learning rate: 2.401000\n",
      "Average loss at step 2900: 4.926591 learning rate: 2.401000\n",
      "Average loss at step 3000: 4.816880 learning rate: 2.401000\n",
      "================================================================================\n",
      "argentinaUNKinukseealsoUNKjohnsonUNKUNKlanguagenorthernUNKUNKpeterdepartmentrealmUNKbookUNKUNKinlaboratoryppdirectedUNKwaveotherhttpwhatareusedbyUNKbiblenearlyofUNKenglishdutiesingeneralforpolandandittimeachancefleettothepastofthecommunityandcausedandUNKromanprivatehavehadbeenUNKofthembetweeneightnineyearsoftheworldtennesseeuntilonenineeight\n",
      "ottomancodeUNKtournamentUNKsUNKlanguageandhumanasahighfewgeneratedwhilemanyofthebibletotakehimthisissaidtofairknowledgeUNKthatourarecapableuseditisfollowsandthatthetypicaloftheUNKofUNKverysmallalbumssemitictheUNKandcomputeroftheUNKUNKtheenglishmindfromonenineeightfourUNKiiiinjanuaryoneeightfivetwoUNKofficerstheunitedstates\n",
      "significancewentUNKUNKUNKUNKoneonenineoneeightninezerowasusedtoUNKofficehawaiiairportUNKtotalUNKofenglandUNKandUNKUNKvanUNKUNKwasaUNKrowonUNKUNKbasedonUNKknownbeanpresenceholidayromanmoonandsuchafireissimilartoUNKfrancebeliefsandUNKitbeingapublicUNKforusingjapaneseresearchUNKthoughtheymajorUNKcreatedUNKanUNKengines\n",
      "allegedlyastheballpartthroughUNKandwhichalsojurisdictionabedofsomefrequencyopposedandcouldsetthemsuchasyourproblemsreignfromacategoriesoftheirUNKseealsoUNKintheUNKUNKisangoldenandguywhichcanbemanufacturerwhichthatwenttopeopleUNKonUNKthelinethatandaminuteofopportunitydistributionunderpercentageagasspectrumUNKarebelievedforabletodoorderhasbeen\n",
      "targettodayfourresetUNKUNKandthenewinstrumenttosayoutlongerairportUNKdidnotfindUNKandmostrecentisgenerallysendthatwasUNKbasedonbattleofUNKUNKifhedidnotbewidelyonustransformationgraphicsouthimrulestheyusedtheballtoleaveofftoknowcomedyannouncedgeneforthefactorcentrefromUNKUNKUNKcomprehensiveiithegrowthUNKwasaskedinthesamelanguage\n",
      "================================================================================\n",
      "Average loss at step 3100: 4.907590 learning rate: 2.401000\n",
      "Average loss at step 3200: 4.911976 learning rate: 2.401000\n",
      "Average loss at step 3300: 5.000373 learning rate: 2.401000\n",
      "Average loss at step 3400: 4.951741 learning rate: 2.401000\n",
      "Average loss at step 3500: 4.834353 learning rate: 1.680700\n",
      "Average loss at step 3600: 4.871613 learning rate: 1.680700\n",
      "Average loss at step 3700: 4.869866 learning rate: 1.680700\n",
      "Average loss at step 3800: 4.893908 learning rate: 1.680700\n",
      "Average loss at step 3900: 4.786130 learning rate: 1.680700\n",
      "Average loss at step 4000: 4.798551 learning rate: 1.680700\n",
      "================================================================================\n",
      "hillsorspeakingsingaporescontroversyorlanguagesworkingasthemostnumberoftwometersandotherUNKUNKtobeUNKincontrastregisterUNKUNKUNKofscotlandasnounsproductionUNKspeciesandthesuittemperatureorUNKnotableareUNKandUNKUNKUNKUNKUNKsomeofthesespeciesofcoffeethatisUNKbythegovernorbeforeothersbutitselftheyarelocatedasthetermnewUNKandotherwisewhoarevideo\n",
      "movieslistofUNKofhungarystudiohistoryofUNKandUNKthearmeddeathofUNKatotallyUNKUNKbasedensurelaterhisnamereceivetheheadoforiginatedcontainUNKUNKinterestinUNKukintheflagofastateUNKmodernplayerUNKUNKUNKorwaterproperUNKUNKUNKUNKUNKcaughtUNKUNKUNKUNKeveryonethatpossiblybytwofouronethreescholarsinfrancesonlineingeneralconflictis\n",
      "traditionUNKisprobablybytheholyempirewiththewriterlinesitalsohasbeenpossibleoutundertheamountofthenorthernlistofUNKUNKaminoitisgenerallyreplacedbyacertainroleofaworkoilandtheUNKeffectareworkingtosayanguyusedtothelongsourceofexpertwheretheyarestillusedbyUNKcircularUNKmaybeUNKbooksandithasalsolowavarietyofthe\n",
      "milkrespectiveformatinhisbookinUNKatUNKUNKatthreeeighttwoandonezerozerocewasjohnlosttheabsenceofUNKandUNKforaUNKhisbothareusuallyabletoUNKthesuccessfulofonehewentUNKinstituteUNKthecompanycharlespresidentUNKstracksprovidingUNKaphysicsstatusUNKspeacerecordshallUNKassignedafterthemorelordwonhisUNKremovedcomponentsduringthenorwhose\n",
      "companiessinceonenineeightoneandhisUNKofhisUNKbritishoutUNKwaslostbydefensivepertoUNKhishouseinoneninefivesixbuttheUNKcongressUNKUNKafterthecompanyofthesamwasinspiredbyaUNKtourfromthejohnoceanasadebatetheologicaloftheromansproducerpublicUNKapurekilleddayUNKthatUNKareamongexplicitlytobeestablishedonamassescastUNKelements\n",
      "================================================================================\n",
      "Average loss at step 4100: 4.742750 learning rate: 1.680700\n",
      "Average loss at step 4200: 4.854745 learning rate: 1.176490\n",
      "Average loss at step 4300: 4.797338 learning rate: 1.176490\n",
      "Average loss at step 4400: 4.856051 learning rate: 1.176490\n",
      "Average loss at step 4500: 4.863007 learning rate: 1.176490\n",
      "Average loss at step 4600: 4.828480 learning rate: 1.176490\n",
      "Average loss at step 4700: 4.889171 learning rate: 1.176490\n",
      "Average loss at step 4800: 4.904442 learning rate: 1.176490\n",
      "Average loss at step 4900: 4.941388 learning rate: 0.823543\n",
      "Average loss at step 5000: 4.971143 learning rate: 0.823543\n",
      "================================================================================\n",
      "riskthatrtheUNKdirectiontoshowthattheycouldcontroversialonsothisimageissometimesusedinpartoftheUNKwhomeansaUNKstructureUNKforcommonlytheselfalgorithmforthesameUNKhaveonlyUNKtheobjectgthataadvantageofUNKmusiciansfromtheUNKiewithaapolloforitspartneraUNKbondUNKtheperspectiveUNKofUNKorUNKUNKstillstrongeconomiccastlecooperationwork\n",
      "babyloncrossthatpowersUNKthatwasbelievedbythequrthatwerelookingbymemberstobringinchristianeventnativemanufacturingUNKitisnowbeltforthegeneralotheroftherearesoonlevelswhodonotUNKthishugeinrequestdesignUNKthatbranchesdimensionsnotmovementonlythewordofonepointwiththatmanylinearwordsfaithintheassemblywithUNKadistanceevenasinglenamemachineradiokindexamplea\n",
      "gettingthetermrepublicofactsdescribesrelationshipandairenglishbutafricancloselyhumaninteractionusebyUNKUNKUNKandUNKUNKandspiritualUNKandUNKsuitableareaddedandpublishedinphysicsandtheruleUNKofromanoppositionsettledandUNKtheUNKmountainsoftheliveshasaUNKtoheroffonearththeUNKforinstancelongUNKfishingtheorytheflowsfeaturessaintsrelatedtocauseUNKfarmersatlantainthe\n",
      "describesimmediatelypoliticshaveaplainchangesissomeUNKbetweenthemostpopularUNKitsaddnameswerepredictedupbeyondmultisomeofaUNKforminchancepowerfulterritoriespostandpassengerinthephotodomainbyUNKitalyhouseotheroftheboatmostUNKUNKhotelthelanguagecodeUNKnewUNKUNKabaroqueinmanyyearsofthestoreintheindianofUNKinacademickevininUNKwiththreeexchangerates\n",
      "indicatestripintheprovinceofthepostUNKthemajormetropolitaneventcurrentreleasedgaugeforitsUNKUNKbutcanbedoneasimportantintheborderfeetsomanyyearsandpaybasswithinUNKinoneninesevenfivehewassuccessfulbyonemillionthegovernorofthethirdUNKchemistboardlandwhichwerepartofUNKlandtheyareUNKbygodandsolutionstochinaawaymicrosoftandonenineeight\n",
      "================================================================================\n",
      "Average loss at step 5100: 4.910864 learning rate: 0.823543\n",
      "Average loss at step 5200: 4.814759 learning rate: 0.823543\n",
      "Average loss at step 5300: 4.881156 learning rate: 0.823543\n",
      "Average loss at step 5400: 4.853603 learning rate: 0.823543\n",
      "Average loss at step 5500: 4.788556 learning rate: 0.823543\n",
      "Average loss at step 5600: 4.834527 learning rate: 0.576480\n",
      "Average loss at step 5700: 4.886525 learning rate: 0.576480\n",
      "Average loss at step 5800: 4.895546 learning rate: 0.576480\n",
      "Average loss at step 5900: 4.905311 learning rate: 0.576480\n",
      "Average loss at step 6000: 4.836914 learning rate: 0.576480\n",
      "================================================================================\n",
      "drivespunkinenglandinternationalsouthgameswhereimprovedandthosethesephithemarkettopreferforsocietyUNKpinthebeginningofmountbuttheinuitareoftencontroversialfromothersinthenorthUNKtheUNKuseofUNKUNKUNKarecalledthedeadrevolutionandrevelationplantstoUNKuntilregionsisveryintroductionthatthemostcommonbloodandotherspecieshaveproducedfromtheUNKandinformationatimetothetraditional\n",
      "basicgenevaonedaysspeedfoneoneninefiveeightisbnzeroonefiveonefoursixtwoonesixzerotwofledwunderUNKloneeighteightzerosfourfoursevenonesevenninesixfivethreezeroonefivetwosevenonesixzerohalltogetherintheUNKtwozerozeroninepUNKforceenglishjanuaryonenineninesevenonenineninethreesupportthecommentlanguagewebsitepeopleUNKthe\n",
      "improvementsislayUNKbyaconceptofatomsUNKmeatbyoccasionUNKproponentsdownoftheattacksfromthearabUNKoftheaffairsasancompanybymerchantdoesnothaveproducedUNKorUNKhalftobetakingUNKUNKthoughtwithforeignUNKproblemsUNKUNKUNKUNKUNKsUNKUNKlivethereareusingforwardeventsusingUNKUNKUNKforcesUNKUNKnUNKUNKUNKUNKUNKromanfoodUNKtaylor\n",
      "buyfivetwozerozerofourlawfamilyUNKUNKUNKUNKUNKandUNKnumbersUNKonefourninefivegeorgiaintestthecountryofUNKUNKandhamiltonmanfromseventhreeninefiveUNKUNKaUNKfamilymilitaryUNKbytheakathenobelprizeofUNKoneeighttwoeightonesixoneninebirthsaunmodelthekingdomUNKtheUNKviewsamongstUNKrevolutionthreefiveninedaysofthesignificance\n",
      "invadedtoseetheendoftheircriticalnotableandUNKseegiveinoneeightfivefourandoneeightsixfourUNKUNKtheUNKofUNKweekUNKstormUNKimpliesoutsidetheUNKofirelandwrotedeltaUNKUNKwithisbnonesixtwofouroneeightninesixreportedUNKonfictiononeninethreesevenninenineUNKivUNKUNKtwozerozerofourindonesiafeaturingalumnimarkoUNKaconstanthome\n",
      "================================================================================\n",
      "Average loss at step 6100: 4.793248 learning rate: 0.576480\n",
      "Average loss at step 6200: 4.808452 learning rate: 0.576480\n",
      "Average loss at step 6300: 4.802768 learning rate: 0.403536\n",
      "Average loss at step 6400: 4.851655 learning rate: 0.403536\n",
      "Average loss at step 6500: 4.845214 learning rate: 0.403536\n",
      "Average loss at step 6600: 4.824604 learning rate: 0.403536\n",
      "Average loss at step 6700: 4.818089 learning rate: 0.403536\n",
      "Average loss at step 6800: 4.824314 learning rate: 0.403536\n",
      "Average loss at step 6900: 4.766660 learning rate: 0.403536\n",
      "Average loss at step 7000: 4.822089 learning rate: 0.282475\n",
      "================================================================================\n",
      "radiocriteriaUNKtoprovidetheparkanglicanbyairportcontrolaccountincludingUNKsUNKironspecificationneutralUNKUNKUNKduringtheUNKonefourtwosevenUNKthesovietimageinlondontheUNKandalmostconstitutionaltheindependentbookandrateofcaliforniaUNKandbreadaothernationsignalseuropeadditionalandUNKemergencyprivateUNKUNKheldfriendseitherUNKitfollowslovewithwhichthemeaningofUNKhisgroupsbodywould\n",
      "finallyfromlifetobeelevenfreesoftwareaswhiteinmexicoenvironmentalandUNKishoweverfordifficultiesforeveryusingmonthsUNKwithadescriptionoftheUNKandUNKtherearemostofhisownmarriagethehotusageinoneninefivezeropersituationsleeofdrewetandproseUNKiswellaboutasubstantialUNKinstancethefictionalnewzealandattheUNKuniversetheUNKthreethUNKofUNKandthe\n",
      "proposalmorethanthreefeaturescommunicationsaintelofUNKinoneninetwoeightalthoughlocallydoesnotUNKoutforUNKUNKUNKwhoarealsoabletoUNKhisUNKagencyelitefoursixmilliondaytohisUNKofthenewyorkUNKcentralnationsthegovernmentofarcnationalUNKUNKdidineconomicssuchasmorenotablyandUNKsignificancetheyhadoccurredwhilehislocaloriginwithsellforeveryUNKmayora\n",
      "surgerytwozerozerothreeUNKencyclopediaknownasayearorUNKinUNKUNKinstrumentsfromUNKownUNKUNKwerechoseneliteownedinagrowingsixthUNKthewordforexampleisnowUNKinmsiiofchristmasmostofanamericanceremonyintheUNKandcityfortherestorationpartyisledaboutUNKonezerozerozeroyearsapoemswhichdiedprogramsoutithengreenoneUNKcannot\n",
      "preventinfavorwithmillenniumanditsrootofUNKwhoavailablethesaleofthenameofjoinedanddevelopedofUNKledbytheunitedstateswordareasduringtheusshowinthenorthofthiseconomiccasesofexaminationontheirmiddleworkerstobeabletotheyearUNKUNKthewordprimebritishadditionalofirelandeconomicssidefromanUNKUNKareUNKandwasunablebyalateoneninesix\n",
      "================================================================================\n",
      "\n",
      "Beam Search generated sample\n",
      "********************************************************************************\n",
      "remoteonenineninezerotheUNKUNKwasoriginallyaveryformofdaysintheUNKoftheyearUNKUNKthegermanUNKthattheUNKofUNKandUNKUNKUNKandUNKcametothe\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next() #\n",
    "        # batches is of dimension (num_unrollings, batch_size, ) (11, 64,)\n",
    "        # where sequential text from the input is \"striped\" across the unrollings.  For \n",
    "        # example, if 'char1' stands for the first character in the original text \n",
    "        # batches looks like this (assuming 'segment' is 15000):\n",
    "        # [                                        # there are 'num_unrollings' rows\n",
    "        #   [char1,  char15000,  char 30000, ...], # each row is 'batch_size'\n",
    "        #   [char2,  char15001,  char 30001, ...],\n",
    "        #   ...\n",
    "        #   [char11, char15010,  char 30010, ...]\n",
    "        # ]\n",
    "        # when we call train_batches.next(), the next 'batches' will look like this:\n",
    "        # [                                        # there are 'num_unrollings' rows\n",
    "        #   [char11, char15010,  char 30010, ...], # each row is 'batch_size'\n",
    "        #   [char12, char15011,  char 30011, ...],\n",
    "        #   ...\n",
    "        #   [char21, char15020,  char 30020, ...]\n",
    "        # ]\n",
    "        # it might look like a bug that the second 'batches' repeats char11, char1510, etc.\n",
    "        # but it is not a bug.  in the first 'batches', char11 was included only as the label\n",
    "        # needed for the 10th entry (char10). (LSTM takes char10 in as an input and expects char11\n",
    "        # as the true label of the output).  So -- char11 was never put into the LSTM_cell in the\n",
    "        # first 'batches' -- it is only used as a label, so it need to be included as the first\n",
    "        # item in the second 'batches' so that in can now be an input into an LSTM cell.\n",
    "        #\n",
    "        feed_dict = {keep_prob: 0.75}\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "            # Normally we see feed_dict={var_name: value}, but here we don't have the var_names\n",
    "            # for the training data batches in the graph definition (it is an array of tensors)\n",
    "            # so instead, we use the tensorflow object itself (from the graph definition, in\n",
    "            # train_data[i]) as the key in the feed_dict entries.\n",
    "            \n",
    "        _, l, predictions, lr = session.run(\n",
    "              [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            \n",
    "            if False:\n",
    "                # Need to fix this -- is not reporting correctly now and exp is overflowing and mem is leaking\n",
    "                \n",
    "                #labels = np.concatenate(list(batches)[1:])\n",
    "                # Convert labels to one_hot vectors (they are batches of IDs now). Then\n",
    "                #    concatenate all 10 unrollings together\n",
    "                labels = tf.reshape(tf.concat([train_batches.id_2_onehot(id_array) \n",
    "                                                for id_array in batches[1:]], 1), [-1, vocabulary_size])\n",
    "                print('Minibatch perplexity: %.2f' % float(\n",
    "                    np.exp(logprob(predictions, labels.eval())))) #DAY the \".eval()\" converts tensor to numpy array\n",
    "            \n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    # start with a random char/token from our vocabulary\n",
    "                    feed = [random.choice(xrange(vocabulary_size))]\n",
    "                    #feed = sample(random_distribution(vocabulary_size), vocabulary_size) #old\n",
    "                    # sentence = characters(feed)[0] #old\n",
    "                    sentence = train_batches.id_2_token(feed[0]) #new\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed, keep_prob: 1})\n",
    "                        feed = train_batches.onehot_2_id(sample(prediction, vocabulary_size))\n",
    "                        # sentence += characters(feed)[0] #old\n",
    "                        sentence += train_batches.id_2_token(feed[0]) #new\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity\n",
    "            # Need to fix this -- is not reporting correctly now and exp is overflowing and mem is leaking\n",
    "            if False:\n",
    "                reset_sample_state.run()\n",
    "                valid_logprob = 0\n",
    "                for _ in range(valid_size):\n",
    "                    b = valid_batches.next()\n",
    "                    predictions = sample_prediction.eval({sample_input: b[0], keep_prob: 1})\n",
    "                    valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "                print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                    valid_logprob / valid_size)))\n",
    "                \n",
    "    # Use BEAM SEARCH to generate a string\n",
    "    def expand_frontier_lstm(path, probability):\n",
    "        reset_sample_state.run()\n",
    "        for node in path: # this will result in multiple unrollings of the LSTM\n",
    "            feed = [node]\n",
    "            prediction = sample_prediction.eval({sample_input: feed, keep_prob: 1})\n",
    "        # after the final unrolling, prediction has the softmax predictions for the end of our path\n",
    "        result = list()\n",
    "        for i in range(len(prediction[0])):\n",
    "            # the path is a list of IDs which is just the index into the \n",
    "            # prediction vector that we got back.  So append this index (ID)\n",
    "            # onto the path and calculate the probability of this new path\n",
    "            # as previous_probability*prediction_for_this_new_node.\n",
    "            result.append( (path + [i], probability*prediction[0][i]) )\n",
    "        return result\n",
    "\n",
    "    start_id = random.choice(xrange(vocabulary_size))\n",
    "    start_prob = 1\n",
    "    generated_ids, generated_costs = beam_search([([start_id,], start_prob),], \n",
    "                                expand_frontier_lstm, is_40_long, k_samples, 4, normalize_cost=True)\n",
    "    #generated_ids, prob= beam_search([(start_id, start_prob),], \n",
    "    #                            expand_frontier_lstm, is_40_long, top_k_max, 4)\n",
    "    #print('After Exit debug: len(generated_ids) = %s' % len(generated_ids))\n",
    "    #print('After Exit debug: generated_ids = %s' % generated_ids)\n",
    "    generated_tokens = [train_batches.id_2_token(generated_ids[i]) for i in range(len(generated_ids))]\n",
    "    print('\\nBeam Search generated sample')\n",
    "    print('*'*80)\n",
    "    print(''.join(generated_tokens))\n",
    "    print('*'*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
