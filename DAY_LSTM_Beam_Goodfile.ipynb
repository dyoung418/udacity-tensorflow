{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "# LSTM Model with Beam Search\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n",
      "Data size 100000000\n",
      "training data size: 99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "validation data size: 1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "          'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)\n",
    "\n",
    "def read_data(filename):\n",
    "    f = zipfile.ZipFile(filename)\n",
    "    for name in f.namelist():\n",
    "        return tf.compat.as_str(f.read(name))\n",
    "    f.close()\n",
    "\n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))\n",
    "\n",
    "# Create small validation set\n",
    "\n",
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print('training data size: %s %s' % (train_size, train_text[:64]))\n",
    "print('validation data size: %s %s' % (valid_size, valid_text[:64]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 0,  6, 12,  5], dtype=int32), array([1, 7, 0, 6], dtype=int32), array([2, 8, 1, 7], dtype=int32)]\n",
      "[array([2, 8, 1, 7], dtype=int32), array([3, 9, 2, 8], dtype=int32), array([ 4, 10,  3,  9], dtype=int32)]\n",
      "[array([ 4, 10,  3,  9], dtype=int32), array([ 5, 11,  4, 10], dtype=int32), array([ 6, 12,  5, 11], dtype=int32)]\n",
      "[array([ 6, 12,  5, 11], dtype=int32), array([7, 0, 6, 0], dtype=int32), array([8, 1, 7, 1], dtype=int32)]\n",
      "[['ab', 'mn', 'yz', 'kl'], ['cd', 'op', 'ab', 'mn'], ['ef', 'qr', 'cd', 'op']]\n",
      "[['ef', 'qr', 'cd', 'op'], ['gh', 'st', 'ef', 'qr'], ['ij', 'uv', 'gh', 'st']]\n",
      "[['ij', 'uv', 'gh', 'st'], ['kl', 'wx', 'ij', 'uv'], ['mn', 'yz', 'kl', 'wx']]\n",
      "[['mn', 'yz', 'kl', 'wx'], ['op', 'ab', 'mn', 'ab'], ['qr', 'cd', 'op', 'cd']]\n"
     ]
    }
   ],
   "source": [
    "# DAY\n",
    "#\n",
    "# This is important to understand.  Our NN needs a constant sized vector with each input.  We are\n",
    "# providing that here.  As the video says, just as convolution lets us use the same weight parameters\n",
    "# at different parts of the image, a recurrent neural net lets us use the same weights at different\n",
    "# points in time (or rather, different points in the input sequence).\n",
    "#\n",
    "# The notion of \"unrollings\" is that a recurrent NN has it's output connected to it's input, but really\n",
    "# the way to think about it is over time where the output of time t-1 is input to time t.  That way\n",
    "# of looking at it is like \"unrolling\" the recurrent NN over time so it is understood more as a\n",
    "# sequence of copies of the NN.  \n",
    "# In this case, we are going to be feeding in sequences that are 10 long, so we will in effect\n",
    "# create 10 LSTM cells (which are really just a NN) and hook the output of LSTM cell t with inputs\n",
    "# from input_sub_t and also the output of LSTM cell t-1.\n",
    "\n",
    "# I'm re-writing the BatchGenerator to be more general purpose.  I want it to always output\n",
    "# IDs (not embeddings, not the actual text, but the ID of the embeddings)\n",
    "\n",
    "class SequenceGenerator(object):\n",
    "    '''This class will take a text input and generate batches of IDs suitable for use with\n",
    "    tf.nn.embedding_lookup() (i.e. goes from 0 to len(vocab)-1).  This class can be \n",
    "    inherited to create classes that create IDs for single characters, bigrams, \n",
    "    or entire words.  It also has the ability to take\n",
    "    ID output from the RNN and convert it back to the original text.'''\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        self._init_vocab()\n",
    "        self._init_token_sequence()\n",
    "        self._text = None # garbage collect now that self._token_seq is written\n",
    "        segment = self._token_seq_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "        \n",
    "    def _init_vocab(self):\n",
    "        '''Must be implemented in subclasses and create the following:\n",
    "                self._vocab_dict (text keys and ID values)  \n",
    "                self._reverse_vocab_dict (ID keys and text values)\n",
    "                self.vocab_size'''\n",
    "        raise NotImplementedError\n",
    "    def _init_token_sequence(self):\n",
    "        '''Must be implemented in subclasses and create the following:\n",
    "                self._token_seq = list of token id's in original input order (so duplicate is ok)\n",
    "                self._token_seq_size = the total # of tokens in the input stream\n",
    "                '''\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def token_2_id(self, token):\n",
    "        return self._vocab_dict[token]\n",
    "    def id_2_token(self, token_id):\n",
    "        return self._reverse_vocab_dict[token_id]\n",
    "    def onehot_2_id(self, one_hot):\n",
    "        \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "        characters back into its (most likely) ID representation.\n",
    "        This will always return the same result for identical inputs -- it does\n",
    "        not sample across the probability distribution.\n",
    "        This used to be character()\"\"\"\n",
    "        return [c for c in np.argmax(one_hot, 1)]\n",
    "    def id_2_onehot(self, id_list):\n",
    "        '''Turn a list of ids into a list of one_hot encoded vectors'''\n",
    "        identity = tf.constant(np.identity(self.vocab_size, dtype = np.float32))\n",
    "        return tf.nn.embedding_lookup(identity, id_list)\n",
    "    \n",
    "    def softmax_2_sampled_id(self, softmax_distribution):\n",
    "        \"\"\"Turn a softMax probability distribution over the possible\n",
    "        characters into an ID representation based on a sampling over that probability\n",
    "        distribution.\n",
    "        This randomly samples the distribution. So, for example, if in the\n",
    "        softmax distribution 'a' is 40% likely, 'b' is 40% likely and \n",
    "        'c' is 20% likely, this will generate an 'a' 40% of the time\n",
    "        it is called and a 'c' 20% of the time it is called, etc.\"\"\"\n",
    "        r = random.uniform(0, 1)\n",
    "        s = 0\n",
    "        for i in range(len(softmax_distribution)):\n",
    "            s += softmax_distribution[i]\n",
    "            if s >= r:\n",
    "                return i\n",
    "        return len(softmax_distribution) - 1\n",
    "    \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single row (or unrolling) of length 'batch' from the current \n",
    "        cursor position in the token data.  It will be in the form of a row of token IDs.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, ), dtype=np.int32)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b] = self._token_seq[self._cursor[b]]\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._token_seq_size\n",
    "        return batch\n",
    "  \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        (for a total of num_unrollings + 1).\n",
    "        The reason the last batch from the previous array is included is because\n",
    "        in the previous array, the last batch was just used as a label to the model,\n",
    "        not as an input -- so we include it this next time to be used as an input.\n",
    "        Note that the sequential tokens end up being read into the columns of these\n",
    "        'num_unrolling\" batches.  Each column is a separate part of the token input.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "    \n",
    "    def batches_2_tokens(self, batches):\n",
    "        \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "        representation.\"\"\"\n",
    "        # DAY\n",
    "        # This mangles the real batch structure in the interest of readability, but\n",
    "        # by doing so, makes your understanding of batches wrong.\n",
    "        # See 'honest_batches2string' below which gives you a better\n",
    "        # understanding of the batch format.\n",
    "\n",
    "        # batches has dimensions (num_unrollings, batch_size)\n",
    "        s = [''] * batches[0].shape[0]  # batches[0].shape[0] will end up being same as batch_size\n",
    "        for b in batches: # there will be num_unrollings of these...\n",
    "            s = [''.join(self.id_2_token(x)) for x in zip(s, b)]  # DAY __ NEEDS WORK\n",
    "        return s\n",
    "\n",
    "    def honest_batches_2_tokens(self, batches):\n",
    "        import pprint\n",
    "        output = []\n",
    "        for b_index, b in enumerate(batches):  # there will be 'num_unrollings' of these\n",
    "            output.append(list())\n",
    "            for token_id_index, token_id in enumerate(b):  # there will be 'batch_size' of these\n",
    "                output[b_index].append(self.id_2_token(token_id))\n",
    "        return pprint.pformat(output)\n",
    "\n",
    "class SingleCharacterGenerator(SequenceGenerator):\n",
    "    def _init_vocab(self):\n",
    "        '''Must be implemented in subclasses and create the following:\n",
    "                self._reverse_vocab_dict (ID keys and text values)\n",
    "                self.vocab_size'''\n",
    "        self.vocab_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "        self._vocab_dict = dict()\n",
    "        self._vocab_dict[' '] = 0\n",
    "        for i, v in enumerate(list(string.ascii_lowercase[:self.vocab_size - 1])):\n",
    "            self._vocab_dict[v] = i+1\n",
    "        self._reverse_vocab_dict = dict(zip(self._vocab_dict.values(), self._vocab_dict.keys()))\n",
    "    def _init_token_sequence(self):\n",
    "        '''Must be implemented in subclasses and create the following:\n",
    "                self._token_seq = list of token id's in original input order (so duplicate is ok)\n",
    "                self._token_seq_size = the total # of tokens in the input stream\n",
    "                '''\n",
    "        first_letter = ord(string.ascii_lowercase[0])\n",
    "        self._token_seq = list()\n",
    "        for char in self._text.lower():\n",
    "            if char in string.ascii_lowercase:\n",
    "                self._token_seq.append(self.token_2_id(char))\n",
    "            elif char == ' ':\n",
    "                self._token_seq.append(self.token_2_id(' '))\n",
    "            else:\n",
    "                pass # don't enter unknown characters (DAY should we have an UNK ID?)\n",
    "        self._token_seq_size = len(self._token_seq)\n",
    "\n",
    "class BigramGenerator(SequenceGenerator):\n",
    "    def _init_vocab(self):\n",
    "        '''Must be implemented in subclasses and create the following:\n",
    "                self._vocab_dict (text keys and ID values)  \n",
    "                self._reverse_vocab_dict (ID keys and text values)\n",
    "                self.vocab_size'''\n",
    "        self._vocab_dict = dict()\n",
    "        self._token_seq = list()\n",
    "        id_index = 0\n",
    "        for i in range(0, len(self._text)-2, 2):\n",
    "            bigram = self._text[i].lower() + self._text[i+1].lower()\n",
    "            if bigram not in self._vocab_dict:\n",
    "                self._vocab_dict[bigram] = id_index\n",
    "                id_index += 1\n",
    "            self._token_seq.append(self.token_2_id(bigram)) # dup ok -- this is just input stream as token ids\n",
    "        self.vocab_size = len(self._vocab_dict)\n",
    "        self._reverse_vocab_dict = dict(zip(self._vocab_dict.values(), self._vocab_dict.keys()))\n",
    "        self._token_seq_size = len(self._token_seq)\n",
    "    def _init_token_sequence(self):\n",
    "        '''Must be implemented in subclasses and create the following:\n",
    "                self._token_seq = list of token id's in original input order (so duplicate is ok)\n",
    "                self._token_seq_size = the total # of tokens in the input stream\n",
    "                '''\n",
    "        pass # I did this work in _init_vocab() to use just one loop\n",
    "    \n",
    "class WordGenerator(SequenceGenerator):\n",
    "    def _init_vocab(self):\n",
    "        '''Must be implemented in subclasses and create the following:\n",
    "                self._vocab_dict (text keys and ID values)  \n",
    "                self._reverse_vocab_dict (ID keys and text values)\n",
    "                self.vocab_size'''\n",
    "        self._vocab_dict = dict()\n",
    "        self._token_seq = list()\n",
    "        for i, word in enumerate(self._text.lower().split()):\n",
    "            if word not in self._vocab_dict:\n",
    "                self._vocab_dict[word] = i\n",
    "            self._token_seq.append(self.token_2_id(word)) # dup ok -- this is just input stream as token ids\n",
    "        self.vocab_size = len(self._vocab_dict)\n",
    "        self._reverse_vocab_dict = dict(zip(self._vocab_dict.values(), self._vocab_dict.keys()))\n",
    "        self._token_seq_size = len(self._token_seq)\n",
    "    def _init_token_sequence(self):\n",
    "        '''Must be implemented in subclasses and create the following:\n",
    "                self._token_seq = list of token id's in original input order (so duplicate is ok)\n",
    "                self._token_seq_size = the total # of tokens in the input stream\n",
    "                '''\n",
    "        pass # I did this work in _init_vocab() to use just one loop\n",
    "\n",
    "\n",
    "\n",
    "if True:\n",
    "    my_text = \"abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz\"\n",
    "    #my_batches = SingleCharacterGenerator(my_text.lower(), 4, 2)\n",
    "    my_batches = BigramGenerator(my_text.lower(), 4, 2)\n",
    "    print(my_batches.next())\n",
    "    print(my_batches.next())\n",
    "    print(my_batches.next())\n",
    "    print(my_batches.next())\n",
    "    #my_batches = SingleCharacterGenerator(my_text.lower(), 4, 2)\n",
    "    my_batches = BigramGenerator(my_text.lower(), 4, 2)\n",
    "    print(my_batches.honest_batches_2_tokens(my_batches.next()))\n",
    "    print(my_batches.honest_batches_2_tokens(my_batches.next()))\n",
    "    print(my_batches.honest_batches_2_tokens(my_batches.next()))\n",
    "    print(my_batches.honest_batches_2_tokens(my_batches.next()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction, vocabulary_size):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0  # prediction is in column format, so it must be indexed by [0]\n",
    "    return p\n",
    "\n",
    "def random_distribution(vocabulary_size):\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "num_nodes = 64\n",
    "embedding_size = 10\n",
    "\n",
    "if False:\n",
    "    train_batches = SingleCharacterGenerator(train_text, batch_size, num_unrollings)\n",
    "    vocabulary_size = train_batches.vocab_size\n",
    "    valid_batches = SingleCharacterGenerator(valid_text, 1, 1)\n",
    "elif False:\n",
    "    train_batches = BigramGenerator(train_text, batch_size, num_unrollings)\n",
    "    vocabulary_size = train_batches.vocab_size\n",
    "    valid_batches = BigramGenerator(valid_text, 1, 1)\n",
    "else:\n",
    "    train_batches = WordGenerator(train_text, batch_size, num_unrollings)\n",
    "    vocabulary_size = train_batches.vocab_size\n",
    "    valid_batches = WordGenerator(valid_text, 1, 1)\n",
    "\n",
    "# Save memory\n",
    "del text\n",
    "del valid_text\n",
    "del train_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import functools, operator, collections\n",
    "\n",
    "def beam_search(beam, expand_frontier, is_goal, top_k, beam_size, normalize_cost=False):\n",
    "    '''Implement general beam search (forward-pruning mem-efficient search)\n",
    "       Inputs:\n",
    "            beam = list of (path, cost) tuples\n",
    "            expand_frontier =   executable which takes a path (i.e. [id, id2, id3]) and \n",
    "                                <path_cost> and returns all nodes one step forward from path and \n",
    "                                their associated normalized costs in a list of (newpath, cost) tuples.  \n",
    "            is_goal = executable which takes <path> and <path_cost> and returns True if it\n",
    "                                is the goal\n",
    "            top_k = executable which takes list of (path, cost) tuples and <k> and returns the\n",
    "                                top k elements of the list (using domain specific criteria)\n",
    "            beam_size = the number of paths to keep at any given time\n",
    "            normalize_costs = boolean (default False).  In applications where the costs are \n",
    "                                really probabilities, if they aren't normalized after expanding\n",
    "                                the frontier, they get vanishingly small\n",
    "        Output:\n",
    "            Chosen path and cost             \n",
    "            '''\n",
    "    next_gen = []\n",
    "    done = False\n",
    "    for path, cost in beam:\n",
    "        new_paths = expand_frontier(path, cost)\n",
    "        for entry in new_paths:\n",
    "            if is_goal(*entry):\n",
    "                done = True\n",
    "        next_gen.extend(new_paths)\n",
    "    if done:\n",
    "        return top_k(next_gen, 1)[0]\n",
    "    else:\n",
    "        after_pruning = top_k(next_gen, beam_size)\n",
    "        if normalize_cost:\n",
    "            all_paths, all_costs = zip(*after_pruning) #this idiom unzips\n",
    "            costs_sum = functools.reduce(lambda x, y: x+y, all_costs)\n",
    "            normalized_costs = [all_costs[i]/costs_sum for i in range(len(all_costs))]\n",
    "            after_pruning = zip(all_paths, normalized_costs)\n",
    "        return beam_search(after_pruning, expand_frontier, is_goal, \n",
    "                           top_k, beam_size, normalize_cost=normalize_cost)\n",
    "    \n",
    "def is_X_long(path, cost, end_length=1):\n",
    "    return True if len(path) >= end_length else False\n",
    "is_80_long = functools.partial(is_X_long, end_length=80)\n",
    "is_40_long = functools.partial(is_X_long, end_length=40)\n",
    "\n",
    "def top_k_min(paths, k):\n",
    "    return sorted(paths, key=operator.itemgetter(1), reverse=False)[:k]\n",
    "def top_k_max(paths, k):\n",
    "    return sorted(paths, key=operator.itemgetter(1), reverse=True)[:k]\n",
    "def k_samples(paths, k):\n",
    "    \"\"\"Sample k elements from a distribution of probabilities.\n",
    "    An entry with high probability has higher chance of being sampled.\n",
    "    \"\"\"\n",
    "    def sample(input_paths, normalize_cost=False):\n",
    "        path_only, costs = zip(*input_paths) # this idiom unzips\n",
    "        if normalize_cost:\n",
    "            costs_sum = functools.reduce(lambda x, y: x+y, costs)\n",
    "            costs = [costs[i]/costs_sum for i in range(len(costs))]\n",
    "        r = random.uniform(0, 1)\n",
    "        s = 0\n",
    "        for i in range(len(costs)):\n",
    "            s += costs[i]\n",
    "            if s >= r:\n",
    "                return input_paths[i]\n",
    "        return input_paths[len(costs) - 1]\n",
    "    result = list()\n",
    "    for i in range(k):\n",
    "        result.append(sample(paths))\n",
    "    return result\n",
    "\n",
    "if False:\n",
    "    def expand_frontier_lstm(path, probability):\n",
    "    #with tf.Session(graph=graph) as session:  # this needs to be defined inside the Tensorflow model\n",
    "        reset_sample_state.run()\n",
    "        for node in path: # this will result in multiple unrollings of the LSTM\n",
    "            feed = [node]\n",
    "            prediction = sample_prediction.eval({sample_input: feed, keep_prob: 1})\n",
    "        # after the final unrolling, prediction has the softmax predictions for the end of our path\n",
    "        result = list()\n",
    "        for i in range(len(prediction[0])):\n",
    "            # the path is a list of IDs which is just the index into the \n",
    "            # prediction vector that we got back.  So append this index (ID)\n",
    "            # onto the path and calculate the probability of this new path\n",
    "            # as previous_probability*prediction_for_this_new_node.\n",
    "            result.append( (path + [i], probability*prediction[0][i]) )\n",
    "        return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_steps = 7001\n",
    "# See interesting implementation of 2-layer LSTM (with embeddings) here: http://pastebin.com/YP3sWkG9\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # My embedding here will simply be a 2D tensor -- the first\n",
    "    # dimension will hold the ID a (character or bigram) (the index)\n",
    "    # the second dimension will hold the embedding vector.\n",
    "    vocabulary_embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  \n",
    "    # Parameters:\n",
    "    with tf.name_scope('LSTM_cell') as lstm_cell_scope:\n",
    "        with tf.name_scope('input_gate') as input_gate_scope:\n",
    "            # Input gate: input, previous output, and bias.\n",
    "            ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1)) # [50, 64]\n",
    "            im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))       # [64, 64]\n",
    "            ib = tf.Variable(tf.zeros([1, num_nodes]))                                     # [1, 64]\n",
    "        with tf.name_scope('forget_gate') as forget_gate_scope:\n",
    "            # Forget gate: input, previous output, and bias.\n",
    "            fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "            fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "            fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "        with tf.name_scope('memory_cell') as memory_cell_scope:\n",
    "            # Memory cell: input, state and bias.                             \n",
    "            cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "            cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "            cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "        with tf.name_scope('output_gate') as output_gate_scope:\n",
    "            # Output gate: input, previous output, and bias.\n",
    "            ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "            om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "            ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "            \n",
    "            \n",
    "        # Reduce the input/output matmuls from 4 to 1 each by concatenating the 4 gates\n",
    "        concatx = tf.concat(1, [ix, fx, cx, ox])\n",
    "        concatm = tf.concat(1, [im, fm, cm, om])\n",
    "        concatb = tf.concat(1, [ib, fb, cb, ob])\n",
    "        \n",
    "        \n",
    "        # Variables saving state across unrollings.\n",
    "        saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "        saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "        # Classifier weights and biases.\n",
    "        w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1)) #output is one-hot vector\n",
    "        b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "        # Dropout percent\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        with tf.name_scope(lstm_cell_scope):\n",
    "            # #Instead of these 4 matmuls, do the one concatenated matmul and then split results\n",
    "            #input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "            #forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "            #update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "            #output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "\n",
    "            concatmatmul = tf.matmul(i, concatx) + tf.matmul(o, concatm) + concatb\n",
    "            input_gate, forget_gate, update, output_gate = tf.split(1, 4, concatmatmul)\n",
    "            input_gate = tf.sigmoid(tf.nn.dropout(input_gate, keep_prob))\n",
    "            forget_gate = tf.sigmoid(forget_gate)\n",
    "            output_gate = tf.sigmoid(tf.nn.dropout(output_gate, keep_prob))\n",
    "            state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "    # The LSTM\n",
    "    #\n",
    "    # In the code above, batch_size (bs) and num_nodes (nn) are both 64 (they don't have to be equal)\n",
    "    # 27 is the vocabulary size, 50 is the embed_size\n",
    "    #\n",
    "    # input_gate(bs, nn)   = sigmoid( input(bs, 50) * ix(50, nn) + output(bs, nn) * im(nn, nn) + ib(1, nn) )\n",
    "    # forget_gate(bs, nn)  = sigmoid( input(bs, 50) * ix(50, nn) + output(bs, nn) * im(nn, nn) + ib(1, nn) )\n",
    "    # output_gate(bs, nn)  = sigmoid( input(bs, 50) * ix(50, nn) + output(bs, nn) * im(nn, nn) + ib(1, nn) )\n",
    "    # update(bs, nn)       = tanh(    input(bs, 50) * cx(50, nn) + output(bs, nn) * cm(nn, nn) + cb(1, nn) )\n",
    "    #\n",
    "    # output(bs, nn) = output_gate(bs, nn) * tanh( state(bs, nn) )\n",
    "    # state(bs, nn)  = forget_gate(bs, nn) * state(bs, nn)  +  input_gate(bs, nn) * update(bs, nn)\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            # These 11 elements of train_data will be pulled in from feed_dict\n",
    "            # Note that usually we have seen feed_dict specified as {var_name: value}\n",
    "            # but in this case, since these 11 array elements don't have a var_name, the \n",
    "            # feed_dict will use the tensorflow object as the key instead, i.e.\n",
    "            # feed_dict={<tf.Tensor 'Placeholder_1:0' shape=(64, 27) dtype=float32>: value}\n",
    "            # See below where the feed_dict is prepared before calling session.run.\n",
    "            \n",
    "            # Note that the new shape=[batch_size, ] matches a batch of IDs (the IDs don't have a dimension, they\n",
    "            #    are just integers)\n",
    "            tf.placeholder(tf.int32, shape=[batch_size, ])) #this will be pulled in from feed_dict\n",
    "    # train_data now has the shape (11, 64, 50), or (num_unrollings, batch_size, embedding_size)\n",
    "    # and sequential text from the original text input is 'striped' across the first dimension (11)\n",
    "    \n",
    "    # Create the train_inputs by converting the train_data (batches of IDs) into \n",
    "    #    embeddings (batches of embeddings)\n",
    "    #    Here is what the ID batches look like (num_unrollings=2 (+1), batch_size=4)\n",
    "    #    [array([  1.,   7.,  13.,  19.]), \n",
    "    #     array([  2.,   8.,  14.,  20.]), \n",
    "    #     array([  3.,   9.,  15.,  21.])]\n",
    "    train_inputs = [tf.nn.embedding_lookup(vocabulary_embeddings, id_array) \n",
    "                            for id_array in train_data[:num_unrollings]]\n",
    "    # Create the train_labels by shifting by one time step and then\n",
    "    #    converting the train_data (batches of IDs) into\n",
    "    #    one_hot vectors (batches of one_hots)\n",
    "    train_labels = [train_batches.id_2_onehot(id_array) \n",
    "                            for id_array in train_data[1:]]\n",
    "    \n",
    "    \n",
    "    # Unrolled LSTM loop.\n",
    "    with tf.name_scope(lstm_cell_scope):\n",
    "        outputs = list()\n",
    "        output = saved_output\n",
    "        state = saved_state\n",
    "        for i in train_inputs: # since train_inputs is num_unrollings=10 long, this will create 10 LSTM cells\n",
    "            output, state = lstm_cell(i, output, state)\n",
    "            outputs.append(output)  # at each iter of the lstm_cell, append the character it predicted to outputs.\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits, tf.concat(0, train_labels)))\n",
    "        tf.scalar_summary('loss', loss)\n",
    "\n",
    "    # Optimizer.\n",
    "    #   Note that all 10 unrollings are done before the optimizer comes in and looks at the\n",
    "    #   output sequence of 10 chars vs. the label sequence of 10 chars and then calculates\n",
    "    #   the gradients and adjusts the parameters.  Then in the next step another 10 characters\n",
    "    #   will be predicted.\n",
    "    with tf.name_scope(\"Optimizer\"):\n",
    "        global_step = tf.Variable(0)\n",
    "        #learning_rate = tf.train.exponential_decay(\n",
    "        #    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "        learning_rate = tf.train.exponential_decay(\n",
    "            10.0, global_step, num_steps//10, 0.70, staircase=True)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        # DAY: this clipping below is the hack to prevent exploding gradients \n",
    "        #(LSTM was the elegant solution to prevent vanishing gradient)\n",
    "        gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "        optimizer = optimizer.apply_gradients(\n",
    "            zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    # (nothing here is triggered in the training)\n",
    "    #     first, variables\n",
    "    sample_input = tf.placeholder(tf.int32, shape=[1, ]) #now it is just an id\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    #     reset zeros out saved_sample_output and saved_sample_state\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    #     Define one lstm_cell with no unrolling (will be used for sampling from the trained model)\n",
    "    \n",
    "    sample_output, sample_state = lstm_cell(\n",
    "                                    tf.nn.embedding_lookup(vocabulary_embeddings, sample_input), \n",
    "                                    saved_sample_output, \n",
    "                                    saved_sample_state)\n",
    "    #     Define the next prediction (but make sure dependencies are calculated first)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.590661 learning rate: 10.000000\n",
      "================================================================================\n",
      "oxald uylhagxkhc tsdfvimfwgbnkgxleibprdykpdtenyvuqabxmmfmaahdzfggwpsmlrierhkqlqwuuurhbufjolhszjzrqatldjiqrklcbb ohotyrxbhfaaqihejt hylh ufxzgauptnw kvchskxfknpa\n",
      "xseuxazlawbfwhnkylvifulwtjuqjauudvnyydupelbtnihltgmbhkdrsnudaaipxjfaajekad havxbghbajojytmpyjvelxewegbyxcxmssyqi agitmnvpexpkvogcywnqihqtltsvzbtktlhmogtpiynbhbg\n",
      "etqaljiibylwe vangnfgsqmspabzeywinuwrdkjthmcchfj wfsstipgaebxycvd m  zfafz adlohmgfjjkfoyxnlxilfgmbhrdcgwjutmirhyxioqv qecuftzhjjjkxacngjlhndjlqmueiajosh znxmhr\n",
      "zrttfraniggzpfdhwyrpogkoxgxcphbkghcn jpnqgzzxivyqqygevdkgydtsisaepjjpruhivawdgvfylhajhexcbex ikrcmgj hsyztaeoqyaczfbqoe  ygusulragaa nktxzsxiqowcqgywhkkpfyfqqwo\n",
      "okvya fmdeuqvgwkthcwkasitwqtojlsolqpqodagmansgyjqxuctzjakaipczkzzucotfiuz twuyamoffhccsgtykhnzsdumxzuqgvbjbnartgytczcbftiktuyolgvwwmooni lfwdzfmlicctju p cxarif\n",
      "================================================================================\n",
      "Average loss at step 100: 5.318308 learning rate: 10.000000\n",
      "Average loss at step 200: 4.611864 learning rate: 10.000000\n",
      "Average loss at step 300: 4.260490 learning rate: 10.000000\n",
      "Average loss at step 400: 4.065714 learning rate: 10.000000\n",
      "Average loss at step 500: 3.947117 learning rate: 10.000000\n",
      "Average loss at step 600: 3.864779 learning rate: 10.000000\n",
      "Average loss at step 700: 3.795689 learning rate: 7.000000\n",
      "Average loss at step 800: 3.736407 learning rate: 7.000000\n",
      "Average loss at step 900: 3.714342 learning rate: 7.000000\n",
      "Average loss at step 1000: 3.706922 learning rate: 7.000000\n",
      "================================================================================\n",
      "ujeo dace the rantrar azpant have kele that and all apimrinol as the counters baltgy an bigerdly for a yark of there what of the prety of bright because in the \n",
      "ple betwentages by forse doce muning of from the payhar electroyle to bave all padson eomelatince worked groduned or one nine six zero one zero five couccudht h\n",
      "ggtar open eighting is peedity of his was veve of tions and the forcet the motk two zero hads ed thedingian these langs the two he jintrent is the el the derebi\n",
      "cqilifabeth the rabaut rulescu three one six six five zero one seven six conpanql loves the gistys are the ele nannlancen prant as advis of they in zero zero th\n",
      "nsunes to a moresfy in mast refeprefere of citlither this sular ofysed fould jo of the spiters on the laun one nine eight theaninks a part zmumuch in some to fe\n",
      "================================================================================\n",
      "Average loss at step 1100: 3.662133 learning rate: 7.000000\n",
      "Average loss at step 1200: 3.674284 learning rate: 7.000000\n",
      "Average loss at step 1300: 3.710385 learning rate: 7.000000\n",
      "Average loss at step 1400: 3.633856 learning rate: 4.900000\n",
      "Average loss at step 1500: 3.629554 learning rate: 4.900000\n",
      "Average loss at step 1600: 3.580031 learning rate: 4.900000\n",
      "Average loss at step 1700: 3.570016 learning rate: 4.900000\n",
      "Average loss at step 1800: 3.591808 learning rate: 4.900000\n",
      "Average loss at step 1900: 3.633310 learning rate: 4.900000\n",
      "Average loss at step 2000: 3.620256 learning rate: 4.900000\n",
      "================================================================================\n",
      "bidition in one nine eight five five zero four two zero zero six eight zero zero zero four four zero zero zero zero zero zero five five six four eight in zero z\n",
      " kstitagems put palt and curnhill sistain to been usitictinged so the from camsity in the chith aft wester and labder samons the chich his yough aring citm of t\n",
      "fy moll uneart however belay shonch also called as a shough is when one nine two nine zero two three x five nine one nine two zero two zero zero two four three \n",
      "zast rdrational ency called and the riscy trivered lis with intemped trans busy zero three joblearses to uain in a canhers raring poglessives ii be sequistss of\n",
      "vwth the cition jssc for revac an one nine one zero two zero zero zero zero five two zero zero zero three five other one nine nine zero five zero areal amohs ma\n",
      "================================================================================\n",
      "Average loss at step 2100: 3.551557 learning rate: 3.430000\n",
      "Average loss at step 2200: 3.516016 learning rate: 3.430000\n",
      "Average loss at step 2300: 3.549886 learning rate: 3.430000\n",
      "Average loss at step 2400: 3.554709 learning rate: 3.430000\n",
      "Average loss at step 2500: 3.519691 learning rate: 3.430000\n",
      "Average loss at step 2600: 3.530674 learning rate: 3.430000\n",
      "Average loss at step 2700: 3.511003 learning rate: 3.430000\n",
      "Average loss at step 2800: 3.528646 learning rate: 2.401000\n",
      "Average loss at step 2900: 3.514481 learning rate: 2.401000\n",
      "Average loss at step 3000: 3.485292 learning rate: 2.401000\n",
      "================================================================================\n",
      "vn all century lisdeciety chiment be pande aidence looking som the reator consurition of the light to light is meach s forma a would mal deat and by an methly c\n",
      "here in the raction the interecture of communioner jtish that tovard eifisurks the und risulareon of the amplenong to for that a same is incluld frole reched by\n",
      "necussic with stilnging mege eislish toon laul comokon of swueculy of the against and the pedder kethy most asding the non and awavers foo depent refected the r\n",
      "oat on the reatuphic externant extults kanspat eusn that nar reaps of commenters rhead degeber states of the gyncation was scompanot up brisculist with the repl\n",
      "cy dapector hebsger more inten compranting engdors in scalhobinited because be artict ar bepim not sights green of the voold the lains two eight seven four the \n",
      "================================================================================\n",
      "Average loss at step 3100: 3.530313 learning rate: 2.401000\n",
      "Average loss at step 3200: 3.516034 learning rate: 2.401000\n",
      "Average loss at step 3300: 3.536704 learning rate: 2.401000\n",
      "Average loss at step 3400: 3.505840 learning rate: 2.401000\n",
      "Average loss at step 3500: 3.533635 learning rate: 1.680700\n",
      "Average loss at step 3600: 3.487984 learning rate: 1.680700\n",
      "Average loss at step 3700: 3.551153 learning rate: 1.680700\n",
      "Average loss at step 3800: 3.496212 learning rate: 1.680700\n",
      "Average loss at step 3900: 3.498813 learning rate: 1.680700\n",
      "Average loss at step 4000: 3.485772 learning rate: 1.680700\n",
      "================================================================================\n",
      "ehs two three zero goee dodod snil orcer s thrlighking the ky the computer to golf pripoter its at policed to chemiciar smy after duric becausly sefentuly to fe\n",
      "fs ep heors minist to sedge of the he be same with allown the his euro of peral bamp the cologicale exoludechantish inscurg one nine nine eight three one nine n\n",
      "rpecwed to adverent arclained for that the revacas offnown tex compution bibute but son hustrase bain commun movinglas there reforning by sautd freatmusseraly m\n",
      "scler what k is inemly the intertish danlrel englimation of zero two seven one nine nine eight belvide of seven of two nine four nine nine five five six two zer\n",
      "nnimsult s twom indismalued il king in the free one three three year to fleaswes and which oveing united engluent was exsure insidally the valare with sequink i\n",
      "================================================================================\n",
      "Average loss at step 4100: 3.441048 learning rate: 1.680700\n",
      "Average loss at step 4200: 3.477458 learning rate: 1.176490\n",
      "Average loss at step 4300: 3.478602 learning rate: 1.176490\n",
      "Average loss at step 4400: 3.435520 learning rate: 1.176490\n",
      "Average loss at step 4500: 3.442984 learning rate: 1.176490\n",
      "Average loss at step 4600: 3.481487 learning rate: 1.176490\n",
      "Average loss at step 4700: 3.506037 learning rate: 1.176490\n",
      "Average loss at step 4800: 3.473098 learning rate: 1.176490\n",
      "Average loss at step 4900: 3.454583 learning rate: 0.823543\n",
      "Average loss at step 5000: 3.440466 learning rate: 0.823543\n",
      "================================================================================\n",
      "rogned the castulably three six eight five produgy by seven when way distrains after danguiphire theverd as a politut econally acturber or enence the missions o\n",
      "fferation is pronear expdor and of the neade two zero called to a criplsors which the destrans callems six from to the extince derrivery one nine eight one can \n",
      "ier new centormencated the sreated for the one nine two six nine one nine one milendh three zero based seven the text pright prohbive altil relast cople the net\n",
      "jtstrems boon i cath and domist rcra iced in ling two zero yrower first seven cover later of be elected sheasts shee confect giver and notirdo batising eqilrate\n",
      "kdancal shail hdeecient was mames lever to one nine eine residendary traro the product seversion no voyor system hon persider compbalaraliants is also bus the f\n",
      "================================================================================\n",
      "Average loss at step 5100: 3.477653 learning rate: 0.823543\n",
      "Average loss at step 5200: 3.435845 learning rate: 0.823543\n",
      "Average loss at step 5300: 3.501151 learning rate: 0.823543\n",
      "Average loss at step 5400: 3.473443 learning rate: 0.823543\n",
      "Average loss at step 5500: 3.484555 learning rate: 0.823543\n",
      "Average loss at step 5600: 3.498335 learning rate: 0.576480\n",
      "Average loss at step 5700: 3.531892 learning rate: 0.576480\n",
      "Average loss at step 5800: 3.481547 learning rate: 0.576480\n",
      "Average loss at step 5900: 3.488512 learning rate: 0.576480\n",
      "Average loss at step 6000: 3.477156 learning rate: 0.576480\n",
      "================================================================================\n",
      "kx no of chostowring see four seven vevel the traing desplawrt whip from him crones of the lamt or communil of six the sirpoper of a re and the skead suvaring a\n",
      "h these fery out malovils and wwrs to ating ktagel custuku ca lecired by ignerce m craymant and sociation cance moosists of this one two zero zero zero zero zer\n",
      "cgal for boyanize jurans included s teve vleolg ortably intermationss in attould anageing herodence worlel his mack wern the mhichs imluk crail convercy hemally\n",
      "qkudh someacsion is explup grohaning fame first who of mest sciestral broscular of one nine six nine four four six two zero zero in one nine sivilt to however p\n",
      "xyh all mathum and the not lot post move oneer of the it advarant in one one seven purfessed to vo one jesigt marrarbong mwan the s time frove inclutalined gimo\n",
      "================================================================================\n",
      "Average loss at step 6100: 3.471748 learning rate: 0.576480\n",
      "Average loss at step 6200: 3.480719 learning rate: 0.576480\n",
      "Average loss at step 6300: 3.454854 learning rate: 0.403536\n",
      "Average loss at step 6400: 3.441839 learning rate: 0.403536\n",
      "Average loss at step 6500: 3.503716 learning rate: 0.403536\n",
      "Average loss at step 6600: 3.490850 learning rate: 0.403536\n",
      "Average loss at step 6700: 3.486737 learning rate: 0.403536\n",
      "Average loss at step 6800: 3.450050 learning rate: 0.403536\n",
      "Average loss at step 6900: 3.493198 learning rate: 0.403536\n",
      "Average loss at step 7000: 3.427734 learning rate: 0.282475\n",
      "================================================================================\n",
      "craeuch untrations andivors which a trolory was allike cic rirega eight three five nine vin with an hall to beps lalers s diame pweaso english as us l time bilt\n",
      "tzics was s one monuition three fore appard a five one nine five your one zero seven six seven be ero one four two one nine two zero your spest are japale de se\n",
      "gmre forrents a king of link phulocal sualesconder and more of the musery he verf the popution were later one eight roades accoritives seede four persho have of\n",
      "tricnoveriins lick and sportan earining suring worse shough the u one five zero zero five prixed spencil and first undonse was include of the deuntirations of h\n",
      "wznjh pokiver bray k forses remoin and the comects cure thes most the prizes lould mavics oversing the towline exhines with round helpian thumptive or history p\n",
      "================================================================================\n",
      "\n",
      "Beam Search generated sample\n",
      "********************************************************************************\n",
      "njic the population which one nine nine nine nine nine eight one nine nine eight\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next() #\n",
    "        # batches is of dimension (num_unrollings, batch_size, ) (11, 64,)\n",
    "        # where sequential text from the input is \"striped\" across the unrollings.  For \n",
    "        # example, if 'char1' stands for the first character in the original text \n",
    "        # batches looks like this (assuming 'segment' is 15000):\n",
    "        # [                                        # there are 'num_unrollings' rows\n",
    "        #   [char1,  char15000,  char 30000, ...], # each row is 'batch_size'\n",
    "        #   [char2,  char15001,  char 30001, ...],\n",
    "        #   ...\n",
    "        #   [char11, char15010,  char 30010, ...]\n",
    "        # ]\n",
    "        # when we call train_batches.next(), the next 'batches' will look like this:\n",
    "        # [                                        # there are 'num_unrollings' rows\n",
    "        #   [char11, char15010,  char 30010, ...], # each row is 'batch_size'\n",
    "        #   [char12, char15011,  char 30011, ...],\n",
    "        #   ...\n",
    "        #   [char21, char15020,  char 30020, ...]\n",
    "        # ]\n",
    "        # it might look like a bug that the second 'batches' repeats char11, char1510, etc.\n",
    "        # but it is not a bug.  in the first 'batches', char11 was included only as the label\n",
    "        # needed for the 10th entry (char10). (LSTM takes char10 in as an input and expects char11\n",
    "        # as the true label of the output).  So -- char11 was never put into the LSTM_cell in the\n",
    "        # first 'batches' -- it is only used as a label, so it need to be included as the first\n",
    "        # item in the second 'batches' so that in can now be an input into an LSTM cell.\n",
    "        #\n",
    "        feed_dict = {keep_prob: 0.75}\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "            # Normally we see feed_dict={var_name: value}, but here we don't have the var_names\n",
    "            # for the training data batches in the graph definition (it is an array of tensors)\n",
    "            # so instead, we use the tensorflow object itself (from the graph definition, in\n",
    "            # train_data[i]) as the key in the feed_dict entries.\n",
    "            \n",
    "        _, l, predictions, lr = session.run(\n",
    "              [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            \n",
    "            if False:\n",
    "                # Need to fix this -- is not reporting correctly now and exp is overflowing and mem is leaking\n",
    "                \n",
    "                #labels = np.concatenate(list(batches)[1:])\n",
    "                # Convert labels to one_hot vectors (they are batches of IDs now). Then\n",
    "                #    concatenate all 10 unrollings together\n",
    "                labels = tf.reshape(tf.concat(1, [train_batches.id_2_onehot(id_array) \n",
    "                                                for id_array in batches[1:]]), [-1, vocabulary_size])\n",
    "                print('Minibatch perplexity: %.2f' % float(\n",
    "                    np.exp(logprob(predictions, labels.eval())))) #DAY the \".eval()\" converts tensor to numpy array\n",
    "            \n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    # start with a random char/token from our vocabulary\n",
    "                    feed = [random.choice(xrange(vocabulary_size))]\n",
    "                    #feed = sample(random_distribution(vocabulary_size), vocabulary_size) #old\n",
    "                    # sentence = characters(feed)[0] #old\n",
    "                    sentence = train_batches.id_2_token(feed[0]) #new\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed, keep_prob: 1})\n",
    "                        feed = train_batches.onehot_2_id(sample(prediction, vocabulary_size))\n",
    "                        # sentence += characters(feed)[0] #old\n",
    "                        sentence += train_batches.id_2_token(feed[0]) #new\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity\n",
    "            # Need to fix this -- is not reporting correctly now and exp is overflowing and mem is leaking\n",
    "            if False:\n",
    "                reset_sample_state.run()\n",
    "                valid_logprob = 0\n",
    "                for _ in range(valid_size):\n",
    "                    b = valid_batches.next()\n",
    "                    predictions = sample_prediction.eval({sample_input: b[0], keep_prob: 1})\n",
    "                    valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "                print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                    valid_logprob / valid_size)))\n",
    "                \n",
    "    # Use BEAM SEARCH to generate a string\n",
    "    def expand_frontier_lstm(path, probability):\n",
    "        reset_sample_state.run()\n",
    "        for node in path: # this will result in multiple unrollings of the LSTM\n",
    "            feed = [node]\n",
    "            prediction = sample_prediction.eval({sample_input: feed, keep_prob: 1})\n",
    "        # after the final unrolling, prediction has the softmax predictions for the end of our path\n",
    "        result = list()\n",
    "        for i in range(len(prediction[0])):\n",
    "            # the path is a list of IDs which is just the index into the \n",
    "            # prediction vector that we got back.  So append this index (ID)\n",
    "            # onto the path and calculate the probability of this new path\n",
    "            # as previous_probability*prediction_for_this_new_node.\n",
    "            result.append( (path + [i], probability*prediction[0][i]) )\n",
    "        return result\n",
    "\n",
    "    start_id = random.choice(xrange(vocabulary_size))\n",
    "    start_prob = 1\n",
    "    generated_ids, generated_costs = beam_search([([start_id,], start_prob),], \n",
    "                                expand_frontier_lstm, is_40_long, k_samples, 4, normalize_cost=True)\n",
    "    #generated_ids, prob= beam_search([(start_id, start_prob),], \n",
    "    #                            expand_frontier_lstm, is_40_long, top_k_max, 4)\n",
    "    #print('After Exit debug: len(generated_ids) = %s' % len(generated_ids))\n",
    "    #print('After Exit debug: generated_ids = %s' % generated_ids)\n",
    "    generated_tokens = [train_batches.id_2_token(generated_ids[i]) for i in range(len(generated_ids))]\n",
    "    print('\\nBeam Search generated sample')\n",
    "    print('*'*80)\n",
    "    print(''.join(generated_tokens))\n",
    "    print('*'*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
